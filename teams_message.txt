
[æ–°ç‰¹æ€§] Azure OpenAIä¸­ä½¿ç”¨PowerShellå¤„ç†è‡ªæœ‰æ•°æ®çš„æ–°æŒ‡å—å‘å¸ƒ2023-11-23 04:15:28https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

è¿™æ˜¯ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œä¸»è¦åŒ…å«äº†å¦‚ä½•ä½¿ç”¨ PowerShell åœ¨ Azure OpenAI ä¸­ä½¿ç”¨è‡ªå·±çš„æ•°æ®çš„ç¤ºä¾‹ä»£ç ã€‚è¿™äº›ä»£ç ä¸»è¦åŒ…æ‹¬è®¾ç½®æ‰€éœ€å˜é‡ï¼Œç¤ºä¾‹ PowerShell å‘½ä»¤ï¼Œä»¥åŠç¤ºä¾‹è¾“å‡ºã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€äº›é‡è¦çš„æç¤ºå’Œå»ºè®®ï¼Œä¾‹å¦‚å¦‚ä½•å®‰å…¨åœ°å­˜å‚¨å’Œè®¿é—®å‡­æ®ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ web åº”ç”¨ä¸æ¨¡å‹è¿›è¡ŒèŠå¤©ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œæ›´æ–°äº†æ—¥æœŸï¼Œå¹¶åœ¨æ–‡ä»¶ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„åŒºåŸŸï¼Œè¯¥åŒºåŸŸåŒ…å«äº† PowerShell å¿«é€Ÿå…¥é—¨çš„é“¾æ¥ã€‚

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œæ·»åŠ äº†ä¸€ä¸ªæ–°çš„ç¼–ç¨‹è¯­è¨€ - PowerShellã€‚

*************************

[æ¨¡å‹] å­æœåŠ¡åç§°æ›´æ–°ï¼šä»"core"æ›´æ”¹ä¸º"prompt-flow"2023-11-23 07:52:01https://learn.microsoft.com/en-us/azure/machine-learning/how-to-retrieval-augmented-generation-cloud-to-local

è¿™ä¸ªæ–‡ä»¶çš„ä¿®æ”¹ä¸»è¦æ˜¯æ›´æ–°äº†å­æœåŠ¡çš„åç§°ï¼Œä»"core"æ›´æ”¹ä¸º"prompt-flow"ã€‚

*************************

[Model] New PowerShell Sample for Azure OpenAI Chat Models Introduced2023-11-23 04:15:28https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

This new file provides a PowerShell sample for the "Use Your Own Data Quickstart" guide. It includes instructions and code snippets on how to set up required variables, example PowerShell commands for interacting with Azure OpenAI chat models, and tips on how to trigger a response from the model. It also provides an example output and important notes on credential security and troubleshooting. Lastly, it provides information on how to chat with the model using a web app.

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

This file has been updated to include the PowerShell quickstart guide in the programming language pivot section. The date of the document has also been updated.

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

This file has been updated to include PowerShell in the list of programming languages.

*************************

commit_patch_data :

From 27da4a67d93dc09aa736b005c751d6f1cad55c1b Mon Sep 17 00:00:00 2001
From: Jimmy Stridh <61634+jimmystridh@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:28:49 +0100
Subject: [PATCH] Update embedding example for beta9 api

+ fix compilation errors.
---
 articles/ai-services/openai/how-to/embeddings.md | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/how-to/embeddings.md b/articles/ai-services/openai/how-to/embeddings.md
index 8e3777d7abda..878d3bb2c3e9 100644
--- a/articles/ai-services/openai/how-to/embeddings.md
+++ b/articles/ai-services/openai/how-to/embeddings.md
@@ -80,11 +80,15 @@ AzureKeyCredential credentials = new (oaiKey);
 
 OpenAIClient openAIClient = new (oaiEndpoint, credentials);
 
-EmbeddingsOptions embeddingOptions = new ("Your text string goes here");
+EmbeddingsOptions embeddingOptions = new()
+{
+    DeploymentName = "text-embedding-ada-002",
+    Input = { "Your text string goes here" },
+};
 
-var returnValue = openAIClient.GetEmbeddings("YOUR_DEPLOYMENT_NAME", embeddingOptions);
+var returnValue = openAIClient.GetEmbeddings(embeddingOptions);
 
-foreach (float item in returnValue.Value.Data[0].Embedding)
+foreach (float item in returnValue.Value.Data[0].Embedding.ToArray())
 {
     Console.WriteLine(item);
 }



[æ¨¡å‹] "EmbeddingsOptions"å®ä¾‹åŒ–å’Œ"GetEmbeddings"æ–¹æ³•çš„é‡è¦ä¿®æ”¹
2023-11-22 08:28:49
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings

è¿™ä¸ªæ–‡ä»¶ä¸»è¦è¿›è¡Œäº†ä¸€äº›ä»£ç çš„ä¿®æ”¹ã€‚é¦–å…ˆï¼Œä¿®å¤äº†ç¼–è¯‘é”™è¯¯ã€‚ç„¶åï¼Œå¯¹"EmbeddingsOptions"çš„å®ä¾‹åŒ–è¿›è¡Œäº†ä¿®æ”¹ï¼Œæ·»åŠ äº†"DeploymentName"å’Œ"Input"ä¸¤ä¸ªå±æ€§ã€‚æœ€åï¼Œå¯¹"GetEmbeddings"æ–¹æ³•çš„è°ƒç”¨å’Œå¯¹è¿”å›å€¼çš„å¤„ç†è¿›è¡Œäº†ä¿®æ”¹ã€‚

*************************

commit_patch_data :

From 0ba0d00c18c4ebfa66264abdeebfcc3293f1cc25 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 08:44:48 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/latency.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 6461e21593c7..169852a518e6 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -20,7 +20,7 @@ This article will provide you with background around how latency works with Azur
 
 The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned. The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
 
-## Completions and chat completions
+## Improve performance
 
 ### Model selection
 
@@ -44,7 +44,7 @@ Streaming impacts perceived latency. If you have streaming enabled you'll receiv
 
 Sentiment analysis, language translation, content generation.
 
-There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
+There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response.
 
 ### Content filtering
 



[æ¨¡å‹] "ä¼˜åŒ–æ€§èƒ½"æ ‡é¢˜æ›´æ–°åŠæ¨¡å‹å“åº”æµæè¿°åˆ é™¤
2023-11-22 13:44:48
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

è¿™ä¸ªæ–‡ä»¶ä¸»è¦è¿›è¡Œäº†ä¸¤å¤„ä¿®æ”¹ã€‚é¦–å…ˆï¼Œå°†"Completions and chat completions"æ ‡é¢˜æ›´æ”¹ä¸º"Improve performance"ã€‚å…¶æ¬¡ï¼Œåˆ é™¤äº†ä¸€æ®µå…³äºæ¨¡å‹å“åº”æµçš„æè¿°ï¼Œå³ä½¿ä»APIåˆ°å®¢æˆ·ç«¯çš„å“åº”ç¦ç”¨äº†æµï¼ŒæŠ€æœ¯ä¸Šæ¨¡å‹æœ¬èº«æ€»æ˜¯åœ¨æµå¼ä¼ è¾“å…¶å“åº”ã€‚

*************************

commit_patch_data :

From a327c4725689c025b40d8c9ceaef2d309672dfb6 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:56:32 -0500
Subject: [PATCH] update

---
 .../openai/how-to/switching-endpoints.md      | 189 +++++++++++++++++-
 1 file changed, 187 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 7d45305693b2..635cae90c973 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,8 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-> [!NOTE]
-> This library is maintained by OpenAI and is currently in preview. Refer to the [release history](https://github.com/openai/openai-python/releases) or the [version.py commit history](https://github.com/openai/openai-python/commits/main/openai/version.py) to track the latest updates to the library.
+# [OpenAI Python 0.28.1](#tab/python)
 
 ## Authentication
 
@@ -201,6 +200,192 @@ embedding = openai.Embedding.create(
 </tr>
 </table>
 
+# [OpenAI Python 1.x](#tab/python-new)
+
+## Authentication
+
+We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
+
+### API key
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+import os
+from openai import AzureOpenAI
+    
+client = AzureOpenAI(
+    api_key=os.getenv("AZURE_OPENAI_KEY"),  
+    api_version="2023-10-01-preview",
+    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
+    )
+```
+
+</td>
+</tr>
+</table>
+
+<a name='azure-active-directory-authentication'></a>
+
+### Microsoft Entra authentication
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+
+
+
+
+```
+
+</td>
+<td>
+
+```python
+from azure.identity import DefaultAzureCredential, get_bearer_token_provider
+from openai import AzureOpenAI
+
+token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")
+
+api_version = "2023-12-01-preview"
+endpoint = "https://my-resource.openai.azure.com"
+
+client = AzureOpenAI(
+    api_version=api_version,
+    azure_endpoint=endpoint,
+    azure_ad_token_provider=token_provider,
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Keyword argument for model
+
+OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
+
+For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+completion = client.completions.create(
+    model='gpt-3.5-turbo-instruct',
+    prompt="<prompt>)
+)
+
+chat_completion = openai.chat.completions.create(
+    model="gpt-4",
+    messages="<messages>"
+)
+
+embedding = client.embeddings.create(
+    input="<input>",
+    model="text-embedding-ada-002"
+)
+```
+
+</td>
+<td>
+
+```python
+client.completions.create(
+    model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
+    prompt=<"prompt">
+)
+
+client.chat.completions.create(
+    model="gpt-35-turbo", # model = "deployment_name".
+    messages=<"messages">
+)
+
+client.embeddings.create(
+    input = "<input>",
+    model= "text-embedding-ada-002" # model = "deployment_name".
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Azure OpenAI embeddings multiple input support
+
+OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+inputs = ["A", "B", "C"] 
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002"
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+inputs = ["A", "B", "C"] #max array size=16
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
+  #engine="text-embedding-ada-002"
+)
+```
+
+</td>
+</tr>
+</table>
+
+---
+
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).



[æ¨¡å‹] OpenAI Python 1.xè®¤è¯æ–¹å¼åŠAzure OpenAI embeddingså¤šè¾“å…¥æ”¯æŒçš„æ›´æ–°è¯´æ˜
2023-11-22 14:56:32
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

æ­¤æ¬¡æäº¤å¯¹æ–‡ä»¶è¿›è¡Œäº†å¤§é‡çš„æ›´æ–°ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. åˆ é™¤äº†å…³äºOpenAIåº“çš„æ³¨æ„äº‹é¡¹ï¼Œæ”¹ä¸ºäº†OpenAI Python 0.28.1çš„æ ‡ç­¾ã€‚

2. åœ¨æ–‡ä»¶ä¸­æ·»åŠ äº†å¤§é‡çš„æ–°å†…å®¹ï¼Œä¸»è¦åŒ…æ‹¬OpenAI Python 1.xçš„è®¤è¯æ–¹å¼ï¼ŒåŒ…æ‹¬APIå¯†é’¥å’ŒMicrosoft Entraè®¤è¯çš„ä»£ç ç¤ºä¾‹ã€‚

3. æ·»åŠ äº†å…³äºæ¨¡å‹å…³é”®å­—å‚æ•°çš„è¯´æ˜ï¼ŒåŒ…æ‹¬OpenAIå’ŒAzure OpenAIçš„ä½¿ç”¨å·®å¼‚ã€‚

4. æ·»åŠ äº†å…³äºAzure OpenAI embeddingså¤šè¾“å…¥æ”¯æŒçš„è¯´æ˜ï¼ŒåŒ…æ‹¬è¾“å…¥é™åˆ¶å’Œä»£ç ç¤ºä¾‹ã€‚

5. åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ äº†ä¸‹ä¸€æ­¥çš„å­¦ä¹ é“¾æ¥ã€‚

*************************

commit_patch_data :

From dbffff50a4994b23b4fa171b747aa0bcc6ee4893 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:58:22 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 635cae90c973..8a8f70c55a10 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -7,7 +7,7 @@ ms.author: mbullwin #delegenz
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to
-ms.date: 07/20/2023
+ms.date: 11/22/2023
 manager: nitinme
 ---
 



commit_patch_data :

From ccbe042065a37d2fafafcba7efc31f7f7a2dc175 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 10:19:33 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/concepts/models.md | 6 +++---
 articles/ai-services/openai/faq.yml            | 4 ++++
 2 files changed, 7 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/concepts/models.md b/articles/ai-services/openai/concepts/models.md
index 0da897bfe2f4..91810be64fad 100644
--- a/articles/ai-services/openai/concepts/models.md
+++ b/articles/ai-services/openai/concepts/models.md
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI
 description: Learn about the different model capabilities that are available with Azure OpenAI. 
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 11/17/2023
+ms.date: 11/22/2023
 ms.custom: event-tier1-build-2022, references_regions, build-2023, build-2023-dataai
 manager: nitinme
 author: mrbullwinkle #ChrisHMSFT
@@ -91,9 +91,9 @@ See [model versions](../concepts/model-versions.md) to learn about how Azure Ope
 | `gpt-4-32k`(0314)  | 32,768               | Sep 2021         |
 | `gpt-4` (0613)     | 8,192                | Sep 2021         |
 | `gpt-4-32k` (0613) | 32,768               | Sep 2021         |
-| `gpt-4` (1106-preview)**<sup>1</sup>** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
+| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
 
-**<sup>1</sup>** We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
+**<sup>1</sup>** GPT-4 Turbo = `gpt-4` (1106-preview). To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
 
 > [!NOTE]
 > Regions where GPT-4 (0314) & (0613) are listed as available have access to both the 8K and 32K versions of the model
diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index f6650e033a3b..6a965e722272 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -26,6 +26,10 @@ sections:
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
           Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+      - question: |
+          I can't find GPT-4 Turbo?
+        answer:
+          GPT-4 Turbo is the `gpt-4` (1106-preview) model. To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
       - question: |
           Does Azure OpenAI support GPT-4?
         answer: |



[æ¨¡å‹] GPT-4 Turboæ¨¡å‹çš„æè¿°å’Œéƒ¨ç½²æ–¹æ³•æ›´æ–°
2023-11-22 15:19:33
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models

è¿™ä¸ªæ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦åŒ…æ‹¬ä¸¤éƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæ›´æ–°äº†æ–‡ä»¶çš„æ—¥æœŸï¼Œä»2023å¹´11æœˆ17æ—¥æ›´æ”¹ä¸º2023å¹´11æœˆ22æ—¥ã€‚å…¶æ¬¡ï¼Œå¯¹æ¨¡å‹çš„æè¿°è¿›è¡Œäº†ä¿®æ”¹ï¼Œç‰¹åˆ«æ˜¯å¯¹`gpt-4` (1106-preview)æ¨¡å‹çš„æè¿°ï¼Œå¢åŠ äº†"GPT-4 Turbo"çš„æ ‡ç­¾ï¼Œå¹¶å¯¹å¦‚ä½•éƒ¨ç½²è¿™ä¸ªæ¨¡å‹çš„è¯´æ˜è¿›è¡Œäº†è¡¥å……ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œæ·»åŠ äº†ä¸€ä¸ªæ–°çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œé—®é¢˜æ˜¯"I can't find GPT-4 Turbo?"ï¼Œç­”æ¡ˆæ˜¯å…³äºå¦‚ä½•æ‰¾åˆ°å’Œéƒ¨ç½²GPT-4 Turboæ¨¡å‹çš„è¯´æ˜ã€‚

*************************

commit_patch_data :

From 38a2e4d833e65431251244bee7feb4757174638f Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 10:26:19 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/concepts/models.md | 4 ++--
 articles/ai-services/openai/faq.yml            | 4 ++--
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/articles/ai-services/openai/concepts/models.md b/articles/ai-services/openai/concepts/models.md
index 91810be64fad..0f054cb1d7fd 100644
--- a/articles/ai-services/openai/concepts/models.md
+++ b/articles/ai-services/openai/concepts/models.md
@@ -91,9 +91,9 @@ See [model versions](../concepts/model-versions.md) to learn about how Azure Ope
 | `gpt-4-32k`(0314)  | 32,768               | Sep 2021         |
 | `gpt-4` (0613)     | 8,192                | Sep 2021         |
 | `gpt-4-32k` (0613) | 32,768               | Sep 2021         |
-| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
+| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo Preview** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
 
-**<sup>1</sup>** GPT-4 Turbo = `gpt-4` (1106-preview). To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
+**<sup>1</sup>** GPT-4 Turbo Preview = `gpt-4` (1106-preview). To deploy this model, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
 
 > [!NOTE]
 > Regions where GPT-4 (0314) & (0613) are listed as available have access to both the 8K and 32K versions of the model
diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 6a965e722272..92a39d0394b9 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -27,9 +27,9 @@ sections:
         answer: |
           Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
-          I can't find GPT-4 Turbo?
+          I can't find GPT-4 Turbo Preview?
         answer:
-          GPT-4 Turbo is the `gpt-4` (1106-preview) model. To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
+          GPT-4 Turbo Preview is the `gpt-4` (1106-preview) model. To deploy this model, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
       - question: |
           Does Azure OpenAI support GPT-4?
         answer: |



[æ¨¡å‹] "GPT-4 Turbo"æ¨¡å‹åç§°åŠæè¿°çš„é‡è¦æ›´æ–°
2023-11-22 15:26:19
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models

è¿™ä¸ªæ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦æ˜¯å¯¹æ¨¡å‹åç§°çš„ä¿®æ”¹ï¼Œå°†"GPT-4 Turbo"æ›´æ”¹ä¸º"GPT-4 Turbo Preview"ã€‚åŒæ—¶ï¼Œå¯¹åº”çš„æè¿°ä¹Ÿè¿›è¡Œäº†ç›¸åº”çš„ä¿®æ”¹ï¼Œå¼ºè°ƒè¿™æ˜¯ä¸€ä¸ªé¢„è§ˆç‰ˆæ¨¡å‹ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œå¯¹äºFAQéƒ¨åˆ†çš„é—®é¢˜"I can't find GPT-4 Turbo?"çš„ç­”æ¡ˆä¹Ÿè¿›è¡Œäº†ä¿®æ”¹ï¼Œå°†"GPT-4 Turbo"æ›´æ”¹ä¸º"GPT-4 Turbo Preview"ï¼Œå¹¶å¯¹åº”ä¿®æ”¹äº†æ¨¡å‹çš„æè¿°ã€‚

*************************

commit_patch_data :

From c834902936f80682d16dc481ece7ebf78006d0b5 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:15:14 -0500
Subject: [PATCH] update

---
 .../openai/how-to/switching-endpoints.md      | 204 +-----------------
 1 file changed, 10 insertions(+), 194 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 8a8f70c55a10..90e3e9b09811 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,192 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-# [OpenAI Python 0.28.1](#tab/python)
-
-## Authentication
-
-We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
-
-### API key
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-
-openai.api_type = "azure"
-openai.api_key = "..."
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-<a name='azure-active-directory-authentication'></a>
-
-### Microsoft Entra authentication
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-from azure.identity import DefaultAzureCredential
-
-credential = DefaultAzureCredential()
-token = credential.get_token("https://cognitiveservices.azure.com/.default")
-
-openai.api_type = "azure_ad"
-openai.api_key = token.token
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-## Keyword argument for model
-
-OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
-
-For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    model="text-davinci-003"
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    model="gpt-4"
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  model="text-embedding-ada-002"
-)
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    deployment_id="text-davinci-003" # This must match the custom deployment name you chose for your model.
-    #engine="text-davinci-003" 
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    deployment_id="gpt-4" # This must match the custom deployment name you chose for your model.
-    #engine="gpt-4"
-
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-## Azure OpenAI embeddings multiple input support
-
-OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-inputs = ["A", "B", "C"] 
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  model="text-embedding-ada-002"
-)
-
-
-```
-
-</td>
-<td>
-
-```python
-inputs = ["A", "B", "C"] #max array size=16
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-# [OpenAI Python 1.x](#tab/python-new)
+This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -223,6 +38,7 @@ client = OpenAI(
 )
 
 
+
 ```
 
 </td>
@@ -266,6 +82,8 @@ client = OpenAI(
 
 
 
+
+
 ```
 
 </td>
@@ -310,7 +128,7 @@ completion = client.completions.create(
     prompt="<prompt>)
 )
 
-chat_completion = openai.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-4",
     messages="<messages>"
 )
@@ -325,17 +143,17 @@ embedding = client.embeddings.create(
 <td>
 
 ```python
-client.completions.create(
+completion = client.completions.create(
     model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
     prompt=<"prompt">
 )
 
-client.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-35-turbo", # model = "deployment_name".
     messages=<"messages">
 )
 
-client.embeddings.create(
+embedding = client.embeddings.create(
     input = "<input>",
     model= "text-embedding-ada-002" # model = "deployment_name".
 )
@@ -364,7 +182,6 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
-
 ```
 
 </td>
@@ -378,15 +195,14 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
   #engine="text-embedding-ada-002"
 )
+
 ```
 
 </td>
 </tr>
 </table>
 
----
-
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file



[æ¨¡å‹] OpenAIå’ŒAzure OpenAIåˆ‡æ¢æ–¹æ³•çš„é‡å¤§æ›´æ–°
2023-11-22 16:15:14
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

æ­¤æ¬¡æäº¤å¯¹æ–‡ä»¶è¿›è¡Œäº†å¤§é‡çš„ä¿®æ”¹ï¼Œåˆ é™¤äº†194è¡Œï¼Œæ·»åŠ äº†10è¡Œã€‚ä¸»è¦çš„æ›´æ”¹åŒ…æ‹¬åˆ é™¤äº†å¤§é‡çš„Pythonä»£ç ç¤ºä¾‹å’Œç›¸å…³çš„è¯´æ˜ï¼Œè¿™äº›ä»£ç ç¤ºä¾‹ä¸»è¦æ˜¯å…³äºå¦‚ä½•åœ¨OpenAIå’ŒAzure OpenAIä¹‹é—´åˆ‡æ¢ã€‚åŒæ—¶ï¼Œä¹Ÿå¯¹ä¸€äº›ç°æœ‰çš„ä»£ç è¿›è¡Œäº†ä¿®æ”¹ï¼Œä¾‹å¦‚å°†ä¸€äº›å‡½æ•°çš„è°ƒç”¨æ–¹å¼è¿›è¡Œäº†æ›´æ”¹ã€‚æ­¤å¤–ï¼Œè¿˜æ›´æ–°äº†ä¸€äº›æ–‡æ¡£çš„é“¾æ¥ï¼Œå¹¶åœ¨æ–‡æ¡£çš„æœ€åæ·»åŠ äº†ä¸€è¡Œæ–°çš„å†…å®¹ã€‚

*************************

commit_patch_data :

From 659bbf38c4b50ecb1aeedf80987724f50d30a389 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:17:59 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 90e3e9b09811..1b74c13e916a 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -2,8 +2,8 @@
 title: How to switch between OpenAI and Azure OpenAI Service endpoints with Python
 titleSuffix: Azure OpenAI Service
 description: Learn about the changes you need to make to your code to swap back and forth between OpenAI and Azure OpenAI endpoints.
-author: mrbullwinkle #dereklegenzoff
-ms.author: mbullwin #delegenz
+author: mrbullwinkle 
+ms.author: mbullwin 
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to



commit_patch_data :

From 85d411bb59d2a61a3c28de546c3697928ccf898b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:29:49 -0500
Subject: [PATCH] updatE

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 1 +
 1 file changed, 1 insertion(+)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 1b74c13e916a..bcaae948c9cb 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -182,6 +182,7 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
+
 ```
 
 </td>



commit_patch_data :

From 77b1b0cdf7e04579ec6146b5b3473b511a32cc2a Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 12:36:48 -0500
Subject: [PATCH 1/7] update

---
 articles/ai-services/openai/how-to/latency.md | 133 ++++++++++++++++++
 articles/ai-services/openai/toc.yml           |   2 +
 2 files changed, 135 insertions(+)
 create mode 100644 articles/ai-services/openai/how-to/latency.md

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
new file mode 100644
index 000000000000..8fe1c252e20a
--- /dev/null
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -0,0 +1,133 @@
+---
+title: Azure OpenAI Service performance & latency
+titleSuffix: Azure OpenAI
+description: Learn about performance and latency with Azure OpenAI
+services: cognitive-services
+manager: nitinme
+ms.service: cognitive-services
+ms.subservice: openai
+ms.topic: how-to
+ms.date: 11/21/2023
+author: mrbullwinkle 
+ms.author: mbullwin
+recommendations: false
+ms.custom:
+---
+
+# Performance and latency
+
+This article will provide you with background around how latency works with Azure OpenAI and how to optimize your environment to improve performance.
+
+## What is latency?
+
+The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned.The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
+
+## Completions and chat completions
+
+### Model selection
+
+Latency varies based on what model you are using. For an identical request, it is expected that `gpt-35-turbo` models will have a faster response, and therefore less latency than the first generation of `gpt-4` models.
+
+|Model|Relative Latency (fastestğŸš€ to slowestğŸ¢)|
+|---|:---|
+|---|---|
+|`babbage-002` (fine-tuned)|ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€  |
+| `gpt-35-turbo` (1106) |  ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€   |
+| `gpt-4` (1106-preview) | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ |
+| `gpt-35-turbo-instruct`| ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€  |
+| `gpt-35-turbo` (0314-0613) |ğŸš€ğŸš€ğŸš€ğŸš€  |
+| `davinci-002`(fine-tuned) |ğŸš€ğŸš€ğŸš€ 
+| `gpt-35-turbo-16k` | ğŸš€ğŸš€ğŸš€ |
+| `gpt-4` (0314-0613) | ğŸš€ğŸš€ |
+| `gpt-4-32k` (0314-0613) |ğŸ¢ğŸš€ |
+
+If you are concerned about latency model selection is extremely important. You need to balance your needs around model capabilities, quality of response, cost, and overall latency. Depending on your use case a smaller well trained fine-tuned model like `babbage-002` may be able to offer the best overall experience. But creating a high quality fine-tuned model comes with additional costs as well as complexity.  
+
+<!--
+Questions:
+1. Can we provide an ordered list of models from fastest to slowest from a latency perspective? 
+2. Is it possible to have noticable regional differences between model latency in the sense of in Region A model is running on A100s, whereas in Region B the identical model is running on H100's. Is each class of model always running on the same speed underlying hardware? 
+ -->
+
+### Max tokens and stop sequences
+
+When you send a completion request to the Azure OpenAI endpoint your input text is converted to tokens which are then sent to your deployed model. The model receives the input tokens and then begins generating a response. It's an iterative sequential process, one token at a time. Another way to think of it is like a for loop with `n tokens = n iterations`.
+
+So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter as well as with `stop` sequences. Reducing the number of tokens generated per request will reduce the latency of each request.
+<!--
+Question:
+1. What is the default max_token value for each model spec says 16 is default but does this vary?
+ -->
+
+### Streaming
+
+Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
+
+If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
+
+### Content filtering
+
+[Content filtering](../concepts/content-filter.md) and other content safety related features within Azure OpenAI increase latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where modifying the content filters to improve performance might be worth exploring.
+
+Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
+
+<!--
+Question:
+
+1. I have heard about deferred safety in shiproom? (Is this released for Azure OpenAI 3P, how do customers access this?)
+2. If I set content filtering to high only does this improve latency or is this ultimately the same whereby latency is only improved if content filtering is off altogether or deferred safety is used?
+ -->
+
+### best_of & n parameters
+
+**best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs. Setting these parameters to values greater than 1, creates increased latency.
+
+### Quota and rate limits
+
+[Quota and rate limits](./quota.md) can play a huge role in the overall request latency you experience.
+
+If you have insufficient [quota](./quota.md) to meet your peak usage requirements this creates a situation where it appears that the model is taking longer to respond, when in reality the model response time itself is remaining the same, but some percentage of calls to the model are not being delivered and are then retried until your traffic drops back below the rate limit thresholds.
+
+The default behavior for the `0.28.1` OpenAI Python API library was to generate a failure message when the rate limit was reached. Whereas the new `1.x` Python API library, and the Azure OpenAI SDK's for C#, Java, JavaScript, and Go, will initially all fail silently when `HTTP 429 (Too many requests)` errors occur and then retry sending the request multiple times.
+
+This means that if you are measuring overall latency of your Azure OpenAI requests, but not also measuring how many `HTTP 429`'s are occuring you are getting an incomplete picture of what is creating latency in your application.
+
+<!--
+Question:
+1. I was told by Mona don't use blocked calls this metric is inaccurate, but when I test blocked calls is the only thing recording 429's
+2. New Metric is not recording 429's?
+ -->
+
+If you find that maxing out standard quota is not sufficient for your performance needs, you may need to explore [provisioned throughout](../concepts/provisioned-throughput.md) which provides a more stable latency experience.
+
+### Regional availabilty
+
+Azure OpenAI models are available in datacenters around the world. While the geographic distance between client and endpoint is not a major contributor to latency different regions have different max quota allocations available and so it may be possible to [reduce latency by migrating to a different region](../quotas-limits.md).
+
+## Summary
+
+* **Model latency**: Some models are faster than others. Experiment with using different models to see if a faster model is appropriate for your use case.
+
+* **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
+
+* **Lower total tokens generated**: The fewer tokens generated the faster the overall response will be. Remember this is like having a for loop with `n tokens = n iterations`. Lower the number of tokens generated and overall response time will improve accordingly.
+
+* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready. While the last token will still take just as long to be generated.
+
+* **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
+
+* **Content Filtering** impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
+
+* **best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs, which occurs when these parameters are set to values greater than 1.
+
+* **Audit blocked call** frequency in **Metrics**. Split by **RatelimitKey** and **Region** to identify specific model deployments that are impacted.
+
+* **Configure [Diagnostic Settings](/azure/ai-services/openai/how-to/monitoring#configure-diagnostic-settings)** across your Azure OpenAI resources to enable consolidated in-depth reporting with Log Analytics workspaces.
+
+* **Use [quota management](/azure/ai-services/openai/how-to/quota?tabs=rest)** to increase TPM on deployments with high traffic, and to reduce TPM on deployments with limited needs.
+
+* **Implement retry logic in your application** if it is not already present. But when diagnosing performance issues, you should also take into account your client SDK's currently configured retry behavior as it may improve the user experience while masking an underlying rate limit issue.
+
+* **Avoid sharp changes in the workload**. Increase the workload gradually.
+
+* **Test different load increase patterns**.
diff --git a/articles/ai-services/openai/toc.yml b/articles/ai-services/openai/toc.yml
index bf4b71f6c106..94128fe2bd6d 100644
--- a/articles/ai-services/openai/toc.yml
+++ b/articles/ai-services/openai/toc.yml
@@ -116,6 +116,8 @@ items:
         href: ./how-to/monitoring.md
       - name: Plan and manage costs
         href: ./how-to/manage-costs.md
+      - name: Performance & latency
+        href: ./how-to/latency.md
       - name: Role-based access control (Azure RBAC)
         href: ./how-to/role-based-access-control.md
       - name: Business continuity & disaster recovery (BCDR)

From a3de2323583a42e55274c37ddd94b3b8022f0c6a Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 13:28:10 -0500
Subject: [PATCH 2/7] update

---
 articles/ai-services/openai/how-to/latency.md | 1 -
 1 file changed, 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 8fe1c252e20a..30d646c589e0 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -30,7 +30,6 @@ Latency varies based on what model you are using. For an identical request, it i
 
 |Model|Relative Latency (fastestğŸš€ to slowestğŸ¢)|
 |---|:---|
-|---|---|
 |`babbage-002` (fine-tuned)|ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€  |
 | `gpt-35-turbo` (1106) |  ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€   |
 | `gpt-4` (1106-preview) | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ |

From 1f70420967c6d85aef958f093a372a0c0ca0a876 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 13:30:13 -0500
Subject: [PATCH 3/7] update

---
 articles/ai-services/openai/how-to/latency.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 30d646c589e0..6ff9f20df36f 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -111,7 +111,7 @@ Azure OpenAI models are available in datacenters around the world. While the geo
 
 * **Lower total tokens generated**: The fewer tokens generated the faster the overall response will be. Remember this is like having a for loop with `n tokens = n iterations`. Lower the number of tokens generated and overall response time will improve accordingly.
 
-* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready. While the last token will still take just as long to be generated.
+* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready.
 
 * **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
 

From d8998cf0565f13534e0a60084737e208d4172a21 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 16:59:42 -0500
Subject: [PATCH 4/7] update

---
 articles/ai-services/openai/how-to/latency.md | 97 ++++---------------
 1 file changed, 17 insertions(+), 80 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 6ff9f20df36f..88c30017ae26 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -2,10 +2,8 @@
 title: Azure OpenAI Service performance & latency
 titleSuffix: Azure OpenAI
 description: Learn about performance and latency with Azure OpenAI
-services: cognitive-services
 manager: nitinme
-ms.service: cognitive-services
-ms.subservice: openai
+ms.service: azure-ai-openai
 ms.topic: how-to
 ms.date: 11/21/2023
 author: mrbullwinkle 
@@ -26,86 +24,41 @@ The high level definition of latency in this context is the amount of time it ta
 
 ### Model selection
 
-Latency varies based on what model you are using. For an identical request, it is expected that `gpt-35-turbo` models will have a faster response, and therefore less latency than the first generation of `gpt-4` models.
+Latency varies based on what model you are using. For an identical request, it is expected that different models will have a different latency. If your use case requires the lowest latency models with the fastest response times we recommend the latest models in the [GPT-3.5 Turbo model series](../concepts/models.md#gpt-35-models).
 
-|Model|Relative Latency (fastestğŸš€ to slowestğŸ¢)|
-|---|:---|
-|`babbage-002` (fine-tuned)|ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€  |
-| `gpt-35-turbo` (1106) |  ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€   |
-| `gpt-4` (1106-preview) | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ |
-| `gpt-35-turbo-instruct`| ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€  |
-| `gpt-35-turbo` (0314-0613) |ğŸš€ğŸš€ğŸš€ğŸš€  |
-| `davinci-002`(fine-tuned) |ğŸš€ğŸš€ğŸš€ 
-| `gpt-35-turbo-16k` | ğŸš€ğŸš€ğŸš€ |
-| `gpt-4` (0314-0613) | ğŸš€ğŸš€ |
-| `gpt-4-32k` (0314-0613) |ğŸ¢ğŸš€ |
-
-If you are concerned about latency model selection is extremely important. You need to balance your needs around model capabilities, quality of response, cost, and overall latency. Depending on your use case a smaller well trained fine-tuned model like `babbage-002` may be able to offer the best overall experience. But creating a high quality fine-tuned model comes with additional costs as well as complexity.  
-
-<!--
-Questions:
-1. Can we provide an ordered list of models from fastest to slowest from a latency perspective? 
-2. Is it possible to have noticable regional differences between model latency in the sense of in Region A model is running on A100s, whereas in Region B the identical model is running on H100's. Is each class of model always running on the same speed underlying hardware? 
- -->
-
-### Max tokens and stop sequences
+### Max tokens
 
 When you send a completion request to the Azure OpenAI endpoint your input text is converted to tokens which are then sent to your deployed model. The model receives the input tokens and then begins generating a response. It's an iterative sequential process, one token at a time. Another way to think of it is like a for loop with `n tokens = n iterations`.
 
-So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter as well as with `stop` sequences. Reducing the number of tokens generated per request will reduce the latency of each request.
-<!--
-Question:
-1. What is the default max_token value for each model spec says 16 is default but does this vary?
- -->
+So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter. Reducing the number of tokens generated per request will reduce the latency of each request.
 
 ### Streaming
 
-Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
-
-If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
-
-### Content filtering
-
-[Content filtering](../concepts/content-filter.md) and other content safety related features within Azure OpenAI increase latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where modifying the content filters to improve performance might be worth exploring.
-
-Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
-
-<!--
-Question:
-
-1. I have heard about deferred safety in shiproom? (Is this released for Azure OpenAI 3P, how do customers access this?)
-2. If I set content filtering to high only does this improve latency or is this ultimately the same whereby latency is only improved if content filtering is off altogether or deferred safety is used?
- -->
+**Examples when to use streaming**:
 
-### best_of & n parameters
+Chat bots and conversational interfaces.
 
-**best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs. Setting these parameters to values greater than 1, creates increased latency.
-
-### Quota and rate limits
+Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
 
-[Quota and rate limits](./quota.md) can play a huge role in the overall request latency you experience.
+**Examples when streaming is less important**:
 
-If you have insufficient [quota](./quota.md) to meet your peak usage requirements this creates a situation where it appears that the model is taking longer to respond, when in reality the model response time itself is remaining the same, but some percentage of calls to the model are not being delivered and are then retried until your traffic drops back below the rate limit thresholds.
+Sentiment analysis, language translation, content generation.
 
-The default behavior for the `0.28.1` OpenAI Python API library was to generate a failure message when the rate limit was reached. Whereas the new `1.x` Python API library, and the Azure OpenAI SDK's for C#, Java, JavaScript, and Go, will initially all fail silently when `HTTP 429 (Too many requests)` errors occur and then retry sending the request multiple times.
+There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
 
-This means that if you are measuring overall latency of your Azure OpenAI requests, but not also measuring how many `HTTP 429`'s are occuring you are getting an incomplete picture of what is creating latency in your application.
+### Content filtering
 
-<!--
-Question:
-1. I was told by Mona don't use blocked calls this metric is inaccurate, but when I test blocked calls is the only thing recording 429's
-2. New Metric is not recording 429's?
- -->
+Azure OpenAI includes a [content filtering system](./content-filters.md) that works alongside core models. This system works by running both the prompt and completion through an ensemble of classification models aimed at detecting and preventing the output of harmful content.
 
-If you find that maxing out standard quota is not sufficient for your performance needs, you may need to explore [provisioned throughout](../concepts/provisioned-throughput.md) which provides a more stable latency experience.
+The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
 
-### Regional availabilty
+The addition of content filtering comes with an increase in safety, but also latency.There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
 
-Azure OpenAI models are available in datacenters around the world. While the geographic distance between client and endpoint is not a major contributor to latency different regions have different max quota allocations available and so it may be possible to [reduce latency by migrating to a different region](../quotas-limits.md).
+Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
 
 ## Summary
 
-* **Model latency**: Some models are faster than others. Experiment with using different models to see if a faster model is appropriate for your use case.
+* **Model latency**: If model latency is important to you we recommend trying out our latest latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
 
 * **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
 
@@ -113,20 +66,4 @@ Azure OpenAI models are available in datacenters around the world. While the geo
 
 * **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready.
 
-* **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
-
-* **Content Filtering** impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
-
-* **best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs, which occurs when these parameters are set to values greater than 1.
-
-* **Audit blocked call** frequency in **Metrics**. Split by **RatelimitKey** and **Region** to identify specific model deployments that are impacted.
-
-* **Configure [Diagnostic Settings](/azure/ai-services/openai/how-to/monitoring#configure-diagnostic-settings)** across your Azure OpenAI resources to enable consolidated in-depth reporting with Log Analytics workspaces.
-
-* **Use [quota management](/azure/ai-services/openai/how-to/quota?tabs=rest)** to increase TPM on deployments with high traffic, and to reduce TPM on deployments with limited needs.
-
-* **Implement retry logic in your application** if it is not already present. But when diagnosing performance issues, you should also take into account your client SDK's currently configured retry behavior as it may improve the user experience while masking an underlying rate limit issue.
-
-* **Avoid sharp changes in the workload**. Increase the workload gradually.
-
-* **Test different load increase patterns**.
+* **Content Filtering** improves safety, but it also impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
\ No newline at end of file

From 5b715be93bc622195950b683fa1a04e66d98a55b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 17:07:32 -0500
Subject: [PATCH 5/7] update

---
 articles/ai-services/openai/how-to/latency.md | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 88c30017ae26..9c2b054b2b9f 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -18,7 +18,7 @@ This article will provide you with background around how latency works with Azur
 
 ## What is latency?
 
-The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned.The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
+The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned. The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
 
 ## Completions and chat completions
 
@@ -34,13 +34,13 @@ So another important factor when evaluating latency is how many tokens are being
 
 ### Streaming
 
-**Examples when to use streaming**:
+**Examples of when to use streaming**:
 
 Chat bots and conversational interfaces.
 
-Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
+Streaming impacts perceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective, this often feels like the model is responding faster even though the overall time to complete the request remains the same.
 
-**Examples when streaming is less important**:
+**Examples of when streaming is less important**:
 
 Sentiment analysis, language translation, content generation.
 
@@ -52,13 +52,13 @@ Azure OpenAI includes a [content filtering system](./content-filters.md) that wo
 
 The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
 
-The addition of content filtering comes with an increase in safety, but also latency.There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
+The addition of content filtering comes with an increase in safety, but also latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
 
 Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
 
 ## Summary
 
-* **Model latency**: If model latency is important to you we recommend trying out our latest latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
+* **Model latency**: If model latency is important to you we recommend trying out our latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
 
 * **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
 

From e0abbb3a8d2dd05731b5082fc589521ce312da95 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 17:17:41 -0500
Subject: [PATCH 6/7] update

---
 articles/ai-services/openai/how-to/latency.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 9c2b054b2b9f..6461e21593c7 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -48,7 +48,7 @@ There are many use cases where you are performing some bulk task where you only
 
 ### Content filtering
 
-Azure OpenAI includes a [content filtering system](./content-filters.md) that works alongside core 


[æ¨¡å‹] "Azure OpenAIæ€§èƒ½å’Œå»¶è¿Ÿé—®é¢˜çš„å¤§å¹…åº¦ä¿®æ”¹å’Œä¼˜åŒ–"
2023-11-22 17:33:26
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤åˆ›å»ºäº†ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶è¯¦ç»†ä»‹ç»äº†Azure OpenAIçš„æ€§èƒ½å’Œå»¶è¿Ÿé—®é¢˜ã€‚å†…å®¹åŒ…æ‹¬å»¶è¿Ÿçš„å®šä¹‰ï¼Œä»¥åŠå½±å“å»¶è¿Ÿçš„å„ç§å› ç´ ï¼Œå¦‚æ¨¡å‹é€‰æ‹©ï¼Œæœ€å¤§ä»¤ç‰Œæ•°ï¼Œæµå¼ä¼ è¾“ï¼Œå†…å®¹è¿‡æ»¤ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€äº›ä¼˜åŒ–æ€§èƒ½çš„å»ºè®®ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/toc

æ­¤æäº¤åœ¨ç›®å½•æ–‡ä»¶ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„æ¡ç›®ï¼Œé“¾æ¥åˆ°æ–°åˆ›å»ºçš„å…³äºæ€§èƒ½å’Œå»¶è¿Ÿçš„æ–‡ç« ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤åˆ é™¤äº†ä¸€ä¸ªå¤šä½™çš„è¡¨æ ¼è¡Œã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤ä¿®æ”¹äº†å…³äºæµå¼ä¼ è¾“çš„æè¿°ï¼Œåˆ é™¤äº†â€œå°½ç®¡æœ€åä¸€ä¸ªä»¤ç‰Œçš„ç”Ÿæˆæ—¶é—´ä»ç„¶ä¸€æ ·é•¿â€çš„éƒ¨åˆ†ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤å¤§å¹…åº¦ä¿®æ”¹äº†æ–‡ä»¶ï¼Œåˆ é™¤äº†å¤§é‡å†…å®¹ï¼ŒåŒ…æ‹¬å…³äºæ¨¡å‹é€‰æ‹©ï¼Œæœ€å¤§ä»¤ç‰Œå’Œåœæ­¢åºåˆ—ï¼Œå†…å®¹è¿‡æ»¤ï¼Œæœ€ä½³å‚æ•°å’Œnå‚æ•°ï¼Œé…é¢å’Œé€Ÿç‡é™åˆ¶ï¼ŒåŒºåŸŸå¯ç”¨æ€§ç­‰çš„è¯¦ç»†è®¨è®ºã€‚åŒæ—¶ï¼Œå¯¹ä¸€äº›éƒ¨åˆ†è¿›è¡Œäº†ç®€åŒ–å’Œæ¦‚æ‹¬ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤å¯¹æ–‡ä»¶è¿›è¡Œäº†ä¸€äº›å°çš„ç¼–è¾‘å’Œä¿®è®¢ï¼ŒåŒ…æ‹¬ä¿®æ­£äº†ä¸€äº›è¯­æ³•é”™è¯¯ï¼Œæ”¹è¿›äº†ä¸€äº›è¡¨è¿°ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

æ­¤æäº¤å¯¹æ–‡ä»¶è¿›è¡Œäº†ä¸€äº›å°çš„ç¼–è¾‘å’Œä¿®è®¢ï¼ŒåŒ…æ‹¬ä¿®æ­£äº†ä¸€äº›è¯­æ³•é”™è¯¯ï¼Œæ”¹è¿›äº†ä¸€äº›è¡¨è¿°ã€‚

*************************

commit_patch_data :

From 0dc8c41ea82cd1641fe65dd5f6e444d7a60f7b48 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Jo=C3=A3o=20Tavares?=
 <60930147+jotavar@users.noreply.github.com>
Date: Wed, 8 Nov 2023 11:09:10 +0000
Subject: [PATCH 01/63] Added alert regarding recommended minimum number of
 CoreDNS pod replicas

---
 articles/aks/coredns-custom.md | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/articles/aks/coredns-custom.md b/articles/aks/coredns-custom.md
index 4c773ab8df11..eeaa5ccbd2a8 100644
--- a/articles/aks/coredns-custom.md
+++ b/articles/aks/coredns-custom.md
@@ -224,6 +224,9 @@ Sudden spikes in DNS traffic within AKS clusters are a common occurrence due to
 
 CoreDNS uses [horizontal cluster proportional autoscaler][cluster-proportional-autoscaler] for pod auto scaling. The `coredns-autoscaler` ConfigMap can be edited to configure the scaling logic for the number of CoreDNS pods. The `coredns-autoscaler` ConfigMap currently supports two different ConfigMap key values: `linear` and `ladder` which correspond to two supported control modes. The `linear` controller yields a number of replicas in [min,max] range equivalent to `max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )`. The `ladder` controller calculates the number of replicas by consulting two different step functions, one for core scaling and another for node scaling, yielding the max of the two replica values. For more information on the control modes and ConfigMap format, please consult the [upstream documentation][cluster-proportional-autoscaler-control-patterns].
 
+> [!IMPORTANT]
+> A minimum of 2 CoreDNS pod replicas per cluster is recommended. Configuring a minimum of 1 CoreDNS pod replica may result in failures during operations which require node draining, such as cluster upgrade operations.
+
 To retrieve the `coredns-autoscaler` ConfigMap, you can run the `kubectl get configmap coredns-autoscaler -n kube-system -o yaml` command which will return the following:
 
 ```yaml

From d6763759429288dd3e4378cb95d91ed04b7fe5cc Mon Sep 17 00:00:00 2001
From: Gearoid O'Donnell <110535959+gearoidodonnell@users.noreply.github.com>
Date: Wed, 8 Nov 2023 18:16:45 +0000
Subject: [PATCH 02/63] Updated overview.md

---
 articles/active-directory-b2c/overview.md | 22 ++++++++++------------
 1 file changed, 10 insertions(+), 12 deletions(-)

diff --git a/articles/active-directory-b2c/overview.md b/articles/active-directory-b2c/overview.md
index f526c1c2b515..39d0744c5734 100644
--- a/articles/active-directory-b2c/overview.md
+++ b/articles/active-directory-b2c/overview.md
@@ -1,10 +1,8 @@
 ---
 title: What is Azure Active Directory B2C?
 description: Learn how you can use Azure Active Directory B2C to support external identities in your applications, including social sign-up with Facebook, Google, and other identity providers.
-services: active-directory-b2c
 author: garrodonnell
 manager: CelesteDG
-
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
@@ -12,28 +10,31 @@ ms.date: 10/26/2022
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
+
+# Customer intent: As an IT admin or developer, I need to understand what Azure AD B2C is and how it can help me build a customer-facing application.
+
 ---
 
 # What is Azure Active Directory B2C?
 
-Azure Active Directory B2C provides business-to-customer identity as a service. Your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.
+Azure Active Directory B2C provides business-to-customer identity as a service. Your customers can use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.
 
 ![Infographic of Azure AD B2C identity providers and downstream applications](./media/overview/azureadb2c-overview.png)
 
 Azure AD B2C is a customer identity access management (CIAM) solution capable of supporting millions of users and billions of authentications per day. It takes care of the scaling and safety of the authentication platform, monitoring, and automatically handling threats like denial-of-service, password spray, or brute force attacks.
 
-Azure AD B2C is a separate service from [Microsoft Entra ID](../active-directory/fundamentals/whatis.md). It is built on the same technology as Microsoft Entra ID but for a different purpose. It allows businesses to build customer facing applications, and then allow anyone to sign-up and into those applications with no restrictions on user account.
+Azure AD B2C is built on the same technology as [Microsoft Entra ID](../active-directory/fundamentals/whatis.md) but for a different purpose and is a separate service. It allows businesses to build customer facing applications, and then allow anyone to sign up and sign in to those applications with no restrictions on user account.
    
 ## Who uses Azure AD B2C?
-Any business or individual who wishes to authenticate end users to their web/mobile applications using a white-label authentication solution. Apart from authentication, Azure AD B2C service is used for authorization such as access to API resources by authenticated users. Azure AD B2C is designed to be used by **IT administrators** and **developers**.
+Any business or individual who wishes to authenticate end users to their web or mobile applications using a white-label authentication solution. Apart from authentication, Azure AD B2C service is used for authorization such as access to API resources by authenticated users. Azure AD B2C is designed to be used by **IT administrators** and **developers**.
 
 ## Custom-branded identity solution
 
-Azure AD B2C is a white-label authentication solution. You can customize the entire user experience with your brand so that it blends seamlessly with your web and mobile applications.
+Azure AD B2C is a white-label authentication solution which means you can customize the entire user experience with your brand so that it blends seamlessly with your web and mobile applications.
 
-Customize every page displayed by Azure AD B2C when your users sign-up, sign in, and modify their profile information. Customize the HTML, CSS, and JavaScript in your user journeys so that the Azure AD B2C experience looks and feels like it's a native part of your application.
+Customize every page displayed by Azure AD B2C when your users sign up, sign in, and modify their profile information. Customize the HTML, CSS, and JavaScript in your user journeys so that the Azure AD B2C experience looks and feels like it's a native part of your application.
 
-![Customized sign-up and sign-in pages and background image](./media/overview/sign-in-small.png)
+:::image type="content" source="./media/overview/sign-in-small.png" alt-text="Customized sign-up and sign-in pages and background image":::
 
 ## Single sign-on access with a user-provided identity
 
@@ -51,7 +52,7 @@ Another external user store scenario is to have Azure AD B2C handle the authenti
 
 :::image type="content" source="./media/overview/scenario-remoteprofile.png" alt-text="A logical diagram of Azure AD B2C communicating with an external user store.":::
 
-Azure AD B2C can facilitate collecting the information from the user during registration or profile editing, then hand that data off to the external system via API. Then, during future authentications, Azure AD B2C can retrieve the data from the external system and, if needed, include it as a part of the authentication token response it sends to your application.
+Azure AD B2C can facilitate collecting information from a user during registration or profile editing, then hand that data off to an external system via API. Then, during future authentications, Azure AD B2C can retrieve that data from the external system and, if needed, include it as a part of the authentication token response it sends to your application.
 
 ## Progressive profiling
 
@@ -63,11 +64,8 @@ Another user journey option includes progressive profiling. Progressive profilin
 
 Use Azure AD B2C to facilitate identity verification and proofing by collecting user data, then passing it to a third-party system to perform validation, trust scoring, and approval for user account creation.
 
-
 :::image type="content" source="./media/overview/scenario-idproofing.png" alt-text="A diagram showing the user flow for third-party identity proofing.":::
 
-You have learned some of the things you can do with Azure AD B2C as your business-to-customer identity platform. You may now move on directly to a more in-depth [technical overview of Azure AD B2C](technical-overview.md).
-
 ## Next steps
 
 Now that you have an idea of what Azure AD B2C is and some of the scenarios it can help with, dig a little deeper into its features and technical aspects.

From 3ca025af661ba5cf29ebaa126c9bfafc877a2cbd Mon Sep 17 00:00:00 2001
From: Gearoid O'Donnell <110535959+gearoidodonnell@users.noreply.github.com>
Date: Thu, 9 Nov 2023 08:42:23 +0000
Subject: [PATCH 03/63] Freshness review for technical-overview.md

---
 articles/active-directory-b2c/overview.md     |  4 +-
 .../technical-overview.md                     | 75 +++++++++----------
 2 files changed, 37 insertions(+), 42 deletions(-)

diff --git a/articles/active-directory-b2c/overview.md b/articles/active-directory-b2c/overview.md
index 39d0744c5734..c2e2eb629137 100644
--- a/articles/active-directory-b2c/overview.md
+++ b/articles/active-directory-b2c/overview.md
@@ -6,12 +6,12 @@ manager: CelesteDG
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
-ms.date: 10/26/2022
+ms.date: 08/11/2023
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
 
-# Customer intent: As an IT admin or developer, I need to understand what Azure AD B2C is and how it can help me build a customer-facing application.
+# Customer intent: As a technical or non-technical customer, I need to understand at a high level what Azure AD B2C is and how it can help me build a customer-facing application.
 
 ---
 
diff --git a/articles/active-directory-b2c/technical-overview.md b/articles/active-directory-b2c/technical-overview.md
index 25a38654baee..21e8dc2bae39 100644
--- a/articles/active-directory-b2c/technical-overview.md
+++ b/articles/active-directory-b2c/technical-overview.md
@@ -1,38 +1,39 @@
 ---
 title: Technical and feature overview - Azure Active Directory B2C
 description: An in-depth introduction to the features and technologies in Azure Active Directory B2C. Azure Active Directory B2C has high availability globally. 
-services: active-directory-b2c
 author: garrodonnell
 manager: CelesteDG
-
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
-ms.date: 10/26/2022
+ms.date: 11/08/2023
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
+
+# Customer intent: As an IT admin or developer, I need to understand in more detail the technical aspects and features of Azure AD B2C and how it can help me build a customer-facing application.
+
 ---
 
 # Technical and feature overview of Azure Active Directory B2C
 
-A companion to [About Azure Active Directory B2C](overview.md), this article provides a more in-depth introduction to the service. Discussed here are the primary resources you work with in the service, its features. Learn how these features enable you to provide a fully custom identity experience for your customers in your applications.
+This article is a companion to [About Azure Active Directory B2C](overview.md) and provides a more in-depth introduction to the service. We will discuss here the primary resources you work with in the service, its features and learn how they enable you to provide a fully custom identity experience for customers in your applications.
 
 ## Azure AD B2C tenant
 
-In Azure Active Directory B2C (Azure AD B2C), a *tenant* represents your organization and is a directory of users. Each Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants. An Azure AD B2C tenant is different than a Microsoft Entra tenant, which you may already have.
+In Azure Active Directory B2C (Azure AD B2C), a *tenant* represents your organization and is a directory of users. Each Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants. An Azure AD B2C tenant is also different from a Microsoft Entra tenant, which you may already have.
 
 The primary resources you work with in an Azure AD B2C tenant are:
 
-* **Directory** - The *directory* is where Azure AD B2C stores your users' credentials, profile data, and your application registrations.
-* **Application registrations** - Register your web, mobile, and native applications with Azure AD B2C to enable identity management. You can also register any APIs you want to protect with Azure AD B2C.
-* **User flows** and **custom policies** - Create identity experiences for your applications with built-in user flows and fully configurable custom policies:
+* **Directory** - This is where Azure AD B2C stores your users' credentials, profile data, and your application registrations.
+* **Application registrations** - You can register your web, mobile, and native applications with Azure AD B2C to enable identity management. You can also register any APIs you want to protect with Azure AD B2C.
+* **User flows** and **custom policies** - These are used to create identity experiences for your applications with built-in user flows and fully configurable custom policies:
   * **User flows** help you quickly enable common identity tasks like sign-up, sign-in, and profile editing.
   * **Custom policies** let you build complex identity workflows unique to your organization, customers, employees, partners, and citizens.
 * **Sign-in options** - Azure AD B2C offers various [sign-up and sign-in options](sign-in-options.md) for users of your applications:
-  * **Username, email, and phone sign-in** - Configure your Azure AD B2C local accounts to allow sign-up and sign-in with a username, email address, phone number, or a combination of methods.
-  * **Social identity providers** - Federate with social providers like Facebook, LinkedIn, or Twitter.
-  * **External identity providers** - Federate with standard identity protocols like OAuth 2.0, OpenID Connect, and more.
+  * **Username, email, and phone sign-in** - You can configure your Azure AD B2C local accounts to allow sign up and sign in with a username, email address, phone number, or a combination of methods.
+  * **Social identity providers** - You can federate with social providers like Facebook, LinkedIn, or Twitter.
+  * **External identity providers** - You can also federate with standard identity protocols like OAuth 2.0, OpenID Connect, and more.
 * **Keys** - Add and manage encryption keys for signing and validating tokens, client secrets, certificates, and passwords.
 
 An Azure AD B2C tenant is the first resource you need to create to get started with Azure AD B2C. Learn how to:
@@ -45,8 +46,8 @@ An Azure AD B2C tenant is the first resource you need to create to get started w
 Azure AD B2C defines several types of user accounts. Microsoft Entra ID, Microsoft Entra B2B, and Azure Active Directory B2C share these account types.
 
 * **Work account** - Users with work accounts can manage resources in a tenant, and with an administrator role, can also manage tenants. Users with work accounts can create new consumer accounts, reset passwords, block/unblock accounts, and set permissions or assign an account to a security group.
-* **Guest account** - External users you invite to your tenant as guests. A typical scenario for inviting a guest user to your Azure AD B2C tenant is to share administration responsibilities.
-* **Consumer account** - Accounts that are managed by Azure AD B2C user flows and custom policies.
+* **Guest account** - These are external users you invite to your tenant as guests. A typical scenario for inviting a guest user to your Azure AD B2C tenant is to share administration responsibilities.
+* **Consumer account** - These are accounts that are managed by Azure AD B2C user flows and custom policies.
 
 :::image type="content" source="media/technical-overview/portal-01-users.png" alt-text="Screenshot of the Azure AD B2C user management page in the Azure portal.":::<br/>*Figure: User directory within an Azure AD B2C tenant in the Azure portal.*
 
@@ -67,7 +68,7 @@ For more information, see [Overview of user accounts in Azure Active Directory B
 
 ## Local account sign-in options
 
-Azure AD B2C provides various ways in which you can authenticate a user. Users can sign-in to a local account, by using username and password, phone verification (also known as password-less authentication). Email sign-up is enabled by default in your local account identity provider settings.
+Azure AD B2C provides various ways in which you can authenticate a user. Users can sign-in to a local account, by using username and password, phone verification (also known as passwordless authentication). Email sign-up is enabled by default in your local account identity provider settings.
 
 Learn more about [sign-in options](sign-in-options.md) or how to [set up the local account identity provider](identity-provider-local.md).
 
@@ -75,14 +76,14 @@ Learn more about [sign-in options](sign-in-options.md) or how to [set up the loc
 
 Azure AD B2C lets you manage common attributes of consumer account profiles. For example display name, surname, given name, city, and others.
 
-You can also extend the Microsoft Entra schema to store additional information about your users. For example, their country/region of residency, preferred language, and preferences like whether they want to subscribe to a newsletter or enable multifactor authentication. For more information, see:
+You can also extend the underlying Microsoft Entra ID schema to store additional information about your users. For example, their country/region of residency, preferred language, and preferences like whether they want to subscribe to a newsletter or enable multifactor authentication. For more information, see:
 
 * [User profile attributes](user-profile-attributes.md)
 * [Add user attributes and customize user input in](configure-user-input.md)
 
 ## Sign-in with external identity providers
 
-You can configure Azure AD B2C to allow users to sign in to your application with credentials from social and enterprise identity providers. Azure AD B2C can federate with identity providers that support OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML protocols. For example, Facebook, Microsoft account, Google, Twitter, and AD-FS.
+You can configure Azure AD B2C to allow users to sign in to your application with credentials from social and enterprise identity providers. Azure AD B2C can federate with identity providers that support OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML protocols. For example, Facebook, Microsoft account, Google, Twitter, and Active Directory Federation Service (AD FS).
 
 :::image type="content" source="media/technical-overview/external-idps.png" alt-text="Diagram showing company logos for a sample of external identity providers.":::
 
@@ -92,23 +93,23 @@ On the sign-up or sign-in page, Azure AD B2C presents a list of external identit
 
 :::image type="content" source="media/technical-overview/external-idp.png" alt-text="Diagram showing a mobile sign-in example with a social account (Facebook).":::
 
-To see how to add identity providers in Azure AD B2C, see [Add identity providers to your applications in Azure Active Directory B2C](add-identity-provider.md).
+To learn more about identity providers, see [Add identity providers to your applications in Azure Active Directory B2C](add-identity-provider.md).
 
 ## Identity experiences: user flows or custom policies
 
-In Azure AD B2C, you can define the business logic that users follow to gain access to your application. For example, you can determine the sequence of steps users follow when they sign in, sign up, edit a profile, or reset a password. After completing the sequence, the user acquires a token and gains access to your application.
+In Azure AD B2C, you can define the business logic that users follow to gain access to your application. For example, you can determine the sequence of steps users follow when they sign in, sign up, edit their profile, or reset a password. After completing the sequence, the user acquires a token and gains access to your application.
 
 In Azure AD B2C, there are two ways to provide identity user experiences:
 
-* **User flows** are predefined, built-in, configurable policies that we provide so you can create sign-up, sign-in, and policy editing experiences in minutes.
+* **User flows** - These are predefined, built-in, configurable policies that we provide so you can create sign-up, sign-in, and policy editing experiences in minutes.
 
-* **Custom policies** enable you to create your own user journeys for complex identity experience scenarios.
+* **Custom policies** - These enable you to create your own user journeys for complex identity experience scenarios.
 
 The following screenshot shows the user flow settings UI, versus custom policy configuration files.
 
 :::image type="content" source="media/technical-overview/user-flow-vs-custom-policy.png" alt-text="Screenshot showing the user flow settings UI versus a custom policy configuration file.":::
 
-Read the [User flows and custom policies overview](user-flow-overview.md) article. It gives an overview of user flows and custom policies, and helps you decide which method will work best for your business needs.
+To learn more about user flows and custom policies, and help you decide which method will work best for your business needs, see [User flows and custom policies overview](user-flow-overview.md).  
 
 ## User interface
 
@@ -125,13 +126,13 @@ For information on UI customization, see:
 
 ## Custom domain
 
-You can customize your Azure AD B2C domain in the redirect URIs for your application. Custom domain allows you to create a seamless experience so that the pages that are shown blend seamlessly with the domain name of your application. From the user's perspective, they remain in your domain during the sign-in process rather than redirecting to the Azure AD B2C default domain .b2clogin.com. 
+You can customize your Azure AD B2C domain in the redirect URIs for your application. Custom domain allows you to create a seamless experience so that the pages that are shown blend seamlessly with the domain name of your application. From the user's perspective, they remain in your domain during the sign-in process rather than redirecting to the Azure AD B2C default domain *.b2clogin.com*. 
 
 For more information, see [Enable custom domains](custom-domain.md).
  
 ## Localization
 
-Language customization in Azure AD B2C allows you to accommodate different languages to suit your customer needs. Microsoft provides the translations for 36 languages, but you can also provide your own translations for any language. 
+Language customization in Azure AD B2C allows you to accommodate different languages to suit your customer needs. Microsoft provides localizations for 36 languages, but you can also provide your own localizations for any language. 
 
 :::image type="content" source="media/technical-overview/localization.png" alt-text="Screenshot of three sign in pages showing UI text in different languages.":::
 
@@ -139,11 +140,11 @@ See how localization works in [Language customization in Azure Active Directory
 
 ## Email verification
 
-Azure AD B2C ensures valid email addresses by requiring customers to verify them during the sign-up, and password reset flows. It also prevents malicious actors from using automated processes to generate fraudulent accounts in your applications.
+Azure AD B2C ensures valid email addresses by requiring customers to verify them during the sign-up, and password reset flows. This also prevents malicious actors from using automated processes to generate fraudulent accounts in your applications.
 
 :::image type="content" source="media/technical-overview/email-verification.png" alt-text="Screenshots showing the process for email verification.":::
 
-You can customize the email to users that sign up to use your applications. By using the third-party email provider, you can use your own email template and From: address and subject, as well as support localization and custom one-time password (OTP) settings. For more information, see:
+You can customize the email sent to users that sign up to use your applications. By using a third-party email provider, you can use your own email template and From: address and subject, as well as support localization and custom one-time password (OTP) settings. For more information, see:
 
 * [Custom email verification with Mailjet](custom-email-mailjet.md)
 * [Custom email verification with SendGrid](custom-email-sendgrid.md)
@@ -171,7 +172,7 @@ You can add a REST API call at any step in a user journey defined by a custom po
 * After Azure AD B2C creates a new account in the directory
 * Before Azure AD B2C issues an access token
 
-For more information, see [Integrate REST API claims exchanges in your Azure AD B2C custom policy](api-connectors-overview.md).
+For more information, see [About API connectors in Azure AD B2C](api-connectors-overview.md).
 
 ## Protocols and tokens
 
@@ -181,10 +182,8 @@ For more information, see [Integrate REST API claims exchanges in your Azure AD
 
 The following diagram shows how Azure AD B2C can communicate using various protocols within the same authentication flow:
 
-![Diagram of OIDC-based client app federating with a SAML-based IdP](media/technical-overview/protocols.png)
 :::image type="content" source="media/technical-overview/protocols.png" alt-text="Diagram of OIDC-based client app federating with a SAML-based IdP.":::
 
-
 1. The relying party application starts an authorization request to Azure AD B2C using OpenID Connect.
 1. When a user of the application chooses to sign in using an external identity provider that uses the SAML protocol, Azure AD B2C invokes the SAML protocol to communicate with that identity provider.
 1. After the user completes the sign-in operation with the external identity provider, Azure AD B2C then returns the token to the relying party application using OpenID Connect.
@@ -201,7 +200,7 @@ For example, to sign in to an application, the application uses the *sign up or
 
 ## Multifactor authentication (MFA)
 
-Azure AD B2C Multi-Factor Authentication (MFA) helps safeguard access to data and applications while maintaining simplicity for your users. It provides extra security by requiring a second form of authentication, and delivers strong authentication by offering a range of easy-to-use authentication methods. 
+Azure AD B2C Multifactor Authentication (MFA) helps safeguard access to data and applications while maintaining simplicity for your users. It provides extra security by requiring a second form of authentication, and delivers strong authentication by offering a range of easy-to-use authentication methods. 
 
 Your users may or may not be challenged for MFA based on configuration decisions that you can make as an administrator.
 
@@ -213,8 +212,7 @@ Microsoft Entra ID Protection risk-detection features, including risky users and
 
 :::image type="content" source="media/technical-overview/conditional-access-flow.png" alt-text="Diagram showing conditional access flow.":::
 
-
-Azure AD B2C evaluates each sign-in event and ensures that all policy requirements are met before granting the user access. Risky users or sign-ins may be blocked, or challenged with a specific remediation like multifactor authentication (MFA). For more information, see [Identity Protection and Conditional Access](conditional-access-identity-protection-overview.md).
+Azure AD B2C evaluates each sign-in event and ensures that all policy requirements are met before granting the user access. Risky users or risky sign-ins may be blocked, or challenged with a specific remediation like multifactor authentication (MFA). For more information, see [Identity Protection and Conditional Access](conditional-access-identity-protection-overview.md).
 
 ## Password complexity
 
@@ -226,9 +224,7 @@ For more information, see [Configure complexity requirements for passwords in Az
 
 ## Force password reset
 
-As an Azure AD B2C tenant administrator, you can [reset a user's password](manage-users-portal.md#reset-a-users-password) if the user forgets their password. Or you would like to force them to reset the password periodically. For more information, see [Set up a force password reset flow](force-password-reset.md).
-
-
+As an Azure AD B2C tenant administrator, you can [reset a user's password](manage-users-portal.md#reset-a-users-password) if the user forgets their password. Or you can set a policy to force users to reset their password periodically. For more information, see [Set up a force password reset flow](force-password-reset.md).
 
 :::image type="content" source="media/technical-overview/force-password-reset-flow.png" alt-text="Force password reset flow.":::
 
@@ -236,16 +232,15 @@ As an Azure AD B2C tenant administrator, you can [reset a user's password](manag
 
 To prevent brute-force password guessing attempts, Azure AD B2C uses a sophisticated strategy to lock accounts based on the IP of the request, the passwords entered, and several other factors. The duration of the lockout is automatically increased based on risk and the number of attempts.
 
-![Account smart lockout](media/technical-overview/smart-lockout1.png)
 :::image type="content" source="media/technical-overview/smart-lockout1.png" alt-text="Screenshot of UI for account lockout with arrows highlighting the lockout notification.":::
 
 For more information about managing password protection settings, see [Mitigate


[æ–°ç‰¹æ€§] Azure AD B2CæŠ€æœ¯æ¦‚è§ˆå’ŒåŠŸèƒ½è¯¦ç»†æ›´æ–°
2023-11-22 17:35:24
https://learn.microsoft.com/en-us/azure/aks/coredns-custom

æ­¤æ–‡ä»¶ä¸­æ·»åŠ äº†å…³äºCoreDNS podå‰¯æœ¬æ¨èæœ€å°æ•°é‡çš„è­¦å‘Šã€‚å»ºè®®æ¯ä¸ªé›†ç¾¤è‡³å°‘æœ‰2ä¸ªCoreDNS podå‰¯æœ¬ã€‚å¦‚æœé…ç½®çš„CoreDNS podå‰¯æœ¬æœ€å°æ•°é‡ä¸º1ï¼Œå¯èƒ½ä¼šåœ¨éœ€è¦èŠ‚ç‚¹æ’ç©ºçš„æ“ä½œä¸­å‡ºç°æ•…éšœï¼Œä¾‹å¦‚é›†ç¾¤å‡çº§æ“ä½œã€‚

https://learn.microsoft.com/en-us/azure/active-directory-b2c/overview

æ­¤æ–‡ä»¶è¿›è¡Œäº†ä¸€äº›æ›´æ–°å’Œä¿®æ”¹ã€‚ä¸»è¦æ›´æ”¹åŒ…æ‹¬å¯¹Azure Active Directory B2Cçš„æè¿°ï¼Œä»¥åŠå¯¹ä½¿ç”¨Azure AD B2Cè¿›è¡Œèº«ä»½éªŒè¯çš„å„ç§æ–¹å¼çš„æè¿°ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹ä¸€äº›è¯­å¥è¿›è¡Œäº†é‡å†™å’Œæ¾„æ¸…ï¼Œä»¥æé«˜å¯è¯»æ€§å’Œå‡†ç¡®æ€§ã€‚

https://learn.microsoft.com/en-us/azure/active-directory-b2c/overview å’Œ https://learn.microsoft.com/en-us/azure/active-directory-b2c/technical-overview

è¿™ä¸¤ä¸ªæ–‡ä»¶éƒ½è¿›è¡Œäº†æ›´æ–°ã€‚åœ¨overviewæ–‡ä»¶ä¸­ï¼Œæ›´æ–°äº†æ—¥æœŸï¼Œå¹¶ä¿®æ”¹äº†å®¢æˆ·æ„å›¾çš„æè¿°ã€‚åœ¨technical-overviewæ–‡ä»¶ä¸­ï¼Œå¯¹Azure AD B2Cçš„æŠ€æœ¯å’ŒåŠŸèƒ½è¿›è¡Œäº†æ›´è¯¦ç»†çš„ä»‹ç»ï¼ŒåŒ…æ‹¬Azure AD B2Cç§Ÿæˆ·ã€æœ¬åœ°å¸æˆ·ç™»å½•é€‰é¡¹ã€å¤–éƒ¨èº«ä»½æä¾›å•†ç™»å½•ã€ç”¨æˆ·ç•Œé¢ã€è‡ªå®šä¹‰åŸŸã€æœ¬åœ°åŒ–ã€ç”µå­é‚®ä»¶éªŒè¯ã€å¤šå› ç´ èº«ä»½éªŒè¯ï¼ˆMFAï¼‰ã€å¯†ç å¤æ‚æ€§ã€å¼ºåˆ¶å¯†ç é‡ç½®ç­‰ã€‚åŒæ—¶ï¼Œè¿™ä¸¤ä¸ªæ–‡ä»¶çš„ä¿®æ”¹ä¹ŸåŒ…æ‹¬äº†ä¸€äº›è¯­å¥çš„é‡å†™å’Œæ¾„æ¸…ï¼Œä»¥æé«˜å¯è¯»æ€§å’Œå‡†ç¡®æ€§ã€‚

*************************

commit_patch_data :

From ea0ce9fb61df6cc8dfaf7c365a57180b1f864dba Mon Sep 17 00:00:00 2001
From: James Barnett <v-jabarnett@microsoft.com>
Date: Wed, 22 Nov 2023 10:37:39 -0700
Subject: [PATCH] Acro fix

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index bcaae948c9cb..d87a25320da1 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,7 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
+This article only shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -206,4 +206,4 @@ embedding = client.embeddings.create(
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)



commit_patch_data :

From a327c4725689c025b40d8c9ceaef2d309672dfb6 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:56:32 -0500
Subject: [PATCH 1/6] update

---
 .../openai/how-to/switching-endpoints.md      | 189 +++++++++++++++++-
 1 file changed, 187 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 7d45305693b2..635cae90c973 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,8 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-> [!NOTE]
-> This library is maintained by OpenAI and is currently in preview. Refer to the [release history](https://github.com/openai/openai-python/releases) or the [version.py commit history](https://github.com/openai/openai-python/commits/main/openai/version.py) to track the latest updates to the library.
+# [OpenAI Python 0.28.1](#tab/python)
 
 ## Authentication
 
@@ -201,6 +200,192 @@ embedding = openai.Embedding.create(
 </tr>
 </table>
 
+# [OpenAI Python 1.x](#tab/python-new)
+
+## Authentication
+
+We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
+
+### API key
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+import os
+from openai import AzureOpenAI
+    
+client = AzureOpenAI(
+    api_key=os.getenv("AZURE_OPENAI_KEY"),  
+    api_version="2023-10-01-preview",
+    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
+    )
+```
+
+</td>
+</tr>
+</table>
+
+<a name='azure-active-directory-authentication'></a>
+
+### Microsoft Entra authentication
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+
+
+
+
+```
+
+</td>
+<td>
+
+```python
+from azure.identity import DefaultAzureCredential, get_bearer_token_provider
+from openai import AzureOpenAI
+
+token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")
+
+api_version = "2023-12-01-preview"
+endpoint = "https://my-resource.openai.azure.com"
+
+client = AzureOpenAI(
+    api_version=api_version,
+    azure_endpoint=endpoint,
+    azure_ad_token_provider=token_provider,
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Keyword argument for model
+
+OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
+
+For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+completion = client.completions.create(
+    model='gpt-3.5-turbo-instruct',
+    prompt="<prompt>)
+)
+
+chat_completion = openai.chat.completions.create(
+    model="gpt-4",
+    messages="<messages>"
+)
+
+embedding = client.embeddings.create(
+    input="<input>",
+    model="text-embedding-ada-002"
+)
+```
+
+</td>
+<td>
+
+```python
+client.completions.create(
+    model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
+    prompt=<"prompt">
+)
+
+client.chat.completions.create(
+    model="gpt-35-turbo", # model = "deployment_name".
+    messages=<"messages">
+)
+
+client.embeddings.create(
+    input = "<input>",
+    model= "text-embedding-ada-002" # model = "deployment_name".
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Azure OpenAI embeddings multiple input support
+
+OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+inputs = ["A", "B", "C"] 
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002"
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+inputs = ["A", "B", "C"] #max array size=16
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
+  #engine="text-embedding-ada-002"
+)
+```
+
+</td>
+</tr>
+</table>
+
+---
+
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).

From dbffff50a4994b23b4fa171b747aa0bcc6ee4893 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:58:22 -0500
Subject: [PATCH 2/6] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 635cae90c973..8a8f70c55a10 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -7,7 +7,7 @@ ms.author: mbullwin #delegenz
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to
-ms.date: 07/20/2023
+ms.date: 11/22/2023
 manager: nitinme
 ---
 

From c834902936f80682d16dc481ece7ebf78006d0b5 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:15:14 -0500
Subject: [PATCH 3/6] update

---
 .../openai/how-to/switching-endpoints.md      | 204 +-----------------
 1 file changed, 10 insertions(+), 194 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 8a8f70c55a10..90e3e9b09811 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,192 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-# [OpenAI Python 0.28.1](#tab/python)
-
-## Authentication
-
-We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
-
-### API key
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-
-openai.api_type = "azure"
-openai.api_key = "..."
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-<a name='azure-active-directory-authentication'></a>
-
-### Microsoft Entra authentication
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-from azure.identity import DefaultAzureCredential
-
-credential = DefaultAzureCredential()
-token = credential.get_token("https://cognitiveservices.azure.com/.default")
-
-openai.api_type = "azure_ad"
-openai.api_key = token.token
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-## Keyword argument for model
-
-OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
-
-For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    model="text-davinci-003"
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    model="gpt-4"
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  model="text-embedding-ada-002"
-)
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    deployment_id="text-davinci-003" # This must match the custom deployment name you chose for your model.
-    #engine="text-davinci-003" 
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    deployment_id="gpt-4" # This must match the custom deployment name you chose for your model.
-    #engine="gpt-4"
-
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-## Azure OpenAI embeddings multiple input support
-
-OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-inputs = ["A", "B", "C"] 
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  model="text-embedding-ada-002"
-)
-
-
-```
-
-</td>
-<td>
-
-```python
-inputs = ["A", "B", "C"] #max array size=16
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-# [OpenAI Python 1.x](#tab/python-new)
+This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -223,6 +38,7 @@ client = OpenAI(
 )
 
 
+
 ```
 
 </td>
@@ -266,6 +82,8 @@ client = OpenAI(
 
 
 
+
+
 ```
 
 </td>
@@ -310,7 +128,7 @@ completion = client.completions.create(
     prompt="<prompt>)
 )
 
-chat_completion = openai.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-4",
     messages="<messages>"
 )
@@ -325,17 +143,17 @@ embedding = client.embeddings.create(
 <td>
 
 ```python
-client.completions.create(
+completion = client.completions.create(
     model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
     prompt=<"prompt">
 )
 
-client.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-35-turbo", # model = "deployment_name".
     messages=<"messages">
 )
 
-client.embeddings.create(
+embedding = client.embeddings.create(
     input = "<input>",
     model= "text-embedding-ada-002" # model = "deployment_name".
 )
@@ -364,7 +182,6 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
-
 ```
 
 </td>
@@ -378,15 +195,14 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
   #engine="text-embedding-ada-002"
 )
+
 ```
 
 </td>
 </tr>
 </table>
 
----
-
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file

From 659bbf38c4b50ecb1aeedf80987724f50d30a389 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:17:59 -0500
Subject: [PATCH 4/6] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 90e3e9b09811..1b74c13e916a 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -2,8 +2,8 @@
 title: How to switch between OpenAI and Azure OpenAI Service endpoints with Python
 titleSuffix: Azure OpenAI Service
 description: Learn about the changes you need to make to your code to swap back and forth between OpenAI and Azure OpenAI endpoints.
-author: mrbullwinkle #dereklegenzoff
-ms.author: mbullwin #delegenz
+author: mrbullwinkle 
+ms.author: mbullwin 
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to

From 85d411bb59d2a61a3c28de546c3697928ccf898b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:29:49 -0500
Subject: [PATCH 5/6] updatE

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 1 +
 1 file changed, 1 insertion(+)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 1b74c13e916a..bcaae948c9cb 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -182,6 +182,7 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
+
 ```
 
 </td>

From ea0ce9fb61df6cc8dfaf7c365a57180b1f864dba Mon Sep 17 00:00:00 2001
From: James Barnett <v-jabarnett@microsoft.com>
Date: Wed, 22 Nov 2023 10:37:39 -0700
Subject: [PATCH 6/6] Acro fix

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index bcaae948c9cb..d87a25320da1 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,7 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
+This article only shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -206,4 +206,4 @@ embedding = client.embeddings.create(
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)



[æ–°åŠŸèƒ½] OpenAIå’ŒAzure OpenAIåˆ‡æ¢æ–¹æ³•çš„è¯¦ç»†æ›´æ–°
2023-11-22 17:51:16
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

åœ¨ç¬¬ä¸€ä¸ªæäº¤ä¸­ï¼Œå¯¹æ–‡ä»¶è¿›è¡Œäº†å¤§é‡çš„æ›´æ–°ï¼Œä¸»è¦æ˜¯å¢åŠ äº†å…³äºå¦‚ä½•åœ¨OpenAIå’ŒAzure OpenAIä¹‹é—´åˆ‡æ¢çš„è¯¦ç»†è¯´æ˜ã€‚è¿™åŒ…æ‹¬äº†å¦‚ä½•è¿›è¡Œèº«ä»½éªŒè¯ï¼Œå¦‚ä½•ä½¿ç”¨APIå¯†é’¥ï¼Œå¦‚ä½•ä½¿ç”¨Microsoft Entraè¿›è¡Œèº«ä»½éªŒè¯ï¼Œå¦‚ä½•ä¸ºæ¨¡å‹ä½¿ç”¨å…³é”®å­—å‚æ•°ï¼Œä»¥åŠå¦‚ä½•æ”¯æŒAzure OpenAIåµŒå…¥å¤šä¸ªè¾“å…¥ã€‚

åœ¨ç¬¬äºŒä¸ªæäº¤ä¸­ï¼Œæ›´æ–°äº†æ–‡ä»¶çš„æ—¥æœŸã€‚

åœ¨ç¬¬ä¸‰ä¸ªæäº¤ä¸­ï¼Œåˆ é™¤äº†å¤§é‡çš„å†…å®¹ï¼Œä¸»è¦æ˜¯å…³äºOpenAI Python 0.28.1çš„ç¤ºä¾‹å’Œè¯´æ˜ï¼Œåªä¿ç•™äº†å…³äºOpenAI Python 1.xçš„å†…å®¹ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªé“¾æ¥åˆ°è¿ç§»æŒ‡å—ã€‚

åœ¨ç¬¬å››ä¸ªæäº¤ä¸­ï¼Œæ›´æ–°äº†ä½œè€…ä¿¡æ¯ã€‚

åœ¨ç¬¬äº”ä¸ªæäº¤ä¸­ï¼Œå¯¹æ–‡ä»¶è¿›è¡Œäº†å¾®å°çš„æ ¼å¼è°ƒæ•´ï¼Œæ·»åŠ äº†ä¸€ä¸ªç©ºè¡Œã€‚

åœ¨ç¬¬å…­ä¸ªæäº¤ä¸­ï¼Œä¿®æ­£äº†ä¸€ä¸ªæ‹¼å†™é”™è¯¯ã€‚

*************************

commit_patch_data :

From 4ade8961db578609d55f5be1b906f3cb68fa6aa3 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 14:01:52 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/content-filters.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/content-filters.md b/articles/ai-services/openai/how-to/content-filters.md
index f1f6850e9ce6..976b2622e288 100644
--- a/articles/ai-services/openai/how-to/content-filters.md
+++ b/articles/ai-services/openai/how-to/content-filters.md
@@ -18,7 +18,7 @@ keywords:
 > [!NOTE]
 > All customers have the ability to modify the content filters to be stricter (for example, to filter content at lower severity levels than the default). Approval is required for turning the content filters partially or fully off. Managed customers only may apply for full content filtering control via this form: [Azure OpenAI Limited Access Review: Modified Content Filters and Abuse Monitoring (microsoft.com)](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xURE01NDY1OUhBRzQ3MkQxMUhZSE1ZUlJKTiQlQCN0PWcu).
 
-The content filtering system integrated into Azure OpenAI Service runs alongside the core models and uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe  is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering system [here](../concepts/content-filter.md). Jailbreak risk detection and protected text and code models are optional and off by default. For jailbreak and protected material text and code models, the configurability feature allows all customers to turn the models on and off. The models are by default off and can be turned on per your scenario. Note that some models are required to be on for certain scenarios to retain coverage under the [Customer Copyright Commitment](https://www.microsoft.com/licensing/news/Microsoft-Copilot-Copyright-Commitment).
+The content filtering system integrated into Azure OpenAI Service runs alongside the core models and uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe  is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering system [here](../concepts/content-filter.md). Jailbreak risk detection and protected text and code models are optional and off by default. For jailbreak and protected material text and code models, the configurability feature allows all customers to turn the models on and off. The models are by default off and can be turned on per your scenario. Note that some models are required to be on for certain scenarios to retain coverage under the [Customer Copyright Commitment](/legal/cognitive-services/openai/customer-copyright-commitment?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext).
 
 Content filters can be configured at resource level. Once a new configuration is created, it can be associated with one or more deployments. For more information about model deployment, see the [resource deployment guide](create-resource.md).
 



commit_patch_data :

From 2d4669b213b7e68d8c4de5684df0703e6fc624f2 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 16:37:59 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/faq.yml | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 92a39d0394b9..c5a8d2c3669f 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -7,7 +7,7 @@ metadata:
   manager: nitinme
   ms.service: azure-ai-openai
   ms.topic: faq
-  ms.date: 11/15/2023
+  ms.date: 11/22/2023
   ms.author: mbullwin
   author: mrbullwinkle
 title: Azure OpenAI Service frequently asked questions
@@ -25,7 +25,7 @@ sections:
       - question: |
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
-          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
           I can't find GPT-4 Turbo Preview?
         answer:
@@ -90,6 +90,20 @@ sections:
           If you wanted to help a GPT based model to accurately respond to the question "what model are you running?", you would need to provide that information to the model through techniques like [prompt engineering of the model's system message](/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions), [Retrieval Augmented Generation (RAG)](/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2) which is the technique used by [Azure OpenAI on your data](/azure/ai-services/openai/concepts/use-your-data) where up-to-date information is injected to the system message at query time, or via [fine-tuning](/azure/ai-services/openai/how-to/fine-tuning?pivots=programming-language-studio) where you could fine-tune specific versions of the model to answer that question in a certain way based on model version. 
 
           To learn more about how GPT models are trained and work we recommend watching [Andrej Karpathy's talk from Build 2023 on the state of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A).
+      - question: |
+          I asked the model when it's knowledge cutoff is and it gave me a different answer than what is on the Azure OpenAI model's page. Why does this happen?
+        answer:
+          This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
+      - question: |
+          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer:
+          This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
+          
+          The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
+
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+                    
+          So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |
           Where do I access pricing information for legacy models which are no longer available for new deployments? 
         answer: | 



[æ¨¡å‹] Azure OpenAIæœåŠ¡FAQæ›´æ–°ï¼šæ–°å¢æ¨¡å‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸç­‰é—®é¢˜ç­”æ¡ˆ
2023-11-22 21:37:59
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

æ­¤æ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦æ¶‰åŠä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. æ›´æ–°äº†å…ƒæ•°æ®ä¸­çš„æ—¥æœŸï¼Œä»2023å¹´11æœˆ15æ—¥æ›´æ”¹ä¸º2023å¹´11æœˆ22æ—¥ã€‚

2. å¯¹ä¸€äº›é—®é¢˜çš„å›ç­”è¿›è¡Œäº†å¾®å°çš„æ–‡æœ¬ä¿®æ”¹ï¼Œä¾‹å¦‚å°†"However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI." ä¿®æ”¹ä¸º "However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI."

3. æ·»åŠ äº†æ–°çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œä¾‹å¦‚å…³äºæ¨¡å‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸçš„é—®é¢˜å’Œç­”æ¡ˆï¼Œä»¥åŠå…³äºæ¨¡å‹å¯¹æœ€è¿‘äº‹ä»¶çš„å›ç­”ä¸å‡†ç¡®çš„é—®é¢˜å’Œç­”æ¡ˆã€‚è¿™äº›æ–°çš„é—®é¢˜å’Œç­”æ¡ˆæä¾›äº†æ›´å¤šå…³äºAzure OpenAIæœåŠ¡çš„ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨è¯¥æœåŠ¡ã€‚

*************************

commit_patch_data :

From a53489cb84301f84243bc2b49678db32809436da Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 17:03:13 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/faq.yml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index c5a8d2c3669f..bb7162dddc9e 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -95,13 +95,13 @@ sections:
         answer:
           This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
       - question: |
-          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
-        answer:
+          I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer: |
           This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
           
           The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
 
-          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield an accurate response which demonstrates training data knowledge going to at least January of 2023.   
                     
           So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |



[æ¨¡å‹] å¯¹AIæ¨¡å‹FAQé—®é¢˜ç­”æ¡ˆçš„é‡è¦ä¿®æ”¹å’Œè¡¥å……
2023-11-22 22:03:13
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

è¿™ä¸ªæ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦æ˜¯å¯¹ä¸€äº›é—®é¢˜å’Œç­”æ¡ˆçš„æ–‡æœ¬è¿›è¡Œäº†ä¿®æ”¹ã€‚é¦–å…ˆï¼Œå¯¹ä¸€ä¸ªé—®é¢˜çš„è¡¨è¿°è¿›è¡Œäº†å¾®è°ƒï¼Œå°†â€œæˆ‘é—®äº†æ¨¡å‹ä¸€ä¸ªé—®é¢˜ï¼Œå®ƒåº”è¯¥çŸ¥é“å…³äºæœ€è¿‘å‘ç”Ÿçš„äº‹æƒ…çš„ç­”æ¡ˆï¼ŒåŸºäºçŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼Œä½†å®ƒå›ç­”é”™äº†ã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿâ€ä¿®æ”¹ä¸ºâ€œæˆ‘é—®äº†æ¨¡å‹ä¸€ä¸ªå…³äºæœ€è¿‘åœ¨çŸ¥è¯†æˆªæ­¢æ—¥æœŸä¹‹å‰å‘ç”Ÿçš„äº‹æƒ…çš„é—®é¢˜ï¼Œä½†å®ƒå›ç­”é”™äº†ã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿâ€ã€‚å…¶æ¬¡ï¼Œå¯¹è¯¥é—®é¢˜çš„ç­”æ¡ˆè¿›è¡Œäº†ä¸€äº›ä¿®æ”¹å’Œè¡¥å……ï¼Œæ›´è¯¦ç»†åœ°è§£é‡Šäº†æ¨¡å‹å¯èƒ½å›ç­”é”™è¯¯çš„åŸå› ã€‚æœ€åï¼Œå¯¹å¦ä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆè¿›è¡Œäº†ä¿®æ”¹ï¼Œå°†â€œç„¶è€Œï¼Œé—®æ¨¡å‹'æ°è¾›è¾¾Â·é˜¿å¾·æ©ä½•æ—¶è¾å»æ–°è¥¿å…°æ€»ç†èŒåŠ¡ï¼Ÿ'å€¾å‘äºå¾—åˆ°ä¸€ä¸ªæ­£ç¡®çš„å›ç­”ï¼Œè¿™è¡¨æ˜è®­ç»ƒæ•°æ®çš„çŸ¥è¯†è‡³å°‘åˆ°2023å¹´1æœˆã€‚â€ä¿®æ”¹ä¸ºâ€œç„¶è€Œï¼Œé—®æ¨¡å‹'æ°è¾›è¾¾Â·é˜¿å¾·æ©ä½•æ—¶è¾å»æ–°è¥¿å…°æ€»ç†èŒåŠ¡ï¼Ÿ'å€¾å‘äºå¾—åˆ°ä¸€ä¸ªå‡†ç¡®çš„å›ç­”ï¼Œè¿™è¡¨æ˜è®­ç»ƒæ•°æ®çš„çŸ¥è¯†è‡³å°‘åˆ°2023å¹´1æœˆã€‚â€ã€‚

*************************

commit_patch_data :

From 2d4669b213b7e68d8c4de5684df0703e6fc624f2 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 16:37:59 -0500
Subject: [PATCH 1/2] update

---
 articles/ai-services/openai/faq.yml | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 92a39d0394b9..c5a8d2c3669f 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -7,7 +7,7 @@ metadata:
   manager: nitinme
   ms.service: azure-ai-openai
   ms.topic: faq
-  ms.date: 11/15/2023
+  ms.date: 11/22/2023
   ms.author: mbullwin
   author: mrbullwinkle
 title: Azure OpenAI Service frequently asked questions
@@ -25,7 +25,7 @@ sections:
       - question: |
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
-          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
           I can't find GPT-4 Turbo Preview?
         answer:
@@ -90,6 +90,20 @@ sections:
           If you wanted to help a GPT based model to accurately respond to the question "what model are you running?", you would need to provide that information to the model through techniques like [prompt engineering of the model's system message](/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions), [Retrieval Augmented Generation (RAG)](/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2) which is the technique used by [Azure OpenAI on your data](/azure/ai-services/openai/concepts/use-your-data) where up-to-date information is injected to the system message at query time, or via [fine-tuning](/azure/ai-services/openai/how-to/fine-tuning?pivots=programming-language-studio) where you could fine-tune specific versions of the model to answer that question in a certain way based on model version. 
 
           To learn more about how GPT models are trained and work we recommend watching [Andrej Karpathy's talk from Build 2023 on the state of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A).
+      - question: |
+          I asked the model when it's knowledge cutoff is and it gave me a different answer than what is on the Azure OpenAI model's page. Why does this happen?
+        answer:
+          This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
+      - question: |
+          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer:
+          This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
+          
+          The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
+
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+                    
+          So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |
           Where do I access pricing information for legacy models which are no longer available for new deployments? 
         answer: | 

From a53489cb84301f84243bc2b49678db32809436da Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 17:03:13 -0500
Subject: [PATCH 2/2] update

---
 articles/ai-services/openai/faq.yml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index c5a8d2c3669f..bb7162dddc9e 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -95,13 +95,13 @@ sections:
         answer:
           This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
       - question: |
-          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
-        answer:
+          I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer: |
           This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
           
           The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
 
-          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield an accurate response which demonstrates training data knowledge going to at least January of 2023.   
                     
           So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |



[æ¨¡å‹] æ›´æ–°äº†å…³äºæ¨¡å‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸå’Œå›ç­”é”™è¯¯åŸå› çš„é—®é¢˜å’Œç­”æ¡ˆ
2023-11-22 22:18:22
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

è¿™ä¸ªæ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
1. æ›´æ–°äº†æ–‡ä»¶çš„å…ƒæ•°æ®ä¸­çš„æ—¥æœŸã€‚
2. å¯¹ä¸€äº›é—®é¢˜çš„å›ç­”è¿›è¡Œäº†å¾®å°çš„æ–‡æœ¬ä¿®æ”¹ï¼Œä¾‹å¦‚å°†"It is important to note"æ›´æ”¹ä¸º"It's important to note"ã€‚
3. æ·»åŠ äº†ä¸¤ä¸ªæ–°çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œè¿™äº›é—®é¢˜ä¸»è¦å…³äºæ¨¡å‹çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸå’Œæ¨¡å‹å¯¹æœ€è¿‘äº‹ä»¶çš„å›ç­”å¯èƒ½å‡ºé”™çš„åŸå› ã€‚
4. å¯¹ä¸€äº›å·²æœ‰é—®é¢˜çš„å›ç­”è¿›è¡Œäº†æ‰©å±•å’Œè¯¦ç»†åŒ–ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

è¿™ä¸ªæ–‡ä»¶ä¸­çš„æ›´æ”¹ä¸»è¦æ˜¯å¯¹ä¸€äº›é—®é¢˜çš„è¡¨è¿°å’Œå›ç­”è¿›è¡Œäº†å¾®è°ƒã€‚ä¾‹å¦‚ï¼Œå°†"I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?"æ›´æ”¹ä¸º"I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?"ã€‚åŒæ—¶ï¼Œå¯¹ä¸€äº›å›ç­”ä¸­çš„æè¿°è¿›è¡Œäº†å¾®è°ƒï¼Œä¾‹å¦‚å°†"tends to yield a correct response"æ›´æ”¹ä¸º"tends to yield an accurate response"ã€‚

*************************

commit_patch_data :

From 603117a8897f5ec3fcd0caf4725e066e9ad4f184 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 16:21:14 -0600
Subject: [PATCH] powershell sample for user your own data quickstart

---
 .../includes/use-your-data-powershell.md      | 83 +++++++++++++++++++
 .../openai/use-your-data-quickstart.md        |  8 +-
 2 files changed, 90 insertions(+), 1 deletion(-)
 create mode 100644 articles/ai-services/openai/includes/use-your-data-powershell.md

diff --git a/articles/ai-services/openai/includes/use-your-data-powershell.md b/articles/ai-services/openai/includes/use-your-data-powershell.md
new file mode 100644
index 000000000000..c862003f3d5d
--- /dev/null
+++ b/articles/ai-services/openai/includes/use-your-data-powershell.md
@@ -0,0 +1,83 @@
+---
+services: cognitive-services
+manager: nitinme
+author: mgreenegit
+ms.author: migreene
+ms.service: azure-ai-openai
+ms.topic: include
+ms.date: 11/22/2023
+---
+
+[!INCLUDE [Set up required variables](./use-your-data-common-variables.md)]
+
+## Example PowerShell commands
+
+The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. The `messages` variable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. The `dataSources` variable connects to your Azure Cognitive Search index, and enables Azure OpenAI models to respond using your data.
+
+To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
+
+> [!TIP]
+> There are several parameters you can use to change the model's response, such as `temperature` or `top_p`. See the [reference documentation](../reference.md#completions-extensions) for more information.
+
+```powershell-interactive
+# Azure OpenAI metadata variables
+   $openai = @{
+       api_key     = $Env:AZURE_OPENAI_KEY
+       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
+       api_version = '2023-07-01-preview' # this may change in the future
+       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
+   }
+
+   $acs = @{
+       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
+       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
+       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
+   }
+
+   # Completion text
+   $body = @{
+    dataSources = @(
+        @{
+            type = 'AzureCognitiveSearch'
+            parameters = @{
+                    endpoint = $acs.search_endpoint
+                    key = $acs.search_key
+                    indexName = $acs.search_index
+                }
+        }
+    )
+    messages = @(
+            @{
+                role = 'user'
+                content = 'How do you query REST using PowerShell'
+            }
+    )
+   } | convertto-json -depth 5
+
+   # Header for authentication
+   $headers = [ordered]@{
+       'api-key' = $openai.api_key
+   }
+
+   # Send a completion call to generate an answer
+   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"
+
+   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
+   return $response.choices.messages[1].content
+```
+
+### Example output
+
+```text
+To query a RESTful web service using PowerShell, you can use the `Invoke-RestMethod` cmdlet. This cmdlet sends HTTP and HTTPS requests to RESTful web services and processes the response based on the data type.
+```
+
+> [!IMPORTANT]
+> For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see the Azure AI services [security](../../security-features.md) article.
+
+> [!div class="nextstepaction"]
+> [I ran into an issue with sending the request.](https://microsoft.qualtrics.com/jfe/form/SV_0Cl5zkG3CnDjq6O?PLanguage=REST&Pillar=AOAI&Product=ownData&Page=quickstart&Section=Send-request)
+
+## Chat with your model using a web app
+
+To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app using [Azure OpenAI studio](../concepts/use-your-data.md#deploying-the-model) or example code we [provide on GitHub](https://go.microsoft.com/fwlink/?linkid=2244395). This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize the [frontend and backend logic](../concepts/use-your-data.md#using-the-web-app) of the web app by making changes to the source code.
diff --git a/articles/ai-services/openai/use-your-data-quickstart.md b/articles/ai-services/openai/use-your-data-quickstart.md
index 9c6861aefabb..b1dd5688b33c 100644
--- a/articles/ai-services/openai/use-your-data-quickstart.md
+++ b/articles/ai-services/openai/use-your-data-quickstart.md
@@ -9,7 +9,7 @@ ms.custom: devx-track-dotnet, devx-track-extended-java, devx-track-js, devx-trac
 ms.topic: quickstart
 author: aahill
 ms.author: aahi
-ms.date: 08/25/2023
+ms.date: 11/22/2023
 recommendations: false
 zone_pivot_groups: openai-use-your-data
 ---
@@ -88,6 +88,12 @@ In this quickstart you can use your own data with Azure OpenAI models. Using Azu
 
 ::: zone-end
 
+::: zone pivot="programming-language-powershell"
+
+[!INCLUDE [PowerShell quickstart](includes/use-your-data-powershell.md)]
+
+::: zone-end
+
 ::: zone pivot="programming-language-go"
 
 [!INCLUDE [Go quickstart](includes/use-your-data-go.md)]



[æ–°åŠŸèƒ½] Azure OpenAIä¸­ä½¿ç”¨PowerShellå¤„ç†è‡ªæœ‰æ•°æ®çš„æ–°æŒ‡å—å‘å¸ƒ
2023-11-22 22:21:14
https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

è¿™æ˜¯ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œä¸»è¦åŒ…å«äº†å¦‚ä½•ä½¿ç”¨PowerShellåœ¨Azure OpenAIä¸­ä½¿ç”¨è‡ªå·±çš„æ•°æ®ã€‚æ–‡ä»¶ä¸­åŒ…å«äº†ä¸€äº›ç¤ºä¾‹çš„PowerShellå‘½ä»¤ï¼Œè¿™äº›å‘½ä»¤ä¼˜åŒ–äº†ä¸è¾“å…¥æ ¼å¼ä¸ºå¯¹è¯çš„Azure OpenAIèŠå¤©æ¨¡å‹çš„å·¥ä½œã€‚è¿˜åŒ…å«äº†å¦‚ä½•è§¦å‘æ¨¡å‹å“åº”çš„ä¿¡æ¯ï¼Œä»¥åŠä¸€äº›å¯ä»¥æ”¹å˜æ¨¡å‹å“åº”çš„å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªç¤ºä¾‹è¾“å‡ºï¼Œè¯´æ˜äº†å¦‚ä½•ä½¿ç”¨PowerShellæŸ¥è¯¢RESTã€‚æœ€åï¼Œæ–‡ä»¶è¿˜æä¾›äº†å¦‚ä½•ä½¿ç”¨webåº”ç”¨ä¸ä½¿ç”¨è‡ªå·±æ•°æ®çš„Azure OpenAIæ¨¡å‹è¿›è¡ŒèŠå¤©çš„ä¿¡æ¯ã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

è¿™ä¸ªæ–‡ä»¶çš„ä¿®æ”¹ä¸»è¦æ˜¯åœ¨åŸæœ‰çš„åŸºç¡€ä¸Šæ·»åŠ äº†ä¸€ä¸ªæ–°çš„åŒºå—ï¼Œè¿™ä¸ªåŒºå—åŒ…å«äº†å¦‚ä½•ä½¿ç”¨PowerShellåœ¨Azure OpenAIä¸­ä½¿ç”¨è‡ªå·±çš„æ•°æ®çš„å¿«é€Ÿå…¥é—¨æŒ‡å—ã€‚

*************************

commit_patch_data :

From 603117a8897f5ec3fcd0caf4725e066e9ad4f184 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 16:21:14 -0600
Subject: [PATCH 1/2] powershell sample for user your own data quickstart

---
 .../includes/use-your-data-powershell.md      | 83 +++++++++++++++++++
 .../openai/use-your-data-quickstart.md        |  8 +-
 2 files changed, 90 insertions(+), 1 deletion(-)
 create mode 100644 articles/ai-services/openai/includes/use-your-data-powershell.md

diff --git a/articles/ai-services/openai/includes/use-your-data-powershell.md b/articles/ai-services/openai/includes/use-your-data-powershell.md
new file mode 100644
index 000000000000..c862003f3d5d
--- /dev/null
+++ b/articles/ai-services/openai/includes/use-your-data-powershell.md
@@ -0,0 +1,83 @@
+---
+services: cognitive-services
+manager: nitinme
+author: mgreenegit
+ms.author: migreene
+ms.service: azure-ai-openai
+ms.topic: include
+ms.date: 11/22/2023
+---
+
+[!INCLUDE [Set up required variables](./use-your-data-common-variables.md)]
+
+## Example PowerShell commands
+
+The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. The `messages` variable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. The `dataSources` variable connects to your Azure Cognitive Search index, and enables Azure OpenAI models to respond using your data.
+
+To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
+
+> [!TIP]
+> There are several parameters you can use to change the model's response, such as `temperature` or `top_p`. See the [reference documentation](../reference.md#completions-extensions) for more information.
+
+```powershell-interactive
+# Azure OpenAI metadata variables
+   $openai = @{
+       api_key     = $Env:AZURE_OPENAI_KEY
+       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
+       api_version = '2023-07-01-preview' # this may change in the future
+       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
+   }
+
+   $acs = @{
+       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
+       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
+       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
+   }
+
+   # Completion text
+   $body = @{
+    dataSources = @(
+        @{
+            type = 'AzureCognitiveSearch'
+            parameters = @{
+                    endpoint = $acs.search_endpoint
+                    key = $acs.search_key
+                    indexName = $acs.search_index
+                }
+        }
+    )
+    messages = @(
+            @{
+                role = 'user'
+                content = 'How do you query REST using PowerShell'
+            }
+    )
+   } | convertto-json -depth 5
+
+   # Header for authentication
+   $headers = [ordered]@{
+       'api-key' = $openai.api_key
+   }
+
+   # Send a completion call to generate an answer
+   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"
+
+   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
+   return $response.choices.messages[1].content
+```
+
+### Example output
+
+```text
+To query a RESTful web service using PowerShell, you can use the `Invoke-RestMethod` cmdlet. This cmdlet sends HTTP and HTTPS requests to RESTful web services and processes the response based on the data type.
+```
+
+> [!IMPORTANT]
+> For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see the Azure AI services [security](../../security-features.md) article.
+
+> [!div class="nextstepaction"]
+> [I ran into an issue with sending the request.](https://microsoft.qualtrics.com/jfe/form/SV_0Cl5zkG3CnDjq6O?PLanguage=REST&Pillar=AOAI&Product=ownData&Page=quickstart&Section=Send-request)
+
+## Chat with your model using a web app
+
+To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app using [Azure OpenAI studio](../concepts/use-your-data.md#deploying-the-model) or example code we [provide on GitHub](https://go.microsoft.com/fwlink/?linkid=2244395). This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize the [frontend and backend logic](../concepts/use-your-data.md#using-the-web-app) of the web app by making changes to the source code.
diff --git a/articles/ai-services/openai/use-your-data-quickstart.md b/articles/ai-services/openai/use-your-data-quickstart.md
index 9c6861aefabb..b1dd5688b33c 100644
--- a/articles/ai-services/openai/use-your-data-quickstart.md
+++ b/articles/ai-services/openai/use-your-data-quickstart.md
@@ -9,7 +9,7 @@ ms.custom: devx-track-dotnet, devx-track-extended-java, devx-track-js, devx-trac
 ms.topic: quickstart
 author: aahill
 ms.author: aahi
-ms.date: 08/25/2023
+ms.date: 11/22/2023
 recommendations: false
 zone_pivot_groups: openai-use-your-data
 ---
@@ -88,6 +88,12 @@ In this quickstart you can use your own data with Azure OpenAI models. Using Azu
 
 ::: zone-end
 
+::: zone pivot="programming-language-powershell"
+
+[!INCLUDE [PowerShell quickstart](includes/use-your-data-powershell.md)]
+
+::: zone-end
+
 ::: zone pivot="programming-language-go"
 
 [!INCLUDE [Go quickstart](includes/use-your-data-go.md)]

From 6ed27f833ca81a5b55572baf06a8cbf9e7fa10b2 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 18:11:30 -0600
Subject: [PATCH 2/2] zone pivot group

---
 articles/zone-pivot-groups.yml | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/articles/zone-pivot-groups.yml b/articles/zone-pivot-groups.yml
index 527edced2671..02cf61560d98 100644
--- a/articles/zone-pivot-groups.yml
+++ b/articles/zone-pivot-groups.yml
@@ -1902,6 +1902,8 @@ groups:
     title: Go 
   - id: rest-api
     title: REST
+  - id: programming-language-powershell
+    title: PowerShell
 # Owner: pafarley
 - id: openai-quickstart-dall-e
   title: Programming languages



[æ–°åŠŸèƒ½] Azure OpenAIæ–°å¢PowerShellä½¿ç”¨æ•™ç¨‹å’Œå¿«é€Ÿå…¥é—¨é“¾æ¥
2023-11-23 04:15:28
https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

è¿™æ˜¯ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œä¸»è¦åŒ…å«äº†å¦‚ä½•ä½¿ç”¨PowerShellåœ¨Azure OpenAIä¸­ä½¿ç”¨è‡ªå·±çš„æ•°æ®çš„ç¤ºä¾‹ã€‚è¿™ä¸ªæ–‡ä»¶è¯¦ç»†è§£é‡Šäº†å¦‚ä½•è®¾ç½®æ‰€éœ€çš„å˜é‡ï¼Œå¦‚ä½•æ ¼å¼åŒ–è¾“å…¥ä»¥é€‚åº”Azure OpenAIçš„èŠå¤©æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•è§¦å‘æ¨¡å‹çš„å“åº”ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªPowerShelläº¤äº’å¼ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•è®¾ç½®Azure OpenAIå’ŒAzure Cognitive Searchçš„å…ƒæ•°æ®å˜é‡ï¼Œå¦‚ä½•æ„å»ºè¯·æ±‚ä½“ï¼Œå¦‚ä½•è®¾ç½®è®¤è¯å¤´ï¼Œä»¥åŠå¦‚ä½•å‘é€å®Œæˆè°ƒç”¨ä»¥ç”Ÿæˆç­”æ¡ˆã€‚æœ€åï¼Œè¿˜ç»™å‡ºäº†ä¸€ä¸ªç¤ºä¾‹è¾“å‡ºï¼Œå¹¶æä¾›äº†ä¸€äº›é‡è¦çš„æç¤ºå’Œä¸‹ä¸€æ­¥æ“ä½œã€‚

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

è¿™ä¸ªæ–‡ä»¶çš„ä¿®æ”¹ä¸»è¦æ˜¯æ›´æ–°äº†æ—¥æœŸï¼Œå¹¶åœ¨æ–‡ä»¶ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„åŒºåŸŸï¼Œè¯¥åŒºåŸŸåŒ…å«äº†ä¸€ä¸ªPowerShellå¿«é€Ÿå…¥é—¨çš„é“¾æ¥ã€‚

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

è¿™ä¸ªæ–‡ä»¶çš„ä¿®æ”¹æ˜¯åœ¨æ–‡ä»¶ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ç¼–ç¨‹è¯­è¨€é€‰é¡¹â€”â€”PowerShellã€‚

*************************

