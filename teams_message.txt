
[新特性] Azure OpenAI中使用PowerShell处理自有数据的新指南发布2023-11-23 04:15:28https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

这是一个新文件，主要包含了如何使用 PowerShell 在 Azure OpenAI 中使用自己的数据的示例代码。这些代码主要包括设置所需变量，示例 PowerShell 命令，以及示例输出。此外，还提供了一些重要的提示和建议，例如如何安全地存储和访问凭据，以及如何使用 web 应用与模型进行聊天。

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

在这个文件中，更新了日期，并在文件中添加了一个新的区域，该区域包含了 PowerShell 快速入门的链接。

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

在这个文件中，添加了一个新的编程语言 - PowerShell。

*************************

[模型] 子服务名称更新：从"core"更改为"prompt-flow"2023-11-23 07:52:01https://learn.microsoft.com/en-us/azure/machine-learning/how-to-retrieval-augmented-generation-cloud-to-local

这个文件的修改主要是更新了子服务的名称，从"core"更改为"prompt-flow"。

*************************

[Model] New PowerShell Sample for Azure OpenAI Chat Models Introduced2023-11-23 04:15:28https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

This new file provides a PowerShell sample for the "Use Your Own Data Quickstart" guide. It includes instructions and code snippets on how to set up required variables, example PowerShell commands for interacting with Azure OpenAI chat models, and tips on how to trigger a response from the model. It also provides an example output and important notes on credential security and troubleshooting. Lastly, it provides information on how to chat with the model using a web app.

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

This file has been updated to include the PowerShell quickstart guide in the programming language pivot section. The date of the document has also been updated.

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

This file has been updated to include PowerShell in the list of programming languages.

*************************

commit_patch_data :

From 27da4a67d93dc09aa736b005c751d6f1cad55c1b Mon Sep 17 00:00:00 2001
From: Jimmy Stridh <61634+jimmystridh@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:28:49 +0100
Subject: [PATCH] Update embedding example for beta9 api

+ fix compilation errors.
---
 articles/ai-services/openai/how-to/embeddings.md | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/how-to/embeddings.md b/articles/ai-services/openai/how-to/embeddings.md
index 8e3777d7abda..878d3bb2c3e9 100644
--- a/articles/ai-services/openai/how-to/embeddings.md
+++ b/articles/ai-services/openai/how-to/embeddings.md
@@ -80,11 +80,15 @@ AzureKeyCredential credentials = new (oaiKey);
 
 OpenAIClient openAIClient = new (oaiEndpoint, credentials);
 
-EmbeddingsOptions embeddingOptions = new ("Your text string goes here");
+EmbeddingsOptions embeddingOptions = new()
+{
+    DeploymentName = "text-embedding-ada-002",
+    Input = { "Your text string goes here" },
+};
 
-var returnValue = openAIClient.GetEmbeddings("YOUR_DEPLOYMENT_NAME", embeddingOptions);
+var returnValue = openAIClient.GetEmbeddings(embeddingOptions);
 
-foreach (float item in returnValue.Value.Data[0].Embedding)
+foreach (float item in returnValue.Value.Data[0].Embedding.ToArray())
 {
     Console.WriteLine(item);
 }



[模型] "EmbeddingsOptions"实例化和"GetEmbeddings"方法的重要修改
2023-11-22 08:28:49
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings

这个文件主要进行了一些代码的修改。首先，修复了编译错误。然后，对"EmbeddingsOptions"的实例化进行了修改，添加了"DeploymentName"和"Input"两个属性。最后，对"GetEmbeddings"方法的调用和对返回值的处理进行了修改。

*************************

commit_patch_data :

From 0ba0d00c18c4ebfa66264abdeebfcc3293f1cc25 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 08:44:48 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/latency.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 6461e21593c7..169852a518e6 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -20,7 +20,7 @@ This article will provide you with background around how latency works with Azur
 
 The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned. The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
 
-## Completions and chat completions
+## Improve performance
 
 ### Model selection
 
@@ -44,7 +44,7 @@ Streaming impacts perceived latency. If you have streaming enabled you'll receiv
 
 Sentiment analysis, language translation, content generation.
 
-There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
+There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response.
 
 ### Content filtering
 



[模型] "优化性能"标题更新及模型响应流描述删除
2023-11-22 13:44:48
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

这个文件主要进行了两处修改。首先，将"Completions and chat completions"标题更改为"Improve performance"。其次，删除了一段关于模型响应流的描述，即使从API到客户端的响应禁用了流，技术上模型本身总是在流式传输其响应。

*************************

commit_patch_data :

From a327c4725689c025b40d8c9ceaef2d309672dfb6 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:56:32 -0500
Subject: [PATCH] update

---
 .../openai/how-to/switching-endpoints.md      | 189 +++++++++++++++++-
 1 file changed, 187 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 7d45305693b2..635cae90c973 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,8 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-> [!NOTE]
-> This library is maintained by OpenAI and is currently in preview. Refer to the [release history](https://github.com/openai/openai-python/releases) or the [version.py commit history](https://github.com/openai/openai-python/commits/main/openai/version.py) to track the latest updates to the library.
+# [OpenAI Python 0.28.1](#tab/python)
 
 ## Authentication
 
@@ -201,6 +200,192 @@ embedding = openai.Embedding.create(
 </tr>
 </table>
 
+# [OpenAI Python 1.x](#tab/python-new)
+
+## Authentication
+
+We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
+
+### API key
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+import os
+from openai import AzureOpenAI
+    
+client = AzureOpenAI(
+    api_key=os.getenv("AZURE_OPENAI_KEY"),  
+    api_version="2023-10-01-preview",
+    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
+    )
+```
+
+</td>
+</tr>
+</table>
+
+<a name='azure-active-directory-authentication'></a>
+
+### Microsoft Entra authentication
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+
+
+
+
+```
+
+</td>
+<td>
+
+```python
+from azure.identity import DefaultAzureCredential, get_bearer_token_provider
+from openai import AzureOpenAI
+
+token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")
+
+api_version = "2023-12-01-preview"
+endpoint = "https://my-resource.openai.azure.com"
+
+client = AzureOpenAI(
+    api_version=api_version,
+    azure_endpoint=endpoint,
+    azure_ad_token_provider=token_provider,
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Keyword argument for model
+
+OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
+
+For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+completion = client.completions.create(
+    model='gpt-3.5-turbo-instruct',
+    prompt="<prompt>)
+)
+
+chat_completion = openai.chat.completions.create(
+    model="gpt-4",
+    messages="<messages>"
+)
+
+embedding = client.embeddings.create(
+    input="<input>",
+    model="text-embedding-ada-002"
+)
+```
+
+</td>
+<td>
+
+```python
+client.completions.create(
+    model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
+    prompt=<"prompt">
+)
+
+client.chat.completions.create(
+    model="gpt-35-turbo", # model = "deployment_name".
+    messages=<"messages">
+)
+
+client.embeddings.create(
+    input = "<input>",
+    model= "text-embedding-ada-002" # model = "deployment_name".
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Azure OpenAI embeddings multiple input support
+
+OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+inputs = ["A", "B", "C"] 
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002"
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+inputs = ["A", "B", "C"] #max array size=16
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
+  #engine="text-embedding-ada-002"
+)
+```
+
+</td>
+</tr>
+</table>
+
+---
+
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).



[模型] OpenAI Python 1.x认证方式及Azure OpenAI embeddings多输入支持的更新说明
2023-11-22 14:56:32
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

此次提交对文件进行了大量的更新，主要包括以下几个方面：

1. 删除了关于OpenAI库的注意事项，改为了OpenAI Python 0.28.1的标签。

2. 在文件中添加了大量的新内容，主要包括OpenAI Python 1.x的认证方式，包括API密钥和Microsoft Entra认证的代码示例。

3. 添加了关于模型关键字参数的说明，包括OpenAI和Azure OpenAI的使用差异。

4. 添加了关于Azure OpenAI embeddings多输入支持的说明，包括输入限制和代码示例。

5. 在文件末尾添加了下一步的学习链接。

*************************

commit_patch_data :

From dbffff50a4994b23b4fa171b747aa0bcc6ee4893 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:58:22 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 635cae90c973..8a8f70c55a10 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -7,7 +7,7 @@ ms.author: mbullwin #delegenz
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to
-ms.date: 07/20/2023
+ms.date: 11/22/2023
 manager: nitinme
 ---
 



commit_patch_data :

From ccbe042065a37d2fafafcba7efc31f7f7a2dc175 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 10:19:33 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/concepts/models.md | 6 +++---
 articles/ai-services/openai/faq.yml            | 4 ++++
 2 files changed, 7 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/concepts/models.md b/articles/ai-services/openai/concepts/models.md
index 0da897bfe2f4..91810be64fad 100644
--- a/articles/ai-services/openai/concepts/models.md
+++ b/articles/ai-services/openai/concepts/models.md
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI
 description: Learn about the different model capabilities that are available with Azure OpenAI. 
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 11/17/2023
+ms.date: 11/22/2023
 ms.custom: event-tier1-build-2022, references_regions, build-2023, build-2023-dataai
 manager: nitinme
 author: mrbullwinkle #ChrisHMSFT
@@ -91,9 +91,9 @@ See [model versions](../concepts/model-versions.md) to learn about how Azure Ope
 | `gpt-4-32k`(0314)  | 32,768               | Sep 2021         |
 | `gpt-4` (0613)     | 8,192                | Sep 2021         |
 | `gpt-4-32k` (0613) | 32,768               | Sep 2021         |
-| `gpt-4` (1106-preview)**<sup>1</sup>** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
+| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
 
-**<sup>1</sup>** We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
+**<sup>1</sup>** GPT-4 Turbo = `gpt-4` (1106-preview). To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
 
 > [!NOTE]
 > Regions where GPT-4 (0314) & (0613) are listed as available have access to both the 8K and 32K versions of the model
diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index f6650e033a3b..6a965e722272 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -26,6 +26,10 @@ sections:
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
           Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+      - question: |
+          I can't find GPT-4 Turbo?
+        answer:
+          GPT-4 Turbo is the `gpt-4` (1106-preview) model. To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
       - question: |
           Does Azure OpenAI support GPT-4?
         answer: |



[模型] GPT-4 Turbo模型的描述和部署方法更新
2023-11-22 15:19:33
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models

这个文件中的更改主要包括两部分。首先，更新了文件的日期，从2023年11月17日更改为2023年11月22日。其次，对模型的描述进行了修改，特别是对`gpt-4` (1106-preview)模型的描述，增加了"GPT-4 Turbo"的标签，并对如何部署这个模型的说明进行了补充。

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

在这个文件中，添加了一个新的问题和答案，问题是"I can't find GPT-4 Turbo?"，答案是关于如何找到和部署GPT-4 Turbo模型的说明。

*************************

commit_patch_data :

From 38a2e4d833e65431251244bee7feb4757174638f Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 10:26:19 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/concepts/models.md | 4 ++--
 articles/ai-services/openai/faq.yml            | 4 ++--
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/articles/ai-services/openai/concepts/models.md b/articles/ai-services/openai/concepts/models.md
index 91810be64fad..0f054cb1d7fd 100644
--- a/articles/ai-services/openai/concepts/models.md
+++ b/articles/ai-services/openai/concepts/models.md
@@ -91,9 +91,9 @@ See [model versions](../concepts/model-versions.md) to learn about how Azure Ope
 | `gpt-4-32k`(0314)  | 32,768               | Sep 2021         |
 | `gpt-4` (0613)     | 8,192                | Sep 2021         |
 | `gpt-4-32k` (0613) | 32,768               | Sep 2021         |
-| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
+| `gpt-4` (1106-preview)**<sup>1</sup>**<br>**GPT-4 Turbo Preview** | Input: 128,000  <br> Output: 4096           | Apr 2023         |
 
-**<sup>1</sup>** GPT-4 Turbo = `gpt-4` (1106-preview). To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
+**<sup>1</sup>** GPT-4 Turbo Preview = `gpt-4` (1106-preview). To deploy this model, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. We don't recommend using this model in production. We will upgrade all deployments of this model to a future stable version. Models designated preview do not follow the standard Azure OpenAI model lifecycle.
 
 > [!NOTE]
 > Regions where GPT-4 (0314) & (0613) are listed as available have access to both the 8K and 32K versions of the model
diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 6a965e722272..92a39d0394b9 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -27,9 +27,9 @@ sections:
         answer: |
           Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
-          I can't find GPT-4 Turbo?
+          I can't find GPT-4 Turbo Preview?
         answer:
-          GPT-4 Turbo is the `gpt-4` (1106-preview) model. To deploy GPT-4 Turbo, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
+          GPT-4 Turbo Preview is the `gpt-4` (1106-preview) model. To deploy this model, under **Deployments** select model **gpt-4**. For **Model version** select **1106-preview**. To check which regions this model is available, refer to the [models page](./concepts/models.md).
       - question: |
           Does Azure OpenAI support GPT-4?
         answer: |



[模型] "GPT-4 Turbo"模型名称及描述的重要更新
2023-11-22 15:26:19
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models

这个文件中的更改主要是对模型名称的修改，将"GPT-4 Turbo"更改为"GPT-4 Turbo Preview"。同时，对应的描述也进行了相应的修改，强调这是一个预览版模型。

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

在这个文件中，对于FAQ部分的问题"I can't find GPT-4 Turbo?"的答案也进行了修改，将"GPT-4 Turbo"更改为"GPT-4 Turbo Preview"，并对应修改了模型的描述。

*************************

commit_patch_data :

From c834902936f80682d16dc481ece7ebf78006d0b5 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:15:14 -0500
Subject: [PATCH] update

---
 .../openai/how-to/switching-endpoints.md      | 204 +-----------------
 1 file changed, 10 insertions(+), 194 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 8a8f70c55a10..90e3e9b09811 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,192 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-# [OpenAI Python 0.28.1](#tab/python)
-
-## Authentication
-
-We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
-
-### API key
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-
-openai.api_type = "azure"
-openai.api_key = "..."
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-<a name='azure-active-directory-authentication'></a>
-
-### Microsoft Entra authentication
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-from azure.identity import DefaultAzureCredential
-
-credential = DefaultAzureCredential()
-token = credential.get_token("https://cognitiveservices.azure.com/.default")
-
-openai.api_type = "azure_ad"
-openai.api_key = token.token
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-## Keyword argument for model
-
-OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
-
-For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    model="text-davinci-003"
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    model="gpt-4"
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  model="text-embedding-ada-002"
-)
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    deployment_id="text-davinci-003" # This must match the custom deployment name you chose for your model.
-    #engine="text-davinci-003" 
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    deployment_id="gpt-4" # This must match the custom deployment name you chose for your model.
-    #engine="gpt-4"
-
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-## Azure OpenAI embeddings multiple input support
-
-OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-inputs = ["A", "B", "C"] 
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  model="text-embedding-ada-002"
-)
-
-
-```
-
-</td>
-<td>
-
-```python
-inputs = ["A", "B", "C"] #max array size=16
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-# [OpenAI Python 1.x](#tab/python-new)
+This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -223,6 +38,7 @@ client = OpenAI(
 )
 
 
+
 ```
 
 </td>
@@ -266,6 +82,8 @@ client = OpenAI(
 
 
 
+
+
 ```
 
 </td>
@@ -310,7 +128,7 @@ completion = client.completions.create(
     prompt="<prompt>)
 )
 
-chat_completion = openai.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-4",
     messages="<messages>"
 )
@@ -325,17 +143,17 @@ embedding = client.embeddings.create(
 <td>
 
 ```python
-client.completions.create(
+completion = client.completions.create(
     model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
     prompt=<"prompt">
 )
 
-client.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-35-turbo", # model = "deployment_name".
     messages=<"messages">
 )
 
-client.embeddings.create(
+embedding = client.embeddings.create(
     input = "<input>",
     model= "text-embedding-ada-002" # model = "deployment_name".
 )
@@ -364,7 +182,6 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
-
 ```
 
 </td>
@@ -378,15 +195,14 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
   #engine="text-embedding-ada-002"
 )
+
 ```
 
 </td>
 </tr>
 </table>
 
----
-
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file



[模型] OpenAI和Azure OpenAI切换方法的重大更新
2023-11-22 16:15:14
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

此次提交对文件进行了大量的修改，删除了194行，添加了10行。主要的更改包括删除了大量的Python代码示例和相关的说明，这些代码示例主要是关于如何在OpenAI和Azure OpenAI之间切换。同时，也对一些现有的代码进行了修改，例如将一些函数的调用方式进行了更改。此外，还更新了一些文档的链接，并在文档的最后添加了一行新的内容。

*************************

commit_patch_data :

From 659bbf38c4b50ecb1aeedf80987724f50d30a389 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:17:59 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 90e3e9b09811..1b74c13e916a 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -2,8 +2,8 @@
 title: How to switch between OpenAI and Azure OpenAI Service endpoints with Python
 titleSuffix: Azure OpenAI Service
 description: Learn about the changes you need to make to your code to swap back and forth between OpenAI and Azure OpenAI endpoints.
-author: mrbullwinkle #dereklegenzoff
-ms.author: mbullwin #delegenz
+author: mrbullwinkle 
+ms.author: mbullwin 
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to



commit_patch_data :

From 85d411bb59d2a61a3c28de546c3697928ccf898b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:29:49 -0500
Subject: [PATCH] updatE

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 1 +
 1 file changed, 1 insertion(+)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 1b74c13e916a..bcaae948c9cb 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -182,6 +182,7 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
+
 ```
 
 </td>



commit_patch_data :

From 77b1b0cdf7e04579ec6146b5b3473b511a32cc2a Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 12:36:48 -0500
Subject: [PATCH 1/7] update

---
 articles/ai-services/openai/how-to/latency.md | 133 ++++++++++++++++++
 articles/ai-services/openai/toc.yml           |   2 +
 2 files changed, 135 insertions(+)
 create mode 100644 articles/ai-services/openai/how-to/latency.md

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
new file mode 100644
index 000000000000..8fe1c252e20a
--- /dev/null
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -0,0 +1,133 @@
+---
+title: Azure OpenAI Service performance & latency
+titleSuffix: Azure OpenAI
+description: Learn about performance and latency with Azure OpenAI
+services: cognitive-services
+manager: nitinme
+ms.service: cognitive-services
+ms.subservice: openai
+ms.topic: how-to
+ms.date: 11/21/2023
+author: mrbullwinkle 
+ms.author: mbullwin
+recommendations: false
+ms.custom:
+---
+
+# Performance and latency
+
+This article will provide you with background around how latency works with Azure OpenAI and how to optimize your environment to improve performance.
+
+## What is latency?
+
+The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned.The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
+
+## Completions and chat completions
+
+### Model selection
+
+Latency varies based on what model you are using. For an identical request, it is expected that `gpt-35-turbo` models will have a faster response, and therefore less latency than the first generation of `gpt-4` models.
+
+|Model|Relative Latency (fastest🚀 to slowest🐢)|
+|---|:---|
+|---|---|
+|`babbage-002` (fine-tuned)|🚀🚀🚀🚀🚀🚀🚀🚀  |
+| `gpt-35-turbo` (1106) |  🚀🚀🚀🚀🚀🚀🚀   |
+| `gpt-4` (1106-preview) | 🚀🚀🚀🚀🚀🚀 |
+| `gpt-35-turbo-instruct`| 🚀🚀🚀🚀🚀  |
+| `gpt-35-turbo` (0314-0613) |🚀🚀🚀🚀  |
+| `davinci-002`(fine-tuned) |🚀🚀🚀 
+| `gpt-35-turbo-16k` | 🚀🚀🚀 |
+| `gpt-4` (0314-0613) | 🚀🚀 |
+| `gpt-4-32k` (0314-0613) |🐢🚀 |
+
+If you are concerned about latency model selection is extremely important. You need to balance your needs around model capabilities, quality of response, cost, and overall latency. Depending on your use case a smaller well trained fine-tuned model like `babbage-002` may be able to offer the best overall experience. But creating a high quality fine-tuned model comes with additional costs as well as complexity.  
+
+<!--
+Questions:
+1. Can we provide an ordered list of models from fastest to slowest from a latency perspective? 
+2. Is it possible to have noticable regional differences between model latency in the sense of in Region A model is running on A100s, whereas in Region B the identical model is running on H100's. Is each class of model always running on the same speed underlying hardware? 
+ -->
+
+### Max tokens and stop sequences
+
+When you send a completion request to the Azure OpenAI endpoint your input text is converted to tokens which are then sent to your deployed model. The model receives the input tokens and then begins generating a response. It's an iterative sequential process, one token at a time. Another way to think of it is like a for loop with `n tokens = n iterations`.
+
+So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter as well as with `stop` sequences. Reducing the number of tokens generated per request will reduce the latency of each request.
+<!--
+Question:
+1. What is the default max_token value for each model spec says 16 is default but does this vary?
+ -->
+
+### Streaming
+
+Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
+
+If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
+
+### Content filtering
+
+[Content filtering](../concepts/content-filter.md) and other content safety related features within Azure OpenAI increase latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where modifying the content filters to improve performance might be worth exploring.
+
+Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
+
+<!--
+Question:
+
+1. I have heard about deferred safety in shiproom? (Is this released for Azure OpenAI 3P, how do customers access this?)
+2. If I set content filtering to high only does this improve latency or is this ultimately the same whereby latency is only improved if content filtering is off altogether or deferred safety is used?
+ -->
+
+### best_of & n parameters
+
+**best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs. Setting these parameters to values greater than 1, creates increased latency.
+
+### Quota and rate limits
+
+[Quota and rate limits](./quota.md) can play a huge role in the overall request latency you experience.
+
+If you have insufficient [quota](./quota.md) to meet your peak usage requirements this creates a situation where it appears that the model is taking longer to respond, when in reality the model response time itself is remaining the same, but some percentage of calls to the model are not being delivered and are then retried until your traffic drops back below the rate limit thresholds.
+
+The default behavior for the `0.28.1` OpenAI Python API library was to generate a failure message when the rate limit was reached. Whereas the new `1.x` Python API library, and the Azure OpenAI SDK's for C#, Java, JavaScript, and Go, will initially all fail silently when `HTTP 429 (Too many requests)` errors occur and then retry sending the request multiple times.
+
+This means that if you are measuring overall latency of your Azure OpenAI requests, but not also measuring how many `HTTP 429`'s are occuring you are getting an incomplete picture of what is creating latency in your application.
+
+<!--
+Question:
+1. I was told by Mona don't use blocked calls this metric is inaccurate, but when I test blocked calls is the only thing recording 429's
+2. New Metric is not recording 429's?
+ -->
+
+If you find that maxing out standard quota is not sufficient for your performance needs, you may need to explore [provisioned throughout](../concepts/provisioned-throughput.md) which provides a more stable latency experience.
+
+### Regional availabilty
+
+Azure OpenAI models are available in datacenters around the world. While the geographic distance between client and endpoint is not a major contributor to latency different regions have different max quota allocations available and so it may be possible to [reduce latency by migrating to a different region](../quotas-limits.md).
+
+## Summary
+
+* **Model latency**: Some models are faster than others. Experiment with using different models to see if a faster model is appropriate for your use case.
+
+* **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
+
+* **Lower total tokens generated**: The fewer tokens generated the faster the overall response will be. Remember this is like having a for loop with `n tokens = n iterations`. Lower the number of tokens generated and overall response time will improve accordingly.
+
+* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready. While the last token will still take just as long to be generated.
+
+* **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
+
+* **Content Filtering** impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
+
+* **best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs, which occurs when these parameters are set to values greater than 1.
+
+* **Audit blocked call** frequency in **Metrics**. Split by **RatelimitKey** and **Region** to identify specific model deployments that are impacted.
+
+* **Configure [Diagnostic Settings](/azure/ai-services/openai/how-to/monitoring#configure-diagnostic-settings)** across your Azure OpenAI resources to enable consolidated in-depth reporting with Log Analytics workspaces.
+
+* **Use [quota management](/azure/ai-services/openai/how-to/quota?tabs=rest)** to increase TPM on deployments with high traffic, and to reduce TPM on deployments with limited needs.
+
+* **Implement retry logic in your application** if it is not already present. But when diagnosing performance issues, you should also take into account your client SDK's currently configured retry behavior as it may improve the user experience while masking an underlying rate limit issue.
+
+* **Avoid sharp changes in the workload**. Increase the workload gradually.
+
+* **Test different load increase patterns**.
diff --git a/articles/ai-services/openai/toc.yml b/articles/ai-services/openai/toc.yml
index bf4b71f6c106..94128fe2bd6d 100644
--- a/articles/ai-services/openai/toc.yml
+++ b/articles/ai-services/openai/toc.yml
@@ -116,6 +116,8 @@ items:
         href: ./how-to/monitoring.md
       - name: Plan and manage costs
         href: ./how-to/manage-costs.md
+      - name: Performance & latency
+        href: ./how-to/latency.md
       - name: Role-based access control (Azure RBAC)
         href: ./how-to/role-based-access-control.md
       - name: Business continuity & disaster recovery (BCDR)

From a3de2323583a42e55274c37ddd94b3b8022f0c6a Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 13:28:10 -0500
Subject: [PATCH 2/7] update

---
 articles/ai-services/openai/how-to/latency.md | 1 -
 1 file changed, 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 8fe1c252e20a..30d646c589e0 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -30,7 +30,6 @@ Latency varies based on what model you are using. For an identical request, it i
 
 |Model|Relative Latency (fastest🚀 to slowest🐢)|
 |---|:---|
-|---|---|
 |`babbage-002` (fine-tuned)|🚀🚀🚀🚀🚀🚀🚀🚀  |
 | `gpt-35-turbo` (1106) |  🚀🚀🚀🚀🚀🚀🚀   |
 | `gpt-4` (1106-preview) | 🚀🚀🚀🚀🚀🚀 |

From 1f70420967c6d85aef958f093a372a0c0ca0a876 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 13:30:13 -0500
Subject: [PATCH 3/7] update

---
 articles/ai-services/openai/how-to/latency.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 30d646c589e0..6ff9f20df36f 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -111,7 +111,7 @@ Azure OpenAI models are available in datacenters around the world. While the geo
 
 * **Lower total tokens generated**: The fewer tokens generated the faster the overall response will be. Remember this is like having a for loop with `n tokens = n iterations`. Lower the number of tokens generated and overall response time will improve accordingly.
 
-* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready. While the last token will still take just as long to be generated.
+* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready.
 
 * **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
 

From d8998cf0565f13534e0a60084737e208d4172a21 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 16:59:42 -0500
Subject: [PATCH 4/7] update

---
 articles/ai-services/openai/how-to/latency.md | 97 ++++---------------
 1 file changed, 17 insertions(+), 80 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 6ff9f20df36f..88c30017ae26 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -2,10 +2,8 @@
 title: Azure OpenAI Service performance & latency
 titleSuffix: Azure OpenAI
 description: Learn about performance and latency with Azure OpenAI
-services: cognitive-services
 manager: nitinme
-ms.service: cognitive-services
-ms.subservice: openai
+ms.service: azure-ai-openai
 ms.topic: how-to
 ms.date: 11/21/2023
 author: mrbullwinkle 
@@ -26,86 +24,41 @@ The high level definition of latency in this context is the amount of time it ta
 
 ### Model selection
 
-Latency varies based on what model you are using. For an identical request, it is expected that `gpt-35-turbo` models will have a faster response, and therefore less latency than the first generation of `gpt-4` models.
+Latency varies based on what model you are using. For an identical request, it is expected that different models will have a different latency. If your use case requires the lowest latency models with the fastest response times we recommend the latest models in the [GPT-3.5 Turbo model series](../concepts/models.md#gpt-35-models).
 
-|Model|Relative Latency (fastest🚀 to slowest🐢)|
-|---|:---|
-|`babbage-002` (fine-tuned)|🚀🚀🚀🚀🚀🚀🚀🚀  |
-| `gpt-35-turbo` (1106) |  🚀🚀🚀🚀🚀🚀🚀   |
-| `gpt-4` (1106-preview) | 🚀🚀🚀🚀🚀🚀 |
-| `gpt-35-turbo-instruct`| 🚀🚀🚀🚀🚀  |
-| `gpt-35-turbo` (0314-0613) |🚀🚀🚀🚀  |
-| `davinci-002`(fine-tuned) |🚀🚀🚀 
-| `gpt-35-turbo-16k` | 🚀🚀🚀 |
-| `gpt-4` (0314-0613) | 🚀🚀 |
-| `gpt-4-32k` (0314-0613) |🐢🚀 |
-
-If you are concerned about latency model selection is extremely important. You need to balance your needs around model capabilities, quality of response, cost, and overall latency. Depending on your use case a smaller well trained fine-tuned model like `babbage-002` may be able to offer the best overall experience. But creating a high quality fine-tuned model comes with additional costs as well as complexity.  
-
-<!--
-Questions:
-1. Can we provide an ordered list of models from fastest to slowest from a latency perspective? 
-2. Is it possible to have noticable regional differences between model latency in the sense of in Region A model is running on A100s, whereas in Region B the identical model is running on H100's. Is each class of model always running on the same speed underlying hardware? 
- -->
-
-### Max tokens and stop sequences
+### Max tokens
 
 When you send a completion request to the Azure OpenAI endpoint your input text is converted to tokens which are then sent to your deployed model. The model receives the input tokens and then begins generating a response. It's an iterative sequential process, one token at a time. Another way to think of it is like a for loop with `n tokens = n iterations`.
 
-So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter as well as with `stop` sequences. Reducing the number of tokens generated per request will reduce the latency of each request.
-<!--
-Question:
-1. What is the default max_token value for each model spec says 16 is default but does this vary?
- -->
+So another important factor when evaluating latency is how many tokens are being generated. This is controlled largely via the `max_tokens` parameter. Reducing the number of tokens generated per request will reduce the latency of each request.
 
 ### Streaming
 
-Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
-
-If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
-
-### Content filtering
-
-[Content filtering](../concepts/content-filter.md) and other content safety related features within Azure OpenAI increase latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where modifying the content filters to improve performance might be worth exploring.
-
-Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
-
-<!--
-Question:
-
-1. I have heard about deferred safety in shiproom? (Is this released for Azure OpenAI 3P, how do customers access this?)
-2. If I set content filtering to high only does this improve latency or is this ultimately the same whereby latency is only improved if content filtering is off altogether or deferred safety is used?
- -->
+**Examples when to use streaming**:
 
-### best_of & n parameters
+Chat bots and conversational interfaces.
 
-**best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs. Setting these parameters to values greater than 1, creates increased latency.
-
-### Quota and rate limits
+Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
 
-[Quota and rate limits](./quota.md) can play a huge role in the overall request latency you experience.
+**Examples when streaming is less important**:
 
-If you have insufficient [quota](./quota.md) to meet your peak usage requirements this creates a situation where it appears that the model is taking longer to respond, when in reality the model response time itself is remaining the same, but some percentage of calls to the model are not being delivered and are then retried until your traffic drops back below the rate limit thresholds.
+Sentiment analysis, language translation, content generation.
 
-The default behavior for the `0.28.1` OpenAI Python API library was to generate a failure message when the rate limit was reached. Whereas the new `1.x` Python API library, and the Azure OpenAI SDK's for C#, Java, JavaScript, and Go, will initially all fail silently when `HTTP 429 (Too many requests)` errors occur and then retry sending the request multiple times.
+There are many use cases where you are performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response. While you do have the ability to choose to enable or disable streaming from an API to client response perspective, technically the model itself is always streaming its response.
 
-This means that if you are measuring overall latency of your Azure OpenAI requests, but not also measuring how many `HTTP 429`'s are occuring you are getting an incomplete picture of what is creating latency in your application.
+### Content filtering
 
-<!--
-Question:
-1. I was told by Mona don't use blocked calls this metric is inaccurate, but when I test blocked calls is the only thing recording 429's
-2. New Metric is not recording 429's?
- -->
+Azure OpenAI includes a [content filtering system](./content-filters.md) that works alongside core models. This system works by running both the prompt and completion through an ensemble of classification models aimed at detecting and preventing the output of harmful content.
 
-If you find that maxing out standard quota is not sufficient for your performance needs, you may need to explore [provisioned throughout](../concepts/provisioned-throughput.md) which provides a more stable latency experience.
+The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
 
-### Regional availabilty
+The addition of content filtering comes with an increase in safety, but also latency.There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
 
-Azure OpenAI models are available in datacenters around the world. While the geographic distance between client and endpoint is not a major contributor to latency different regions have different max quota allocations available and so it may be possible to [reduce latency by migrating to a different region](../quotas-limits.md).
+Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
 
 ## Summary
 
-* **Model latency**: Some models are faster than others. Experiment with using different models to see if a faster model is appropriate for your use case.
+* **Model latency**: If model latency is important to you we recommend trying out our latest latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
 
 * **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
 
@@ -113,20 +66,4 @@ Azure OpenAI models are available in datacenters around the world. While the geo
 
 * **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready.
 
-* **Regional availability**: Azure OpenAI is available in multiple regions globally. Evaluate which regions best fit your needs based both on [region specific default TPM/RPM limits](../quotas-limits.md) as well as the round trip latency between your client traffic and a given endpoint.  
-
-* **Content Filtering** impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
-
-* **best_of** & **n**: parameters can greatly increase latency if they're configured to generate multiple outputs, which occurs when these parameters are set to values greater than 1.
-
-* **Audit blocked call** frequency in **Metrics**. Split by **RatelimitKey** and **Region** to identify specific model deployments that are impacted.
-
-* **Configure [Diagnostic Settings](/azure/ai-services/openai/how-to/monitoring#configure-diagnostic-settings)** across your Azure OpenAI resources to enable consolidated in-depth reporting with Log Analytics workspaces.
-
-* **Use [quota management](/azure/ai-services/openai/how-to/quota?tabs=rest)** to increase TPM on deployments with high traffic, and to reduce TPM on deployments with limited needs.
-
-* **Implement retry logic in your application** if it is not already present. But when diagnosing performance issues, you should also take into account your client SDK's currently configured retry behavior as it may improve the user experience while masking an underlying rate limit issue.
-
-* **Avoid sharp changes in the workload**. Increase the workload gradually.
-
-* **Test different load increase patterns**.
+* **Content Filtering** improves safety, but it also impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](./content-filters.md).
\ No newline at end of file

From 5b715be93bc622195950b683fa1a04e66d98a55b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 17:07:32 -0500
Subject: [PATCH 5/7] update

---
 articles/ai-services/openai/how-to/latency.md | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 88c30017ae26..9c2b054b2b9f 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -18,7 +18,7 @@ This article will provide you with background around how latency works with Azur
 
 ## What is latency?
 
-The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned.The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
+The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type as well as the number of tokens generated and returned. The number of tokens sent to the model as part of the input token limit, has a much smaller overall impact on latency.
 
 ## Completions and chat completions
 
@@ -34,13 +34,13 @@ So another important factor when evaluating latency is how many tokens are being
 
 ### Streaming
 
-**Examples when to use streaming**:
+**Examples of when to use streaming**:
 
 Chat bots and conversational interfaces.
 
-Streaming impacts preceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective this can feel like the model is responding faster even though the overall time to complete the request remains the same.
+Streaming impacts perceived latency. If you have streaming enabled you'll receive tokens back in chunks as soon as they're available. From a user perspective, this often feels like the model is responding faster even though the overall time to complete the request remains the same.
 
-**Examples when streaming is less important**:
+**Examples of when streaming is less important**:
 
 Sentiment analysis, language translation, content generation.
 
@@ -52,13 +52,13 @@ Azure OpenAI includes a [content filtering system](./content-filters.md) that wo
 
 The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
 
-The addition of content filtering comes with an increase in safety, but also latency.There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
+The addition of content filtering comes with an increase in safety, but also latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.
 
 Learn more about requesting modifications to the default, [content filtering policies](./content-filters.md).
 
 ## Summary
 
-* **Model latency**: If model latency is important to you we recommend trying out our latest latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
+* **Model latency**: If model latency is important to you we recommend trying out our latest models in the [GPT-3.5 Turbo model series](../concepts/models.md).
 
 * **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.
 

From e0abbb3a8d2dd05731b5082fc589521ce312da95 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Tue, 21 Nov 2023 17:17:41 -0500
Subject: [PATCH 6/7] update

---
 articles/ai-services/openai/how-to/latency.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/latency.md b/articles/ai-services/openai/how-to/latency.md
index 9c2b054b2b9f..6461e21593c7 100644
--- a/articles/ai-services/openai/how-to/latency.md
+++ b/articles/ai-services/openai/how-to/latency.md
@@ -48,7 +48,7 @@ There are many use cases where you are performing some bulk task where you only
 
 ### Content filtering
 
-Azure OpenAI includes a [content filtering system](./content-filters.md) that works alongside core 


[模型] "Azure OpenAI性能和延迟问题的大幅度修改和优化"
2023-11-22 17:33:26
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交创建了一个新文件，该文件详细介绍了Azure OpenAI的性能和延迟问题。内容包括延迟的定义，以及影响延迟的各种因素，如模型选择，最大令牌数，流式传输，内容过滤等。此外，还提供了一些优化性能的建议。

https://learn.microsoft.com/en-us/azure/ai-services/openai/toc

此提交在目录文件中添加了一个新的条目，链接到新创建的关于性能和延迟的文章。

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交删除了一个多余的表格行。

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交修改了关于流式传输的描述，删除了“尽管最后一个令牌的生成时间仍然一样长”的部分。

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交大幅度修改了文件，删除了大量内容，包括关于模型选择，最大令牌和停止序列，内容过滤，最佳参数和n参数，配额和速率限制，区域可用性等的详细讨论。同时，对一些部分进行了简化和概括。

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交对文件进行了一些小的编辑和修订，包括修正了一些语法错误，改进了一些表述。

https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency

此提交对文件进行了一些小的编辑和修订，包括修正了一些语法错误，改进了一些表述。

*************************

commit_patch_data :

From 0dc8c41ea82cd1641fe65dd5f6e444d7a60f7b48 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Jo=C3=A3o=20Tavares?=
 <60930147+jotavar@users.noreply.github.com>
Date: Wed, 8 Nov 2023 11:09:10 +0000
Subject: [PATCH 01/63] Added alert regarding recommended minimum number of
 CoreDNS pod replicas

---
 articles/aks/coredns-custom.md | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/articles/aks/coredns-custom.md b/articles/aks/coredns-custom.md
index 4c773ab8df11..eeaa5ccbd2a8 100644
--- a/articles/aks/coredns-custom.md
+++ b/articles/aks/coredns-custom.md
@@ -224,6 +224,9 @@ Sudden spikes in DNS traffic within AKS clusters are a common occurrence due to
 
 CoreDNS uses [horizontal cluster proportional autoscaler][cluster-proportional-autoscaler] for pod auto scaling. The `coredns-autoscaler` ConfigMap can be edited to configure the scaling logic for the number of CoreDNS pods. The `coredns-autoscaler` ConfigMap currently supports two different ConfigMap key values: `linear` and `ladder` which correspond to two supported control modes. The `linear` controller yields a number of replicas in [min,max] range equivalent to `max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )`. The `ladder` controller calculates the number of replicas by consulting two different step functions, one for core scaling and another for node scaling, yielding the max of the two replica values. For more information on the control modes and ConfigMap format, please consult the [upstream documentation][cluster-proportional-autoscaler-control-patterns].
 
+> [!IMPORTANT]
+> A minimum of 2 CoreDNS pod replicas per cluster is recommended. Configuring a minimum of 1 CoreDNS pod replica may result in failures during operations which require node draining, such as cluster upgrade operations.
+
 To retrieve the `coredns-autoscaler` ConfigMap, you can run the `kubectl get configmap coredns-autoscaler -n kube-system -o yaml` command which will return the following:
 
 ```yaml

From d6763759429288dd3e4378cb95d91ed04b7fe5cc Mon Sep 17 00:00:00 2001
From: Gearoid O'Donnell <110535959+gearoidodonnell@users.noreply.github.com>
Date: Wed, 8 Nov 2023 18:16:45 +0000
Subject: [PATCH 02/63] Updated overview.md

---
 articles/active-directory-b2c/overview.md | 22 ++++++++++------------
 1 file changed, 10 insertions(+), 12 deletions(-)

diff --git a/articles/active-directory-b2c/overview.md b/articles/active-directory-b2c/overview.md
index f526c1c2b515..39d0744c5734 100644
--- a/articles/active-directory-b2c/overview.md
+++ b/articles/active-directory-b2c/overview.md
@@ -1,10 +1,8 @@
 ---
 title: What is Azure Active Directory B2C?
 description: Learn how you can use Azure Active Directory B2C to support external identities in your applications, including social sign-up with Facebook, Google, and other identity providers.
-services: active-directory-b2c
 author: garrodonnell
 manager: CelesteDG
-
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
@@ -12,28 +10,31 @@ ms.date: 10/26/2022
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
+
+# Customer intent: As an IT admin or developer, I need to understand what Azure AD B2C is and how it can help me build a customer-facing application.
+
 ---
 
 # What is Azure Active Directory B2C?
 
-Azure Active Directory B2C provides business-to-customer identity as a service. Your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.
+Azure Active Directory B2C provides business-to-customer identity as a service. Your customers can use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.
 
 ![Infographic of Azure AD B2C identity providers and downstream applications](./media/overview/azureadb2c-overview.png)
 
 Azure AD B2C is a customer identity access management (CIAM) solution capable of supporting millions of users and billions of authentications per day. It takes care of the scaling and safety of the authentication platform, monitoring, and automatically handling threats like denial-of-service, password spray, or brute force attacks.
 
-Azure AD B2C is a separate service from [Microsoft Entra ID](../active-directory/fundamentals/whatis.md). It is built on the same technology as Microsoft Entra ID but for a different purpose. It allows businesses to build customer facing applications, and then allow anyone to sign-up and into those applications with no restrictions on user account.
+Azure AD B2C is built on the same technology as [Microsoft Entra ID](../active-directory/fundamentals/whatis.md) but for a different purpose and is a separate service. It allows businesses to build customer facing applications, and then allow anyone to sign up and sign in to those applications with no restrictions on user account.
    
 ## Who uses Azure AD B2C?
-Any business or individual who wishes to authenticate end users to their web/mobile applications using a white-label authentication solution. Apart from authentication, Azure AD B2C service is used for authorization such as access to API resources by authenticated users. Azure AD B2C is designed to be used by **IT administrators** and **developers**.
+Any business or individual who wishes to authenticate end users to their web or mobile applications using a white-label authentication solution. Apart from authentication, Azure AD B2C service is used for authorization such as access to API resources by authenticated users. Azure AD B2C is designed to be used by **IT administrators** and **developers**.
 
 ## Custom-branded identity solution
 
-Azure AD B2C is a white-label authentication solution. You can customize the entire user experience with your brand so that it blends seamlessly with your web and mobile applications.
+Azure AD B2C is a white-label authentication solution which means you can customize the entire user experience with your brand so that it blends seamlessly with your web and mobile applications.
 
-Customize every page displayed by Azure AD B2C when your users sign-up, sign in, and modify their profile information. Customize the HTML, CSS, and JavaScript in your user journeys so that the Azure AD B2C experience looks and feels like it's a native part of your application.
+Customize every page displayed by Azure AD B2C when your users sign up, sign in, and modify their profile information. Customize the HTML, CSS, and JavaScript in your user journeys so that the Azure AD B2C experience looks and feels like it's a native part of your application.
 
-![Customized sign-up and sign-in pages and background image](./media/overview/sign-in-small.png)
+:::image type="content" source="./media/overview/sign-in-small.png" alt-text="Customized sign-up and sign-in pages and background image":::
 
 ## Single sign-on access with a user-provided identity
 
@@ -51,7 +52,7 @@ Another external user store scenario is to have Azure AD B2C handle the authenti
 
 :::image type="content" source="./media/overview/scenario-remoteprofile.png" alt-text="A logical diagram of Azure AD B2C communicating with an external user store.":::
 
-Azure AD B2C can facilitate collecting the information from the user during registration or profile editing, then hand that data off to the external system via API. Then, during future authentications, Azure AD B2C can retrieve the data from the external system and, if needed, include it as a part of the authentication token response it sends to your application.
+Azure AD B2C can facilitate collecting information from a user during registration or profile editing, then hand that data off to an external system via API. Then, during future authentications, Azure AD B2C can retrieve that data from the external system and, if needed, include it as a part of the authentication token response it sends to your application.
 
 ## Progressive profiling
 
@@ -63,11 +64,8 @@ Another user journey option includes progressive profiling. Progressive profilin
 
 Use Azure AD B2C to facilitate identity verification and proofing by collecting user data, then passing it to a third-party system to perform validation, trust scoring, and approval for user account creation.
 
-
 :::image type="content" source="./media/overview/scenario-idproofing.png" alt-text="A diagram showing the user flow for third-party identity proofing.":::
 
-You have learned some of the things you can do with Azure AD B2C as your business-to-customer identity platform. You may now move on directly to a more in-depth [technical overview of Azure AD B2C](technical-overview.md).
-
 ## Next steps
 
 Now that you have an idea of what Azure AD B2C is and some of the scenarios it can help with, dig a little deeper into its features and technical aspects.

From 3ca025af661ba5cf29ebaa126c9bfafc877a2cbd Mon Sep 17 00:00:00 2001
From: Gearoid O'Donnell <110535959+gearoidodonnell@users.noreply.github.com>
Date: Thu, 9 Nov 2023 08:42:23 +0000
Subject: [PATCH 03/63] Freshness review for technical-overview.md

---
 articles/active-directory-b2c/overview.md     |  4 +-
 .../technical-overview.md                     | 75 +++++++++----------
 2 files changed, 37 insertions(+), 42 deletions(-)

diff --git a/articles/active-directory-b2c/overview.md b/articles/active-directory-b2c/overview.md
index 39d0744c5734..c2e2eb629137 100644
--- a/articles/active-directory-b2c/overview.md
+++ b/articles/active-directory-b2c/overview.md
@@ -6,12 +6,12 @@ manager: CelesteDG
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
-ms.date: 10/26/2022
+ms.date: 08/11/2023
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
 
-# Customer intent: As an IT admin or developer, I need to understand what Azure AD B2C is and how it can help me build a customer-facing application.
+# Customer intent: As a technical or non-technical customer, I need to understand at a high level what Azure AD B2C is and how it can help me build a customer-facing application.
 
 ---
 
diff --git a/articles/active-directory-b2c/technical-overview.md b/articles/active-directory-b2c/technical-overview.md
index 25a38654baee..21e8dc2bae39 100644
--- a/articles/active-directory-b2c/technical-overview.md
+++ b/articles/active-directory-b2c/technical-overview.md
@@ -1,38 +1,39 @@
 ---
 title: Technical and feature overview - Azure Active Directory B2C
 description: An in-depth introduction to the features and technologies in Azure Active Directory B2C. Azure Active Directory B2C has high availability globally. 
-services: active-directory-b2c
 author: garrodonnell
 manager: CelesteDG
-
 ms.service: active-directory
 ms.workload: identity
 ms.topic: overview
-ms.date: 10/26/2022
+ms.date: 11/08/2023
 ms.custom: engagement-fy23
 ms.author: godonnell
 ms.subservice: B2C
+
+# Customer intent: As an IT admin or developer, I need to understand in more detail the technical aspects and features of Azure AD B2C and how it can help me build a customer-facing application.
+
 ---
 
 # Technical and feature overview of Azure Active Directory B2C
 
-A companion to [About Azure Active Directory B2C](overview.md), this article provides a more in-depth introduction to the service. Discussed here are the primary resources you work with in the service, its features. Learn how these features enable you to provide a fully custom identity experience for your customers in your applications.
+This article is a companion to [About Azure Active Directory B2C](overview.md) and provides a more in-depth introduction to the service. We will discuss here the primary resources you work with in the service, its features and learn how they enable you to provide a fully custom identity experience for customers in your applications.
 
 ## Azure AD B2C tenant
 
-In Azure Active Directory B2C (Azure AD B2C), a *tenant* represents your organization and is a directory of users. Each Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants. An Azure AD B2C tenant is different than a Microsoft Entra tenant, which you may already have.
+In Azure Active Directory B2C (Azure AD B2C), a *tenant* represents your organization and is a directory of users. Each Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants. An Azure AD B2C tenant is also different from a Microsoft Entra tenant, which you may already have.
 
 The primary resources you work with in an Azure AD B2C tenant are:
 
-* **Directory** - The *directory* is where Azure AD B2C stores your users' credentials, profile data, and your application registrations.
-* **Application registrations** - Register your web, mobile, and native applications with Azure AD B2C to enable identity management. You can also register any APIs you want to protect with Azure AD B2C.
-* **User flows** and **custom policies** - Create identity experiences for your applications with built-in user flows and fully configurable custom policies:
+* **Directory** - This is where Azure AD B2C stores your users' credentials, profile data, and your application registrations.
+* **Application registrations** - You can register your web, mobile, and native applications with Azure AD B2C to enable identity management. You can also register any APIs you want to protect with Azure AD B2C.
+* **User flows** and **custom policies** - These are used to create identity experiences for your applications with built-in user flows and fully configurable custom policies:
   * **User flows** help you quickly enable common identity tasks like sign-up, sign-in, and profile editing.
   * **Custom policies** let you build complex identity workflows unique to your organization, customers, employees, partners, and citizens.
 * **Sign-in options** - Azure AD B2C offers various [sign-up and sign-in options](sign-in-options.md) for users of your applications:
-  * **Username, email, and phone sign-in** - Configure your Azure AD B2C local accounts to allow sign-up and sign-in with a username, email address, phone number, or a combination of methods.
-  * **Social identity providers** - Federate with social providers like Facebook, LinkedIn, or Twitter.
-  * **External identity providers** - Federate with standard identity protocols like OAuth 2.0, OpenID Connect, and more.
+  * **Username, email, and phone sign-in** - You can configure your Azure AD B2C local accounts to allow sign up and sign in with a username, email address, phone number, or a combination of methods.
+  * **Social identity providers** - You can federate with social providers like Facebook, LinkedIn, or Twitter.
+  * **External identity providers** - You can also federate with standard identity protocols like OAuth 2.0, OpenID Connect, and more.
 * **Keys** - Add and manage encryption keys for signing and validating tokens, client secrets, certificates, and passwords.
 
 An Azure AD B2C tenant is the first resource you need to create to get started with Azure AD B2C. Learn how to:
@@ -45,8 +46,8 @@ An Azure AD B2C tenant is the first resource you need to create to get started w
 Azure AD B2C defines several types of user accounts. Microsoft Entra ID, Microsoft Entra B2B, and Azure Active Directory B2C share these account types.
 
 * **Work account** - Users with work accounts can manage resources in a tenant, and with an administrator role, can also manage tenants. Users with work accounts can create new consumer accounts, reset passwords, block/unblock accounts, and set permissions or assign an account to a security group.
-* **Guest account** - External users you invite to your tenant as guests. A typical scenario for inviting a guest user to your Azure AD B2C tenant is to share administration responsibilities.
-* **Consumer account** - Accounts that are managed by Azure AD B2C user flows and custom policies.
+* **Guest account** - These are external users you invite to your tenant as guests. A typical scenario for inviting a guest user to your Azure AD B2C tenant is to share administration responsibilities.
+* **Consumer account** - These are accounts that are managed by Azure AD B2C user flows and custom policies.
 
 :::image type="content" source="media/technical-overview/portal-01-users.png" alt-text="Screenshot of the Azure AD B2C user management page in the Azure portal.":::<br/>*Figure: User directory within an Azure AD B2C tenant in the Azure portal.*
 
@@ -67,7 +68,7 @@ For more information, see [Overview of user accounts in Azure Active Directory B
 
 ## Local account sign-in options
 
-Azure AD B2C provides various ways in which you can authenticate a user. Users can sign-in to a local account, by using username and password, phone verification (also known as password-less authentication). Email sign-up is enabled by default in your local account identity provider settings.
+Azure AD B2C provides various ways in which you can authenticate a user. Users can sign-in to a local account, by using username and password, phone verification (also known as passwordless authentication). Email sign-up is enabled by default in your local account identity provider settings.
 
 Learn more about [sign-in options](sign-in-options.md) or how to [set up the local account identity provider](identity-provider-local.md).
 
@@ -75,14 +76,14 @@ Learn more about [sign-in options](sign-in-options.md) or how to [set up the loc
 
 Azure AD B2C lets you manage common attributes of consumer account profiles. For example display name, surname, given name, city, and others.
 
-You can also extend the Microsoft Entra schema to store additional information about your users. For example, their country/region of residency, preferred language, and preferences like whether they want to subscribe to a newsletter or enable multifactor authentication. For more information, see:
+You can also extend the underlying Microsoft Entra ID schema to store additional information about your users. For example, their country/region of residency, preferred language, and preferences like whether they want to subscribe to a newsletter or enable multifactor authentication. For more information, see:
 
 * [User profile attributes](user-profile-attributes.md)
 * [Add user attributes and customize user input in](configure-user-input.md)
 
 ## Sign-in with external identity providers
 
-You can configure Azure AD B2C to allow users to sign in to your application with credentials from social and enterprise identity providers. Azure AD B2C can federate with identity providers that support OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML protocols. For example, Facebook, Microsoft account, Google, Twitter, and AD-FS.
+You can configure Azure AD B2C to allow users to sign in to your application with credentials from social and enterprise identity providers. Azure AD B2C can federate with identity providers that support OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML protocols. For example, Facebook, Microsoft account, Google, Twitter, and Active Directory Federation Service (AD FS).
 
 :::image type="content" source="media/technical-overview/external-idps.png" alt-text="Diagram showing company logos for a sample of external identity providers.":::
 
@@ -92,23 +93,23 @@ On the sign-up or sign-in page, Azure AD B2C presents a list of external identit
 
 :::image type="content" source="media/technical-overview/external-idp.png" alt-text="Diagram showing a mobile sign-in example with a social account (Facebook).":::
 
-To see how to add identity providers in Azure AD B2C, see [Add identity providers to your applications in Azure Active Directory B2C](add-identity-provider.md).
+To learn more about identity providers, see [Add identity providers to your applications in Azure Active Directory B2C](add-identity-provider.md).
 
 ## Identity experiences: user flows or custom policies
 
-In Azure AD B2C, you can define the business logic that users follow to gain access to your application. For example, you can determine the sequence of steps users follow when they sign in, sign up, edit a profile, or reset a password. After completing the sequence, the user acquires a token and gains access to your application.
+In Azure AD B2C, you can define the business logic that users follow to gain access to your application. For example, you can determine the sequence of steps users follow when they sign in, sign up, edit their profile, or reset a password. After completing the sequence, the user acquires a token and gains access to your application.
 
 In Azure AD B2C, there are two ways to provide identity user experiences:
 
-* **User flows** are predefined, built-in, configurable policies that we provide so you can create sign-up, sign-in, and policy editing experiences in minutes.
+* **User flows** - These are predefined, built-in, configurable policies that we provide so you can create sign-up, sign-in, and policy editing experiences in minutes.
 
-* **Custom policies** enable you to create your own user journeys for complex identity experience scenarios.
+* **Custom policies** - These enable you to create your own user journeys for complex identity experience scenarios.
 
 The following screenshot shows the user flow settings UI, versus custom policy configuration files.
 
 :::image type="content" source="media/technical-overview/user-flow-vs-custom-policy.png" alt-text="Screenshot showing the user flow settings UI versus a custom policy configuration file.":::
 
-Read the [User flows and custom policies overview](user-flow-overview.md) article. It gives an overview of user flows and custom policies, and helps you decide which method will work best for your business needs.
+To learn more about user flows and custom policies, and help you decide which method will work best for your business needs, see [User flows and custom policies overview](user-flow-overview.md).  
 
 ## User interface
 
@@ -125,13 +126,13 @@ For information on UI customization, see:
 
 ## Custom domain
 
-You can customize your Azure AD B2C domain in the redirect URIs for your application. Custom domain allows you to create a seamless experience so that the pages that are shown blend seamlessly with the domain name of your application. From the user's perspective, they remain in your domain during the sign-in process rather than redirecting to the Azure AD B2C default domain .b2clogin.com. 
+You can customize your Azure AD B2C domain in the redirect URIs for your application. Custom domain allows you to create a seamless experience so that the pages that are shown blend seamlessly with the domain name of your application. From the user's perspective, they remain in your domain during the sign-in process rather than redirecting to the Azure AD B2C default domain *.b2clogin.com*. 
 
 For more information, see [Enable custom domains](custom-domain.md).
  
 ## Localization
 
-Language customization in Azure AD B2C allows you to accommodate different languages to suit your customer needs. Microsoft provides the translations for 36 languages, but you can also provide your own translations for any language. 
+Language customization in Azure AD B2C allows you to accommodate different languages to suit your customer needs. Microsoft provides localizations for 36 languages, but you can also provide your own localizations for any language. 
 
 :::image type="content" source="media/technical-overview/localization.png" alt-text="Screenshot of three sign in pages showing UI text in different languages.":::
 
@@ -139,11 +140,11 @@ See how localization works in [Language customization in Azure Active Directory
 
 ## Email verification
 
-Azure AD B2C ensures valid email addresses by requiring customers to verify them during the sign-up, and password reset flows. It also prevents malicious actors from using automated processes to generate fraudulent accounts in your applications.
+Azure AD B2C ensures valid email addresses by requiring customers to verify them during the sign-up, and password reset flows. This also prevents malicious actors from using automated processes to generate fraudulent accounts in your applications.
 
 :::image type="content" source="media/technical-overview/email-verification.png" alt-text="Screenshots showing the process for email verification.":::
 
-You can customize the email to users that sign up to use your applications. By using the third-party email provider, you can use your own email template and From: address and subject, as well as support localization and custom one-time password (OTP) settings. For more information, see:
+You can customize the email sent to users that sign up to use your applications. By using a third-party email provider, you can use your own email template and From: address and subject, as well as support localization and custom one-time password (OTP) settings. For more information, see:
 
 * [Custom email verification with Mailjet](custom-email-mailjet.md)
 * [Custom email verification with SendGrid](custom-email-sendgrid.md)
@@ -171,7 +172,7 @@ You can add a REST API call at any step in a user journey defined by a custom po
 * After Azure AD B2C creates a new account in the directory
 * Before Azure AD B2C issues an access token
 
-For more information, see [Integrate REST API claims exchanges in your Azure AD B2C custom policy](api-connectors-overview.md).
+For more information, see [About API connectors in Azure AD B2C](api-connectors-overview.md).
 
 ## Protocols and tokens
 
@@ -181,10 +182,8 @@ For more information, see [Integrate REST API claims exchanges in your Azure AD
 
 The following diagram shows how Azure AD B2C can communicate using various protocols within the same authentication flow:
 
-![Diagram of OIDC-based client app federating with a SAML-based IdP](media/technical-overview/protocols.png)
 :::image type="content" source="media/technical-overview/protocols.png" alt-text="Diagram of OIDC-based client app federating with a SAML-based IdP.":::
 
-
 1. The relying party application starts an authorization request to Azure AD B2C using OpenID Connect.
 1. When a user of the application chooses to sign in using an external identity provider that uses the SAML protocol, Azure AD B2C invokes the SAML protocol to communicate with that identity provider.
 1. After the user completes the sign-in operation with the external identity provider, Azure AD B2C then returns the token to the relying party application using OpenID Connect.
@@ -201,7 +200,7 @@ For example, to sign in to an application, the application uses the *sign up or
 
 ## Multifactor authentication (MFA)
 
-Azure AD B2C Multi-Factor Authentication (MFA) helps safeguard access to data and applications while maintaining simplicity for your users. It provides extra security by requiring a second form of authentication, and delivers strong authentication by offering a range of easy-to-use authentication methods. 
+Azure AD B2C Multifactor Authentication (MFA) helps safeguard access to data and applications while maintaining simplicity for your users. It provides extra security by requiring a second form of authentication, and delivers strong authentication by offering a range of easy-to-use authentication methods. 
 
 Your users may or may not be challenged for MFA based on configuration decisions that you can make as an administrator.
 
@@ -213,8 +212,7 @@ Microsoft Entra ID Protection risk-detection features, including risky users and
 
 :::image type="content" source="media/technical-overview/conditional-access-flow.png" alt-text="Diagram showing conditional access flow.":::
 
-
-Azure AD B2C evaluates each sign-in event and ensures that all policy requirements are met before granting the user access. Risky users or sign-ins may be blocked, or challenged with a specific remediation like multifactor authentication (MFA). For more information, see [Identity Protection and Conditional Access](conditional-access-identity-protection-overview.md).
+Azure AD B2C evaluates each sign-in event and ensures that all policy requirements are met before granting the user access. Risky users or risky sign-ins may be blocked, or challenged with a specific remediation like multifactor authentication (MFA). For more information, see [Identity Protection and Conditional Access](conditional-access-identity-protection-overview.md).
 
 ## Password complexity
 
@@ -226,9 +224,7 @@ For more information, see [Configure complexity requirements for passwords in Az
 
 ## Force password reset
 
-As an Azure AD B2C tenant administrator, you can [reset a user's password](manage-users-portal.md#reset-a-users-password) if the user forgets their password. Or you would like to force them to reset the password periodically. For more information, see [Set up a force password reset flow](force-password-reset.md).
-
-
+As an Azure AD B2C tenant administrator, you can [reset a user's password](manage-users-portal.md#reset-a-users-password) if the user forgets their password. Or you can set a policy to force users to reset their password periodically. For more information, see [Set up a force password reset flow](force-password-reset.md).
 
 :::image type="content" source="media/technical-overview/force-password-reset-flow.png" alt-text="Force password reset flow.":::
 
@@ -236,16 +232,15 @@ As an Azure AD B2C tenant administrator, you can [reset a user's password](manag
 
 To prevent brute-force password guessing attempts, Azure AD B2C uses a sophisticated strategy to lock accounts based on the IP of the request, the passwords entered, and several other factors. The duration of the lockout is automatically increased based on risk and the number of attempts.
 
-![Account smart lockout](media/technical-overview/smart-lockout1.png)
 :::image type="content" source="media/technical-overview/smart-lockout1.png" alt-text="Screenshot of UI for account lockout with arrows highlighting the lockout notification.":::
 
 For more information about managing password protection settings, see [Mitigate


[新特性] Azure AD B2C技术概览和功能详细更新
2023-11-22 17:35:24
https://learn.microsoft.com/en-us/azure/aks/coredns-custom

此文件中添加了关于CoreDNS pod副本推荐最小数量的警告。建议每个集群至少有2个CoreDNS pod副本。如果配置的CoreDNS pod副本最小数量为1，可能会在需要节点排空的操作中出现故障，例如集群升级操作。

https://learn.microsoft.com/en-us/azure/active-directory-b2c/overview

此文件进行了一些更新和修改。主要更改包括对Azure Active Directory B2C的描述，以及对使用Azure AD B2C进行身份验证的各种方式的描述。此外，还对一些语句进行了重写和澄清，以提高可读性和准确性。

https://learn.microsoft.com/en-us/azure/active-directory-b2c/overview 和 https://learn.microsoft.com/en-us/azure/active-directory-b2c/technical-overview

这两个文件都进行了更新。在overview文件中，更新了日期，并修改了客户意图的描述。在technical-overview文件中，对Azure AD B2C的技术和功能进行了更详细的介绍，包括Azure AD B2C租户、本地帐户登录选项、外部身份提供商登录、用户界面、自定义域、本地化、电子邮件验证、多因素身份验证（MFA）、密码复杂性、强制密码重置等。同时，这两个文件的修改也包括了一些语句的重写和澄清，以提高可读性和准确性。

*************************

commit_patch_data :

From ea0ce9fb61df6cc8dfaf7c365a57180b1f864dba Mon Sep 17 00:00:00 2001
From: James Barnett <v-jabarnett@microsoft.com>
Date: Wed, 22 Nov 2023 10:37:39 -0700
Subject: [PATCH] Acro fix

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index bcaae948c9cb..d87a25320da1 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,7 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
+This article only shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -206,4 +206,4 @@ embedding = client.embeddings.create(
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)



commit_patch_data :

From a327c4725689c025b40d8c9ceaef2d309672dfb6 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:56:32 -0500
Subject: [PATCH 1/6] update

---
 .../openai/how-to/switching-endpoints.md      | 189 +++++++++++++++++-
 1 file changed, 187 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 7d45305693b2..635cae90c973 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,8 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-> [!NOTE]
-> This library is maintained by OpenAI and is currently in preview. Refer to the [release history](https://github.com/openai/openai-python/releases) or the [version.py commit history](https://github.com/openai/openai-python/commits/main/openai/version.py) to track the latest updates to the library.
+# [OpenAI Python 0.28.1](#tab/python)
 
 ## Authentication
 
@@ -201,6 +200,192 @@ embedding = openai.Embedding.create(
 </tr>
 </table>
 
+# [OpenAI Python 1.x](#tab/python-new)
+
+## Authentication
+
+We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
+
+### API key
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+import os
+from openai import AzureOpenAI
+    
+client = AzureOpenAI(
+    api_key=os.getenv("AZURE_OPENAI_KEY"),  
+    api_version="2023-10-01-preview",
+    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
+    )
+```
+
+</td>
+</tr>
+</table>
+
+<a name='azure-active-directory-authentication'></a>
+
+### Microsoft Entra authentication
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+from openai import OpenAI
+
+client = OpenAI(
+  api_key=os.environ['OPENAI_API_KEY']  
+)
+
+
+
+
+
+
+```
+
+</td>
+<td>
+
+```python
+from azure.identity import DefaultAzureCredential, get_bearer_token_provider
+from openai import AzureOpenAI
+
+token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")
+
+api_version = "2023-12-01-preview"
+endpoint = "https://my-resource.openai.azure.com"
+
+client = AzureOpenAI(
+    api_version=api_version,
+    azure_endpoint=endpoint,
+    azure_ad_token_provider=token_provider,
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Keyword argument for model
+
+OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
+
+For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+completion = client.completions.create(
+    model='gpt-3.5-turbo-instruct',
+    prompt="<prompt>)
+)
+
+chat_completion = openai.chat.completions.create(
+    model="gpt-4",
+    messages="<messages>"
+)
+
+embedding = client.embeddings.create(
+    input="<input>",
+    model="text-embedding-ada-002"
+)
+```
+
+</td>
+<td>
+
+```python
+client.completions.create(
+    model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
+    prompt=<"prompt">
+)
+
+client.chat.completions.create(
+    model="gpt-35-turbo", # model = "deployment_name".
+    messages=<"messages">
+)
+
+client.embeddings.create(
+    input = "<input>",
+    model= "text-embedding-ada-002" # model = "deployment_name".
+)
+```
+
+</td>
+</tr>
+</table>
+
+## Azure OpenAI embeddings multiple input support
+
+OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
+
+<table>
+<tr>
+<td> OpenAI </td> <td> Azure OpenAI </td>
+</tr>
+<tr>
+<td>
+
+```python
+inputs = ["A", "B", "C"] 
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002"
+)
+
+
+```
+
+</td>
+<td>
+
+```python
+inputs = ["A", "B", "C"] #max array size=16
+
+embedding = client.embeddings.create(
+  input=inputs,
+  model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
+  #engine="text-embedding-ada-002"
+)
+```
+
+</td>
+</tr>
+</table>
+
+---
+
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).

From dbffff50a4994b23b4fa171b747aa0bcc6ee4893 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 09:58:22 -0500
Subject: [PATCH 2/6] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 635cae90c973..8a8f70c55a10 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -7,7 +7,7 @@ ms.author: mbullwin #delegenz
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to
-ms.date: 07/20/2023
+ms.date: 11/22/2023
 manager: nitinme
 ---
 

From c834902936f80682d16dc481ece7ebf78006d0b5 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:15:14 -0500
Subject: [PATCH 3/6] update

---
 .../openai/how-to/switching-endpoints.md      | 204 +-----------------
 1 file changed, 10 insertions(+), 194 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 8a8f70c55a10..90e3e9b09811 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,192 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-# [OpenAI Python 0.28.1](#tab/python)
-
-## Authentication
-
-We recommend using environment variables. If you haven't done this before our [Python quickstarts](../quickstart.md) walk you through this configuration.
-
-### API key
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-
-openai.api_type = "azure"
-openai.api_key = "..."
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-<a name='azure-active-directory-authentication'></a>
-
-### Microsoft Entra authentication
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-import openai
-
-openai.api_key = "sk-..."
-openai.organization = "..."
-
-
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-import openai
-from azure.identity import DefaultAzureCredential
-
-credential = DefaultAzureCredential()
-token = credential.get_token("https://cognitiveservices.azure.com/.default")
-
-openai.api_type = "azure_ad"
-openai.api_key = token.token
-openai.api_base = "https://example-endpoint.openai.azure.com"
-openai.api_version = "2023-05-15"  # subject to change
-```
-
-</td>
-</tr>
-</table>
-
-## Keyword argument for model
-
-OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of [deployments](create-resource.md?pivots=web-portal#deploy-a-model) and uses the `deployment_id` keyword argument to describe which model deployment to use. Azure OpenAI also supports the use of `engine` interchangeably with `deployment_id`. `deployment_id` corresponds to the custom name you chose for your model during model deployment. By convention in our docs, we often show `deployment_id`'s which match the underlying model name, but if you chose a different deployment name that doesn't match the model name you need to use that name when working with models in Azure OpenAI.
-
-For OpenAI `engine` still works in most instances, but it's deprecated and `model` is preferred.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    model="text-davinci-003"
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    model="gpt-4"
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  model="text-embedding-ada-002"
-)
-
-
-
-
-```
-
-</td>
-<td>
-
-```python
-completion = openai.Completion.create(
-    prompt="<prompt>",
-    deployment_id="text-davinci-003" # This must match the custom deployment name you chose for your model.
-    #engine="text-davinci-003" 
-)
-  
-chat_completion = openai.ChatCompletion.create(
-    messages="<messages>",
-    deployment_id="gpt-4" # This must match the custom deployment name you chose for your model.
-    #engine="gpt-4"
-
-)
-
-embedding = openai.Embedding.create(
-  input="<input>",
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-## Azure OpenAI embeddings multiple input support
-
-OpenAI currently allows a larger number of array inputs with text-embedding-ada-002. Azure OpenAI currently supports input arrays up to 16 for text-embedding-ada-002 Version 2. Both require the max input token limit per API request to remain under 8191 for this model.
-
-<table>
-<tr>
-<td> OpenAI </td> <td> Azure OpenAI </td>
-</tr>
-<tr>
-<td>
-
-```python
-inputs = ["A", "B", "C"] 
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  model="text-embedding-ada-002"
-)
-
-
-```
-
-</td>
-<td>
-
-```python
-inputs = ["A", "B", "C"] #max array size=16
-
-embedding = openai.Embedding.create(
-  input=inputs,
-  deployment_id="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
-  #engine="text-embedding-ada-002"
-)
-```
-
-</td>
-</tr>
-</table>
-
-# [OpenAI Python 1.x](#tab/python-new)
+This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -223,6 +38,7 @@ client = OpenAI(
 )
 
 
+
 ```
 
 </td>
@@ -266,6 +82,8 @@ client = OpenAI(
 
 
 
+
+
 ```
 
 </td>
@@ -310,7 +128,7 @@ completion = client.completions.create(
     prompt="<prompt>)
 )
 
-chat_completion = openai.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-4",
     messages="<messages>"
 )
@@ -325,17 +143,17 @@ embedding = client.embeddings.create(
 <td>
 
 ```python
-client.completions.create(
+completion = client.completions.create(
     model=gpt-35-turbo-instruct, # This must match the custom deployment name you chose for your model.
     prompt=<"prompt">
 )
 
-client.chat.completions.create(
+chat_completion = client.chat.completions.create(
     model="gpt-35-turbo", # model = "deployment_name".
     messages=<"messages">
 )
 
-client.embeddings.create(
+embedding = client.embeddings.create(
     input = "<input>",
     model= "text-embedding-ada-002" # model = "deployment_name".
 )
@@ -364,7 +182,6 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
-
 ```
 
 </td>
@@ -378,15 +195,14 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002" # This must match the custom deployment name you chose for your model.
   #engine="text-embedding-ada-002"
 )
+
 ```
 
 </td>
 </tr>
 </table>
 
----
-
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file

From 659bbf38c4b50ecb1aeedf80987724f50d30a389 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:17:59 -0500
Subject: [PATCH 4/6] update

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 90e3e9b09811..1b74c13e916a 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -2,8 +2,8 @@
 title: How to switch between OpenAI and Azure OpenAI Service endpoints with Python
 titleSuffix: Azure OpenAI Service
 description: Learn about the changes you need to make to your code to swap back and forth between OpenAI and Azure OpenAI endpoints.
-author: mrbullwinkle #dereklegenzoff
-ms.author: mbullwin #delegenz
+author: mrbullwinkle 
+ms.author: mbullwin 
 ms.service: azure-ai-openai
 ms.custom: devx-track-python
 ms.topic: how-to

From 85d411bb59d2a61a3c28de546c3697928ccf898b Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 11:29:49 -0500
Subject: [PATCH 5/6] updatE

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 1 +
 1 file changed, 1 insertion(+)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index 1b74c13e916a..bcaae948c9cb 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -182,6 +182,7 @@ embedding = client.embeddings.create(
   model="text-embedding-ada-002"
 )
 
+
 ```
 
 </td>

From ea0ce9fb61df6cc8dfaf7c365a57180b1f864dba Mon Sep 17 00:00:00 2001
From: James Barnett <v-jabarnett@microsoft.com>
Date: Wed, 22 Nov 2023 10:37:39 -0700
Subject: [PATCH 6/6] Acro fix

---
 articles/ai-services/openai/how-to/switching-endpoints.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/how-to/switching-endpoints.md b/articles/ai-services/openai/how-to/switching-endpoints.md
index bcaae948c9cb..d87a25320da1 100644
--- a/articles/ai-services/openai/how-to/switching-endpoints.md
+++ b/articles/ai-services/openai/how-to/switching-endpoints.md
@@ -15,7 +15,7 @@ manager: nitinme
 
 While OpenAI and Azure OpenAI Service rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.
 
-This article onlys shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
+This article only shows examples with the new OpenAI Python 1.x API library. For information on migrating from `0.28.1` to `1.x` refer to our [migration guide](./migration.md).
 
 ## Authentication
 
@@ -206,4 +206,4 @@ embedding = client.embeddings.create(
 ## Next steps
 
 * Learn more about how to work with GPT-35-Turbo and the GPT-4 models with [our how-to guide](../how-to/chatgpt.md).
-* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)
\ No newline at end of file
+* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://aka.ms/AOAICodeSamples)



[新功能] OpenAI和Azure OpenAI切换方法的详细更新
2023-11-22 17:51:16
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints

在第一个提交中，对文件进行了大量的更新，主要是增加了关于如何在OpenAI和Azure OpenAI之间切换的详细说明。这包括了如何进行身份验证，如何使用API密钥，如何使用Microsoft Entra进行身份验证，如何为模型使用关键字参数，以及如何支持Azure OpenAI嵌入多个输入。

在第二个提交中，更新了文件的日期。

在第三个提交中，删除了大量的内容，主要是关于OpenAI Python 0.28.1的示例和说明，只保留了关于OpenAI Python 1.x的内容，并添加了一个链接到迁移指南。

在第四个提交中，更新了作者信息。

在第五个提交中，对文件进行了微小的格式调整，添加了一个空行。

在第六个提交中，修正了一个拼写错误。

*************************

commit_patch_data :

From 4ade8961db578609d55f5be1b906f3cb68fa6aa3 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 14:01:52 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/how-to/content-filters.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/articles/ai-services/openai/how-to/content-filters.md b/articles/ai-services/openai/how-to/content-filters.md
index f1f6850e9ce6..976b2622e288 100644
--- a/articles/ai-services/openai/how-to/content-filters.md
+++ b/articles/ai-services/openai/how-to/content-filters.md
@@ -18,7 +18,7 @@ keywords:
 > [!NOTE]
 > All customers have the ability to modify the content filters to be stricter (for example, to filter content at lower severity levels than the default). Approval is required for turning the content filters partially or fully off. Managed customers only may apply for full content filtering control via this form: [Azure OpenAI Limited Access Review: Modified Content Filters and Abuse Monitoring (microsoft.com)](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xURE01NDY1OUhBRzQ3MkQxMUhZSE1ZUlJKTiQlQCN0PWcu).
 
-The content filtering system integrated into Azure OpenAI Service runs alongside the core models and uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe  is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering system [here](../concepts/content-filter.md). Jailbreak risk detection and protected text and code models are optional and off by default. For jailbreak and protected material text and code models, the configurability feature allows all customers to turn the models on and off. The models are by default off and can be turned on per your scenario. Note that some models are required to be on for certain scenarios to retain coverage under the [Customer Copyright Commitment](https://www.microsoft.com/licensing/news/Microsoft-Copilot-Copyright-Commitment).
+The content filtering system integrated into Azure OpenAI Service runs alongside the core models and uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe  is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering system [here](../concepts/content-filter.md). Jailbreak risk detection and protected text and code models are optional and off by default. For jailbreak and protected material text and code models, the configurability feature allows all customers to turn the models on and off. The models are by default off and can be turned on per your scenario. Note that some models are required to be on for certain scenarios to retain coverage under the [Customer Copyright Commitment](/legal/cognitive-services/openai/customer-copyright-commitment?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext).
 
 Content filters can be configured at resource level. Once a new configuration is created, it can be associated with one or more deployments. For more information about model deployment, see the [resource deployment guide](create-resource.md).
 



commit_patch_data :

From 2d4669b213b7e68d8c4de5684df0703e6fc624f2 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 16:37:59 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/faq.yml | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 92a39d0394b9..c5a8d2c3669f 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -7,7 +7,7 @@ metadata:
   manager: nitinme
   ms.service: azure-ai-openai
   ms.topic: faq
-  ms.date: 11/15/2023
+  ms.date: 11/22/2023
   ms.author: mbullwin
   author: mrbullwinkle
 title: Azure OpenAI Service frequently asked questions
@@ -25,7 +25,7 @@ sections:
       - question: |
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
-          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
           I can't find GPT-4 Turbo Preview?
         answer:
@@ -90,6 +90,20 @@ sections:
           If you wanted to help a GPT based model to accurately respond to the question "what model are you running?", you would need to provide that information to the model through techniques like [prompt engineering of the model's system message](/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions), [Retrieval Augmented Generation (RAG)](/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2) which is the technique used by [Azure OpenAI on your data](/azure/ai-services/openai/concepts/use-your-data) where up-to-date information is injected to the system message at query time, or via [fine-tuning](/azure/ai-services/openai/how-to/fine-tuning?pivots=programming-language-studio) where you could fine-tune specific versions of the model to answer that question in a certain way based on model version. 
 
           To learn more about how GPT models are trained and work we recommend watching [Andrej Karpathy's talk from Build 2023 on the state of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A).
+      - question: |
+          I asked the model when it's knowledge cutoff is and it gave me a different answer than what is on the Azure OpenAI model's page. Why does this happen?
+        answer:
+          This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
+      - question: |
+          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer:
+          This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
+          
+          The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
+
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+                    
+          So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |
           Where do I access pricing information for legacy models which are no longer available for new deployments? 
         answer: | 



[模型] Azure OpenAI服务FAQ更新：新增模型知识截止日期等问题答案
2023-11-22 21:37:59
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

此文件中的更改主要涉及以下几个方面：

1. 更新了元数据中的日期，从2023年11月15日更改为2023年11月22日。

2. 对一些问题的回答进行了微小的文本修改，例如将"However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI." 修改为 "However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI."

3. 添加了新的问题和答案，例如关于模型知识截止日期的问题和答案，以及关于模型对最近事件的回答不准确的问题和答案。这些新的问题和答案提供了更多关于Azure OpenAI服务的信息，帮助用户更好地理解和使用该服务。

*************************

commit_patch_data :

From a53489cb84301f84243bc2b49678db32809436da Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 17:03:13 -0500
Subject: [PATCH] update

---
 articles/ai-services/openai/faq.yml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index c5a8d2c3669f..bb7162dddc9e 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -95,13 +95,13 @@ sections:
         answer:
           This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
       - question: |
-          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
-        answer:
+          I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer: |
           This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
           
           The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
 
-          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield an accurate response which demonstrates training data knowledge going to at least January of 2023.   
                     
           So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |



[模型] 对AI模型FAQ问题答案的重要修改和补充
2023-11-22 22:03:13
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

这个文件中的更改主要是对一些问题和答案的文本进行了修改。首先，对一个问题的表述进行了微调，将“我问了模型一个问题，它应该知道关于最近发生的事情的答案，基于知识截止日期，但它回答错了。为什么会这样？”修改为“我问了模型一个关于最近在知识截止日期之前发生的事情的问题，但它回答错了。为什么会这样？”。其次，对该问题的答案进行了一些修改和补充，更详细地解释了模型可能回答错误的原因。最后，对另一个问题的答案进行了修改，将“然而，问模型'杰辛达·阿德恩何时辞去新西兰总理职务？'倾向于得到一个正确的回答，这表明训练数据的知识至少到2023年1月。”修改为“然而，问模型'杰辛达·阿德恩何时辞去新西兰总理职务？'倾向于得到一个准确的回答，这表明训练数据的知识至少到2023年1月。”。

*************************

commit_patch_data :

From 2d4669b213b7e68d8c4de5684df0703e6fc624f2 Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 16:37:59 -0500
Subject: [PATCH 1/2] update

---
 articles/ai-services/openai/faq.yml | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index 92a39d0394b9..c5a8d2c3669f 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -7,7 +7,7 @@ metadata:
   manager: nitinme
   ms.service: azure-ai-openai
   ms.topic: faq
-  ms.date: 11/15/2023
+  ms.date: 11/22/2023
   ms.author: mbullwin
   author: mrbullwinkle
 title: Azure OpenAI Service frequently asked questions
@@ -25,7 +25,7 @@ sections:
       - question: |
           Does Azure OpenAI work with the latest Python library released by OpenAI (version>=1.0)?
         answer: |
-          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it is important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
+          Azure OpenAI is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). However, it's important to note migration of your codebase using `openai migrate` is not supported and will not work with code that targets Azure OpenAI.  
       - question: |
           I can't find GPT-4 Turbo Preview?
         answer:
@@ -90,6 +90,20 @@ sections:
           If you wanted to help a GPT based model to accurately respond to the question "what model are you running?", you would need to provide that information to the model through techniques like [prompt engineering of the model's system message](/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions), [Retrieval Augmented Generation (RAG)](/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2) which is the technique used by [Azure OpenAI on your data](/azure/ai-services/openai/concepts/use-your-data) where up-to-date information is injected to the system message at query time, or via [fine-tuning](/azure/ai-services/openai/how-to/fine-tuning?pivots=programming-language-studio) where you could fine-tune specific versions of the model to answer that question in a certain way based on model version. 
 
           To learn more about how GPT models are trained and work we recommend watching [Andrej Karpathy's talk from Build 2023 on the state of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A).
+      - question: |
+          I asked the model when it's knowledge cutoff is and it gave me a different answer than what is on the Azure OpenAI model's page. Why does this happen?
+        answer:
+          This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
+      - question: |
+          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer:
+          This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
+          
+          The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
+
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+                    
+          So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |
           Where do I access pricing information for legacy models which are no longer available for new deployments? 
         answer: | 

From a53489cb84301f84243bc2b49678db32809436da Mon Sep 17 00:00:00 2001
From: mrbullwinkle <31510320+mrbullwinkle@users.noreply.github.com>
Date: Wed, 22 Nov 2023 17:03:13 -0500
Subject: [PATCH 2/2] update

---
 articles/ai-services/openai/faq.yml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/articles/ai-services/openai/faq.yml b/articles/ai-services/openai/faq.yml
index c5a8d2c3669f..bb7162dddc9e 100644
--- a/articles/ai-services/openai/faq.yml
+++ b/articles/ai-services/openai/faq.yml
@@ -95,13 +95,13 @@ sections:
         answer:
           This is expected behavior. The models aren't able to answer questions about themselves. If you want to know when the knowledge cutoff for the model's training data is, consult the [models page](./concepts/models.md).
       - question: |
-          I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?
-        answer:
+          I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?
+        answer: |
           This is expected behavior. First there's no guarantee that every recent event that has occurred was part of the model's training data. And even when information was part of the training data, without using additional techniques like Retrieval Augmented Generation (RAG) to help ground the model's responses there's always a chance of ungrounded responses occurring. Both Azure OpenAI's [use your data feature](./concepts/use-your-data.md) and [Bing Chat](https://www.microsoft.com/edge/features/bing-chat?form=MT00D8) use Azure OpenAI models combined with Retrieval Augmented Generation to help further ground model responses. 
           
           The frequency that a given piece of information appeared in the training data can also impact the likelihood that the model will respond in a certain way. 
 
-          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield a correct response which demonstrates training data knowledge going to at least January of 2023.   
+          Asking the latest GPT-4 Turbo Preview model about something that changed more recently like "Who is the prime minister of New Zealand?", is likely to result in the fabricated response `Jacinda Ardern`. However, asking the model "When did `Jacinda Ardern` step down as prime minister?" Tends to yield an accurate response which demonstrates training data knowledge going to at least January of 2023.   
                     
           So while it is possible to probe the model with questions to guess its training data knowledge cutoff, the [model's page](./concepts/models.md) is the best place to check a model's knowledge cutoff.       
       - question: |



[模型] 更新了关于模型知识截止日期和回答错误原因的问题和答案
2023-11-22 22:18:22
https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

这个文件中的更改主要包括以下几个部分：
1. 更新了文件的元数据中的日期。
2. 对一些问题的回答进行了微小的文本修改，例如将"It is important to note"更改为"It's important to note"。
3. 添加了两个新的问题和答案，这些问题主要关于模型的知识截止日期和模型对最近事件的回答可能出错的原因。
4. 对一些已有问题的回答进行了扩展和详细化。

https://learn.microsoft.com/en-us/azure/ai-services/openai/faq

这个文件中的更改主要是对一些问题的表述和回答进行了微调。例如，将"I asked the model a question it should know the answer to about something that happened recently based on the knowledge cutoff and it got the answer wrong. Why does this happen?"更改为"I asked the model a question about something that happened recently before the knowledge cutoff and it got the answer wrong. Why does this happen?"。同时，对一些回答中的描述进行了微调，例如将"tends to yield a correct response"更改为"tends to yield an accurate response"。

*************************

commit_patch_data :

From 603117a8897f5ec3fcd0caf4725e066e9ad4f184 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 16:21:14 -0600
Subject: [PATCH] powershell sample for user your own data quickstart

---
 .../includes/use-your-data-powershell.md      | 83 +++++++++++++++++++
 .../openai/use-your-data-quickstart.md        |  8 +-
 2 files changed, 90 insertions(+), 1 deletion(-)
 create mode 100644 articles/ai-services/openai/includes/use-your-data-powershell.md

diff --git a/articles/ai-services/openai/includes/use-your-data-powershell.md b/articles/ai-services/openai/includes/use-your-data-powershell.md
new file mode 100644
index 000000000000..c862003f3d5d
--- /dev/null
+++ b/articles/ai-services/openai/includes/use-your-data-powershell.md
@@ -0,0 +1,83 @@
+---
+services: cognitive-services
+manager: nitinme
+author: mgreenegit
+ms.author: migreene
+ms.service: azure-ai-openai
+ms.topic: include
+ms.date: 11/22/2023
+---
+
+[!INCLUDE [Set up required variables](./use-your-data-common-variables.md)]
+
+## Example PowerShell commands
+
+The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. The `messages` variable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. The `dataSources` variable connects to your Azure Cognitive Search index, and enables Azure OpenAI models to respond using your data.
+
+To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
+
+> [!TIP]
+> There are several parameters you can use to change the model's response, such as `temperature` or `top_p`. See the [reference documentation](../reference.md#completions-extensions) for more information.
+
+```powershell-interactive
+# Azure OpenAI metadata variables
+   $openai = @{
+       api_key     = $Env:AZURE_OPENAI_KEY
+       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
+       api_version = '2023-07-01-preview' # this may change in the future
+       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
+   }
+
+   $acs = @{
+       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
+       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
+       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
+   }
+
+   # Completion text
+   $body = @{
+    dataSources = @(
+        @{
+            type = 'AzureCognitiveSearch'
+            parameters = @{
+                    endpoint = $acs.search_endpoint
+                    key = $acs.search_key
+                    indexName = $acs.search_index
+                }
+        }
+    )
+    messages = @(
+            @{
+                role = 'user'
+                content = 'How do you query REST using PowerShell'
+            }
+    )
+   } | convertto-json -depth 5
+
+   # Header for authentication
+   $headers = [ordered]@{
+       'api-key' = $openai.api_key
+   }
+
+   # Send a completion call to generate an answer
+   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"
+
+   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
+   return $response.choices.messages[1].content
+```
+
+### Example output
+
+```text
+To query a RESTful web service using PowerShell, you can use the `Invoke-RestMethod` cmdlet. This cmdlet sends HTTP and HTTPS requests to RESTful web services and processes the response based on the data type.
+```
+
+> [!IMPORTANT]
+> For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see the Azure AI services [security](../../security-features.md) article.
+
+> [!div class="nextstepaction"]
+> [I ran into an issue with sending the request.](https://microsoft.qualtrics.com/jfe/form/SV_0Cl5zkG3CnDjq6O?PLanguage=REST&Pillar=AOAI&Product=ownData&Page=quickstart&Section=Send-request)
+
+## Chat with your model using a web app
+
+To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app using [Azure OpenAI studio](../concepts/use-your-data.md#deploying-the-model) or example code we [provide on GitHub](https://go.microsoft.com/fwlink/?linkid=2244395). This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize the [frontend and backend logic](../concepts/use-your-data.md#using-the-web-app) of the web app by making changes to the source code.
diff --git a/articles/ai-services/openai/use-your-data-quickstart.md b/articles/ai-services/openai/use-your-data-quickstart.md
index 9c6861aefabb..b1dd5688b33c 100644
--- a/articles/ai-services/openai/use-your-data-quickstart.md
+++ b/articles/ai-services/openai/use-your-data-quickstart.md
@@ -9,7 +9,7 @@ ms.custom: devx-track-dotnet, devx-track-extended-java, devx-track-js, devx-trac
 ms.topic: quickstart
 author: aahill
 ms.author: aahi
-ms.date: 08/25/2023
+ms.date: 11/22/2023
 recommendations: false
 zone_pivot_groups: openai-use-your-data
 ---
@@ -88,6 +88,12 @@ In this quickstart you can use your own data with Azure OpenAI models. Using Azu
 
 ::: zone-end
 
+::: zone pivot="programming-language-powershell"
+
+[!INCLUDE [PowerShell quickstart](includes/use-your-data-powershell.md)]
+
+::: zone-end
+
 ::: zone pivot="programming-language-go"
 
 [!INCLUDE [Go quickstart](includes/use-your-data-go.md)]



[新功能] Azure OpenAI中使用PowerShell处理自有数据的新指南发布
2023-11-22 22:21:14
https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

这是一个新文件，主要包含了如何使用PowerShell在Azure OpenAI中使用自己的数据。文件中包含了一些示例的PowerShell命令，这些命令优化了与输入格式为对话的Azure OpenAI聊天模型的工作。还包含了如何触发模型响应的信息，以及一些可以改变模型响应的参数。此外，还提供了一个示例输出，说明了如何使用PowerShell查询REST。最后，文件还提供了如何使用web应用与使用自己数据的Azure OpenAI模型进行聊天的信息。

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

这个文件的修改主要是在原有的基础上添加了一个新的区块，这个区块包含了如何使用PowerShell在Azure OpenAI中使用自己的数据的快速入门指南。

*************************

commit_patch_data :

From 603117a8897f5ec3fcd0caf4725e066e9ad4f184 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 16:21:14 -0600
Subject: [PATCH 1/2] powershell sample for user your own data quickstart

---
 .../includes/use-your-data-powershell.md      | 83 +++++++++++++++++++
 .../openai/use-your-data-quickstart.md        |  8 +-
 2 files changed, 90 insertions(+), 1 deletion(-)
 create mode 100644 articles/ai-services/openai/includes/use-your-data-powershell.md

diff --git a/articles/ai-services/openai/includes/use-your-data-powershell.md b/articles/ai-services/openai/includes/use-your-data-powershell.md
new file mode 100644
index 000000000000..c862003f3d5d
--- /dev/null
+++ b/articles/ai-services/openai/includes/use-your-data-powershell.md
@@ -0,0 +1,83 @@
+---
+services: cognitive-services
+manager: nitinme
+author: mgreenegit
+ms.author: migreene
+ms.service: azure-ai-openai
+ms.topic: include
+ms.date: 11/22/2023
+---
+
+[!INCLUDE [Set up required variables](./use-your-data-common-variables.md)]
+
+## Example PowerShell commands
+
+The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. The `messages` variable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. The `dataSources` variable connects to your Azure Cognitive Search index, and enables Azure OpenAI models to respond using your data.
+
+To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
+
+> [!TIP]
+> There are several parameters you can use to change the model's response, such as `temperature` or `top_p`. See the [reference documentation](../reference.md#completions-extensions) for more information.
+
+```powershell-interactive
+# Azure OpenAI metadata variables
+   $openai = @{
+       api_key     = $Env:AZURE_OPENAI_KEY
+       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
+       api_version = '2023-07-01-preview' # this may change in the future
+       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
+   }
+
+   $acs = @{
+       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
+       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
+       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
+   }
+
+   # Completion text
+   $body = @{
+    dataSources = @(
+        @{
+            type = 'AzureCognitiveSearch'
+            parameters = @{
+                    endpoint = $acs.search_endpoint
+                    key = $acs.search_key
+                    indexName = $acs.search_index
+                }
+        }
+    )
+    messages = @(
+            @{
+                role = 'user'
+                content = 'How do you query REST using PowerShell'
+            }
+    )
+   } | convertto-json -depth 5
+
+   # Header for authentication
+   $headers = [ordered]@{
+       'api-key' = $openai.api_key
+   }
+
+   # Send a completion call to generate an answer
+   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"
+
+   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
+   return $response.choices.messages[1].content
+```
+
+### Example output
+
+```text
+To query a RESTful web service using PowerShell, you can use the `Invoke-RestMethod` cmdlet. This cmdlet sends HTTP and HTTPS requests to RESTful web services and processes the response based on the data type.
+```
+
+> [!IMPORTANT]
+> For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see the Azure AI services [security](../../security-features.md) article.
+
+> [!div class="nextstepaction"]
+> [I ran into an issue with sending the request.](https://microsoft.qualtrics.com/jfe/form/SV_0Cl5zkG3CnDjq6O?PLanguage=REST&Pillar=AOAI&Product=ownData&Page=quickstart&Section=Send-request)
+
+## Chat with your model using a web app
+
+To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app using [Azure OpenAI studio](../concepts/use-your-data.md#deploying-the-model) or example code we [provide on GitHub](https://go.microsoft.com/fwlink/?linkid=2244395). This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize the [frontend and backend logic](../concepts/use-your-data.md#using-the-web-app) of the web app by making changes to the source code.
diff --git a/articles/ai-services/openai/use-your-data-quickstart.md b/articles/ai-services/openai/use-your-data-quickstart.md
index 9c6861aefabb..b1dd5688b33c 100644
--- a/articles/ai-services/openai/use-your-data-quickstart.md
+++ b/articles/ai-services/openai/use-your-data-quickstart.md
@@ -9,7 +9,7 @@ ms.custom: devx-track-dotnet, devx-track-extended-java, devx-track-js, devx-trac
 ms.topic: quickstart
 author: aahill
 ms.author: aahi
-ms.date: 08/25/2023
+ms.date: 11/22/2023
 recommendations: false
 zone_pivot_groups: openai-use-your-data
 ---
@@ -88,6 +88,12 @@ In this quickstart you can use your own data with Azure OpenAI models. Using Azu
 
 ::: zone-end
 
+::: zone pivot="programming-language-powershell"
+
+[!INCLUDE [PowerShell quickstart](includes/use-your-data-powershell.md)]
+
+::: zone-end
+
 ::: zone pivot="programming-language-go"
 
 [!INCLUDE [Go quickstart](includes/use-your-data-go.md)]

From 6ed27f833ca81a5b55572baf06a8cbf9e7fa10b2 Mon Sep 17 00:00:00 2001
From: mgreenegit <michael.greene@microsoft.com>
Date: Wed, 22 Nov 2023 18:11:30 -0600
Subject: [PATCH 2/2] zone pivot group

---
 articles/zone-pivot-groups.yml | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/articles/zone-pivot-groups.yml b/articles/zone-pivot-groups.yml
index 527edced2671..02cf61560d98 100644
--- a/articles/zone-pivot-groups.yml
+++ b/articles/zone-pivot-groups.yml
@@ -1902,6 +1902,8 @@ groups:
     title: Go 
   - id: rest-api
     title: REST
+  - id: programming-language-powershell
+    title: PowerShell
 # Owner: pafarley
 - id: openai-quickstart-dall-e
   title: Programming languages



[新功能] Azure OpenAI新增PowerShell使用教程和快速入门链接
2023-11-23 04:15:28
https://learn.microsoft.com/en-us/azure/ai-services/openai/includes/use-your-data-powershell

这是一个新文件，主要包含了如何使用PowerShell在Azure OpenAI中使用自己的数据的示例。这个文件详细解释了如何设置所需的变量，如何格式化输入以适应Azure OpenAI的聊天模型，以及如何触发模型的响应。此外，还提供了一个PowerShell交互式示例，展示了如何设置Azure OpenAI和Azure Cognitive Search的元数据变量，如何构建请求体，如何设置认证头，以及如何发送完成调用以生成答案。最后，还给出了一个示例输出，并提供了一些重要的提示和下一步操作。

https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart

这个文件的修改主要是更新了日期，并在文件中添加了一个新的区域，该区域包含了一个PowerShell快速入门的链接。

https://learn.microsoft.com/en-us/azure/zone-pivot-groups

这个文件的修改是在文件中添加了一个新的编程语言选项——PowerShell。

*************************

